<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <style>html, body {
  margin: 0;
  padding: 0;
}

.app {
  margin: 10px;
  padding: 0;
}

.files-list {
  margin: 10px 0 0;
  width: 100%;
  border-collapse: collapse;
}
.files-list__head {
  border: 1px solid #999;
}
.files-list__head > tr > th {
  padding: 10px;
  border: 1px solid #999;
  text-align: left;
  font-weight: normal;
  background: #ddd;
}
.files-list__body {
}
.files-list__file {
  cursor: pointer;
}
.files-list__file:hover {
  background: #ccf;
}
.files-list__file > td {
  padding: 10px;
  border: 1px solid #999;
}
.files-list__file > td:first-child::before {
  content: '\01F4C4';
  margin-right: 1em;
}
.files-list__file_low {
  background: #fcc;
}
.files-list__file_medium {
  background: #ffc;
}
.files-list__file_high {
  background: #cfc;
}
.files-list__file_folder > td:first-child::before {
  content: '\01F4C1';
  margin-right: 1em;
}

.file-header {
  border: 1px solid #999;
  display: flex;
  justify-content: space-between;
  align-items: center;
  position: sticky;
  top: 0;
  background: white;
}

.file-header__back {
  margin: 10px;
  cursor: pointer;
  flex-shrink: 0;
  flex-grow: 0;
  text-decoration: underline;
  color: #338;
}

.file-header__name {
  margin: 10px;
  flex-shrink: 2;
  flex-grow: 2;
}

.file-header__stat {
  margin: 10px;
  flex-shrink: 0;
  flex-grow: 0;
}

.file-content {
  margin: 10px 0 0;
  border: 1px solid #999;
  padding: 10px;
  counter-reset: line;
  display: flex;
  flex-direction: column;
}

.code-line::before {
    content: counter(line);
    margin-right: 10px;
}
.code-line {
  margin: 0;
  padding: 0.3em;
  height: 1em;
  counter-increment: line;
}
.code-line_covered {
  background: #cfc;
}
.code-line_uncovered {
  background: #fcc;
}
</style>
</head>
<body>
    <div id="root"></div>
    <script>
        var data = {"files":[{"path":["/","Users","d4mr","work","thirdweb","engine-core","executors","src","lib.rs"],"content":"pub mod webhook;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","executors","src","webhook","mod.rs"],"content":"use std::collections::HashMap;\nuse std::sync::Arc;\nuse std::time::{Duration, SystemTime, UNIX_EPOCH};\n\nuse hex;\nuse hmac::{Hmac, Mac};\nuse reqwest::header::{HeaderMap, HeaderName, HeaderValue};\nuse serde::{Deserialize, Serialize};\nuse twmq::hooks::TransactionContext;\nuse twmq::job::{Job, JobResult, RequeuePosition};\nuse twmq::{DurableExecution, FailHookData, NackHookData, SuccessHookData};\n\n// --- Configuration ---\n#[derive(Clone, Debug)]\npub struct WebhookRetryConfig {\n    pub max_attempts: u32,\n    pub initial_delay_ms: u64,\n    pub max_delay_ms: u64,\n    pub backoff_factor: f64,\n}\n\nimpl Default for WebhookRetryConfig {\n    fn default() -\u003e Self {\n        Self {\n            max_attempts: 5,\n            initial_delay_ms: 1000, // 1 second\n            max_delay_ms: 30000,    // 30 seconds\n            backoff_factor: 2.0,\n        }\n    }\n}\n\n// --- Execution Context ---\n#[derive(Clone)]\npub struct WebhookExecutionContext {\n    pub http_client: reqwest::Client,\n    pub retry_config: Arc\u003cWebhookRetryConfig\u003e,\n}\n\n// --- Webhook Job Data Structures ---\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct WebhookJobPayload {\n    pub url: String,\n    pub body: String, // Assuming pre-serialized JSON or other content\n    pub headers: Option\u003cHashMap\u003cString, String\u003e\u003e,\n    pub hmac_secret: Option\u003cString\u003e, // Secret key for HMAC-SHA256\n    pub http_method: Option\u003cString\u003e, // e.g., \"POST\", \"PUT\". Defaults to \"POST\"\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]\npub struct WebhookJobOutput {\n    pub status_code: u16,\n    pub response_body: Option\u003cString\u003e,\n}\n\n// --- Webhook Error Enum ---\n#[derive(Serialize, Deserialize, Debug, Clone, thiserror::Error)]\npub enum WebhookError {\n    #[error(\"Network error during webhook dispatch: {0}\")]\n    Network(String),\n\n    #[error(\"Failed to construct webhook request: {0}\")]\n    RequestConstruction(String),\n\n    #[error(\"HMAC signature generation failed: {0}\")]\n    HmacGeneration(String),\n\n    #[error(\"Webhook request timed out: {0}\")]\n    Timeout(String),\n\n    #[error(\"HTTP error from endpoint: status {status}, body: {body_preview}\")]\n    Http {\n        status: u16,\n        body_preview: String, // Store a preview of the raw response body\n    },\n\n    #[error(\"Failed to read or decode webhook response body: {0}\")]\n    ResponseReadError(String),\n\n    #[error(\"Unsupported HTTP method: {0}\")]\n    UnsupportedHttpMethod(String),\n}\n\n// --- DurableExecution Implementation ---\nimpl DurableExecution for WebhookJobPayload {\n    type Output = WebhookJobOutput;\n    type ErrorData = WebhookError;\n    type ExecutionContext = WebhookExecutionContext;\n\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        ec: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        let payload = \u0026job.data;\n        let mut request_headers = HeaderMap::new();\n\n        // Set default Content-Type if body is present, can be overridden by payload.headers\n        if !payload.body.is_empty() {\n            request_headers.insert(\n                reqwest::header::CONTENT_TYPE,\n                HeaderValue::from_static(\"application/json; charset=utf-8\"),\n            );\n        }\n\n        // Apply custom headers\n        if let Some(custom_headers) = \u0026payload.headers {\n            for (key, value) in custom_headers {\n                match HeaderName::from_bytes(key.as_bytes()) {\n                    Ok(header_name) =\u003e match HeaderValue::from_str(value) {\n                        Ok(header_value) =\u003e {\n                            request_headers.insert(header_name, header_value);\n                        }\n                        Err(e) =\u003e {\n                            return JobResult::Fail(WebhookError::RequestConstruction(format!(\n                                \"Invalid header value for '{}': {}\",\n                                key, e\n                            )));\n                        }\n                    },\n                    Err(e) =\u003e {\n                        return JobResult::Fail(WebhookError::RequestConstruction(format!(\n                            \"Invalid header name '{}': {}\",\n                            key, e\n                        )));\n                    }\n                }\n            }\n        }\n\n        // HMAC Signature with Timestamp\n        if let Some(secret) = \u0026payload.hmac_secret {\n            if secret.is_empty() {\n                return JobResult::Fail(WebhookError::HmacGeneration(\n                    \"HMAC secret cannot be empty\".to_string(),\n                ));\n            }\n\n            let timestamp_secs = match SystemTime::now().duration_since(UNIX_EPOCH) {\n                Ok(duration) =\u003e duration.as_secs(),\n                Err(e) =\u003e {\n                    // This is highly unlikely to fail but good to handle\n                    return JobResult::Fail(WebhookError::RequestConstruction(format!(\n                        \"Failed to get system time for timestamp: {}\",\n                        e\n                    )));\n                }\n            };\n            let timestamp_str = timestamp_secs.to_string();\n\n            // Canonical message to sign: \"timestamp.body\"\n            // The dot (.) is a common separator.\n            let message_to_sign = format!(\"{}.{}\", timestamp_str, payload.body);\n\n            type HmacSha256 = Hmac\u003csha2::Sha256\u003e;\n            let mut mac = match HmacSha256::new_from_slice(secret.as_bytes()) {\n                Ok(m) =\u003e m,\n                Err(e) =\u003e {\n                    return JobResult::Fail(WebhookError::HmacGeneration(format!(\n                        \"Failed to initialize HMAC: {}\",\n                        e\n                    )));\n                }\n            };\n            mac.update(message_to_sign.as_bytes());\n            let signature_bytes = mac.finalize().into_bytes();\n            let signature_hex = hex::encode(signature_bytes);\n\n            // Standard header names\n            let signature_header_name = \"X-Signature-SHA256\";\n            let timestamp_header_name = \"X-Request-Timestamp\";\n\n            match HeaderValue::from_str(\u0026signature_hex) {\n                Ok(val) =\u003e {\n                    request_headers.insert(HeaderName::from_static(signature_header_name), val);\n                }\n                Err(_) =\u003e {\n                    // Should not happen with hex string\n                    return JobResult::Fail(WebhookError::HmacGeneration(\n                        \"Generated HMAC signature is not a valid header value\".to_string(),\n                    ));\n                }\n            }\n\n            match HeaderValue::from_str(\u0026timestamp_str) {\n                Ok(val) =\u003e {\n                    request_headers.insert(HeaderName::from_static(timestamp_header_name), val);\n                }\n                Err(_) =\u003e {\n                    // Should not happen with a string representation of u64\n                    return JobResult::Fail(WebhookError::RequestConstruction(\n                        \"Timestamp is not a valid header value\".to_string(),\n                    ));\n                }\n            }\n        }\n\n        let http_method_str = payload\n            .http_method\n            .as_deref()\n            .unwrap_or(\"POST\")\n            .to_uppercase();\n        let method = match reqwest::Method::from_bytes(http_method_str.as_bytes()) {\n            Ok(m) =\u003e m,\n            Err(_) =\u003e {\n                return JobResult::Fail(WebhookError::UnsupportedHttpMethod(http_method_str));\n            }\n        };\n\n        let request_builder = ec\n            .http_client\n            .request(method, \u0026payload.url)\n            .headers(request_headers)\n            .body(payload.body.clone());\n\n        tracing::debug!(\n            job_id = %job.id,\n            url = %payload.url,\n            method = %http_method_str,\n            attempt = %job.attempts,\n            \"Sending webhook request\"\n        );\n\n        match request_builder.send().await {\n            Ok(response) =\u003e {\n                let status = response.status();\n                let response_body_text_result = response.text().await;\n\n                let response_body_text = match response_body_text_result {\n                    Ok(text) =\u003e Some(text),\n                    Err(e) =\u003e {\n                        if status.is_success() {\n                            let err = WebhookError::ResponseReadError(format!(\n                                \"Failed to read response body: {}\",\n                                e\n                            ));\n                            return JobResult::Fail(err);\n                        }\n                        tracing::warn!(job_id = %job.id, \"Failed to read response body for error status {}: {}\", status, e);\n                        None\n                    }\n                };\n\n                if status.is_success() {\n                    tracing::info!(job_id = %job.id, status = %status, \"Webhook delivered successfully\");\n                    JobResult::Success(WebhookJobOutput {\n                        status_code: status.as_u16(),\n                        response_body: response_body_text,\n                    })\n                } else {\n                    let error_body_preview = response_body_text\n                        .map(|s| {\n                            if s.len() \u003e 512 {\n                                format!(\"{}...\", \u0026s[..512])\n                            } else {\n                                s\n                            }\n                        })\n                        .unwrap_or_else(|| \"No body or failed to read body\".to_string());\n\n                    let webhook_error = WebhookError::Http {\n                        status: status.as_u16(),\n                        body_preview: error_body_preview,\n                    };\n\n                    if status.is_server_error() || status.as_u16() == 429 {\n                        if job.attempts \u003c ec.retry_config.max_attempts {\n                            let delay_ms = ec.retry_config.initial_delay_ms as f64\n                                * ec.retry_config.backoff_factor.powi(job.attempts as i32 - 1); // Use current attempts for backoff\n                            let delay_ms =\n                                (delay_ms.min(ec.retry_config.max_delay_ms as f64)) as u64;\n                            let delay = Duration::from_millis(delay_ms);\n\n                            tracing::warn!(\n                                job_id = %job.id,\n                                status = %status,\n                                attempt = %job.attempts,\n                                max_attempts = %ec.retry_config.max_attempts,\n                                delay_ms = %delay.as_millis(),\n                                \"Webhook failed with retryable status, NACKing.\"\n                            );\n                            JobResult::Nack {\n                                error: webhook_error,\n                                delay: Some(delay),\n                                position: RequeuePosition::Last,\n                            }\n                        } else {\n                            tracing::error!(\n                                job_id = %job.id,\n                                status = %status,\n                                attempt = %job.attempts,\n                                \"Webhook failed after max attempts, FAILING.\"\n                            );\n                            JobResult::Fail(webhook_error)\n                        }\n                    } else {\n                        tracing::error!(\n                            job_id = %job.id,\n                            status = %status,\n                            \"Webhook failed with non-retryable client error, FAILING.\"\n                        );\n                        JobResult::Fail(webhook_error)\n                    }\n                }\n            }\n            Err(e) =\u003e {\n                let webhook_error = if e.is_timeout() {\n                    WebhookError::Timeout(e.to_string())\n                } else if e.is_connect() || e.is_request() {\n                    WebhookError::Network(e.to_string())\n                } else {\n                    WebhookError::RequestConstruction(e.to_string())\n                };\n\n                if matches!(webhook_error, WebhookError::RequestConstruction(_))\n                    \u0026\u0026 !e.is_connect()\n                    \u0026\u0026 !e.is_timeout()\n                {\n                    tracing::error!(job_id = %job.id, error = %webhook_error, \"Webhook construction error, FAILING.\");\n                    return JobResult::Fail(webhook_error);\n                }\n\n                if job.attempts \u003c ec.retry_config.max_attempts {\n                    let delay_ms = ec.retry_config.initial_delay_ms as f64\n                        * ec.retry_config.backoff_factor.powi(job.attempts as i32 - 1); // Use current attempts for backoff\n                    let delay_ms = (delay_ms.min(ec.retry_config.max_delay_ms as f64)) as u64;\n                    let delay = Duration::from_millis(delay_ms);\n\n                    tracing::warn!(\n                        job_id = %job.id,\n                        error = %webhook_error,\n                        attempt = %job.attempts,\n                        max_attempts = %ec.retry_config.max_attempts,\n                        delay_ms = %delay.as_millis(),\n                        \"Webhook request failed, NACKing.\"\n                    );\n                    JobResult::Nack {\n                        error: webhook_error,\n                        delay: Some(delay),\n                        position: RequeuePosition::Last,\n                    }\n                } else {\n                    tracing::error!(\n                        job_id = %job.id,\n                        error = %webhook_error,\n                        attempt = %job.attempts,\n                        \"Webhook request failed after max attempts, FAILING.\"\n                    );\n                    JobResult::Fail(webhook_error)\n                }\n            }\n        }\n    }\n\n    async fn on_success(\n        \u0026self,\n        job: \u0026Job\u003cSelf\u003e,\n        d: SuccessHookData\u003c'_, Self::Output\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\n            job_id = %job.id,\n            url = %self.url,\n            status = %d.result.status_code,\n            \"Webhook successfully processed (on_success hook).\"\n        );\n    }\n\n    async fn on_nack(\n        \u0026self,\n        job: \u0026Job\u003cSelf\u003e,\n        d: NackHookData\u003c'_, Self::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::warn!(\n            job_id = %job.id,\n            url = %self.url,\n            attempt = %job.attempts,\n            error = ?d.error,\n            delay_ms = %d.delay.map_or(0, |dur| dur.as_millis()),\n            \"Webhook NACKed (on_nack hook).\"\n        );\n    }\n\n    async fn on_fail(\n        \u0026self,\n        job: \u0026Job\u003cSelf\u003e,\n        d: FailHookData\u003c'_, Self::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::error!(\n            job_id = %job.id,\n            url = %self.url,\n            attempt = %job.attempts,\n            error = ?d.error,\n            \"Webhook FAILED permanently (on_fail hook).\"\n        );\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","benches","throughput.rs"],"content":"use criterion::{BenchmarkId, Criterion, Throughput, criterion_group, criterion_main};\nuse rand::Rng;\nuse serde::{Deserialize, Serialize};\nuse std::hint::black_box;\nuse std::sync::{\n    Arc,\n    atomic::{AtomicU64, Ordering},\n};\nuse std::time::{Duration, SystemTime, UNIX_EPOCH};\nuse tokio::runtime::Runtime;\n\nuse twmq::{\n    DurableExecution, NackHookData, Queue, SuccessHookData,\n    hooks::TransactionContext,\n    job::{Job, JobResult, JobStatus, RequeuePosition},\n    queue::QueueOptions,\n};\n\nconst REDIS_URL: \u0026str = \"redis://127.0.0.1:6379/\";\n\n// Benchmark job that either succeeds immediately or nacks based on probability\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct BenchmarkJob {\n    pub job_id: String,\n    pub nack_probability: f64, // For metrics\n    pub created_at: u64,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct BenchmarkOutput {\n    pub job_id: String,\n    pub processed_at: u64,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct BenchmarkErrorData {\n    pub job_id: String,\n    pub attempt: u32,\n}\n\n// Shared metrics across all benchmark jobs\n#[derive(Clone)]\npub struct BenchmarkMetrics {\n    pub jobs_processed: Arc\u003cAtomicU64\u003e,\n    pub jobs_nacked: Arc\u003cAtomicU64\u003e,\n    pub jobs_succeeded: Arc\u003cAtomicU64\u003e,\n    pub total_processing_time_ms: Arc\u003cAtomicU64\u003e,\n}\n\nimpl BenchmarkMetrics {\n    fn new() -\u003e Self {\n        Self {\n            jobs_processed: Arc::new(AtomicU64::new(0)),\n            jobs_nacked: Arc::new(AtomicU64::new(0)),\n            jobs_succeeded: Arc::new(AtomicU64::new(0)),\n            total_processing_time_ms: Arc::new(AtomicU64::new(0)),\n        }\n    }\n\n    fn reset(\u0026self) {\n        self.jobs_processed.store(0, Ordering::SeqCst);\n        self.jobs_nacked.store(0, Ordering::SeqCst);\n        self.jobs_succeeded.store(0, Ordering::SeqCst);\n        self.total_processing_time_ms.store(0, Ordering::SeqCst);\n    }\n\n    fn processed_count(\u0026self) -\u003e u64 {\n        self.jobs_processed.load(Ordering::SeqCst)\n    }\n\n    fn success_rate(\u0026self) -\u003e f64 {\n        let succeeded = self.jobs_succeeded.load(Ordering::SeqCst) as f64;\n        let total = self.jobs_processed.load(Ordering::SeqCst) as f64;\n        if total \u003e 0.0 { succeeded / total } else { 0.0 }\n    }\n\n    fn avg_processing_time_ms(\u0026self) -\u003e f64 {\n        let total_time = self.total_processing_time_ms.load(Ordering::SeqCst) as f64;\n        let total_jobs = self.jobs_processed.load(Ordering::SeqCst) as f64;\n        if total_jobs \u003e 0.0 {\n            total_time / total_jobs\n        } else {\n            0.0\n        }\n    }\n}\n\nimpl DurableExecution for BenchmarkJob {\n    type Output = BenchmarkOutput;\n    type ErrorData = BenchmarkErrorData;\n    type ExecutionContext = BenchmarkMetrics;\n\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        metrics: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        let start_time = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_millis() as u64;\n\n        // Simulate minimal work (just increment counter)\n        metrics.jobs_processed.fetch_add(1, Ordering::SeqCst);\n\n        let end_time = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_millis() as u64;\n        let processing_time = end_time - start_time;\n        metrics\n            .total_processing_time_ms\n            .fetch_add(processing_time, Ordering::SeqCst);\n\n        // Fresh random decision each processing attempt\n        if rand::thread_rng().gen_bool(job.data.nack_probability) {\n            metrics.jobs_nacked.fetch_add(1, Ordering::SeqCst);\n\n            // Random position for nacks as requested\n            let position = if rand::thread_rng().gen_bool(0.5) {\n                RequeuePosition::First\n            } else {\n                RequeuePosition::Last\n            };\n\n            JobResult::Nack {\n                error: BenchmarkErrorData {\n                    job_id: job.data.job_id.clone(),\n                    attempt: job.attempts,\n                },\n                delay: None, // No delay as requested\n                position,\n            }\n        } else {\n            metrics.jobs_succeeded.fetch_add(1, Ordering::SeqCst);\n\n            JobResult::Success(BenchmarkOutput {\n                job_id: job.data.job_id.clone(),\n                processed_at: end_time,\n            })\n        }\n    }\n\n    async fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        _d: SuccessHookData\u003c'_, Self::Output\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _metrics: \u0026Self::ExecutionContext,\n    ) {\n        // Keep hooks minimal for max performance\n    }\n\n    async fn on_nack(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        _d: NackHookData\u003c'_, Self::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _metrics: \u0026Self::ExecutionContext,\n    ) {\n        // Keep hooks minimal for max performance\n    }\n\n    async fn on_timeout(\u0026self, _tx: \u0026mut TransactionContext\u003c'_\u003e) {\n        // Minimal timeout handling\n    }\n}\n\n// Load test that finds maximum sustainable throughput\nasync fn load_test_throughput(\n    jobs_per_second: usize,\n    duration_seconds: u64,\n    nack_percentage: f64,\n) -\u003e (u64, f64, f64, bool) {\n    let test_id = nanoid::nanoid!(8);\n    let queue_name = format!(\"bench_queue_{}\", test_id);\n\n    let metrics = BenchmarkMetrics::new();\n\n    // Optimize queue for high throughput\n    let queue_options = QueueOptions {\n        local_concurrency: 200,                      // High concurrency\n        polling_interval: Duration::from_millis(10), // Fast polling\n        always_poll: true,                           // Always poll for max responsiveness\n        lease_duration: Duration::from_secs(30),     // Reasonable lease time\n        max_success: 10000,                          // Large success queue\n        max_failed: 1000,                            // Reasonable failed queue\n    };\n\n    let queue = Arc::new(\n        Queue::\u003cBenchmarkJob, BenchmarkOutput, BenchmarkErrorData, BenchmarkMetrics\u003e::new(\n            REDIS_URL,\n            \u0026queue_name,\n            Some(queue_options),\n            metrics.clone(),\n        )\n        .await\n        .expect(\"Failed to create benchmark queue\"),\n    );\n\n    // Clean up any existing data\n    let mut redis_conn = queue.redis.clone();\n    let keys: Vec\u003cString\u003e = redis::cmd(\"KEYS\")\n        .arg(format!(\"{}:*\", queue_name))\n        .query_async(\u0026mut redis_conn)\n        .await\n        .unwrap_or_default();\n    if !keys.is_empty() {\n        let _: () = redis::cmd(\"DEL\")\n            .arg(keys)\n            .query_async(\u0026mut redis_conn)\n            .await\n            .unwrap_or(());\n    }\n\n    // Start workers\n    let worker_handle = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            let _ = queue.work().await;\n        })\n    };\n\n    // Job producer task\n    let producer_handle = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            let mut job_counter = 0u64;\n            let interval_duration = Duration::from_secs_f64(1.0 / jobs_per_second as f64);\n            let mut interval = tokio::time::interval(interval_duration);\n            interval.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Delay);\n\n            let start_time = SystemTime::now();\n\n            loop {\n                if start_time.elapsed().unwrap().as_secs() \u003e= duration_seconds {\n                    break;\n                }\n\n                interval.tick().await;\n\n                let job = BenchmarkJob {\n                    job_id: format!(\"job_{}\", job_counter),\n                    nack_probability: nack_percentage,\n                    created_at: SystemTime::now()\n                        .duration_since(UNIX_EPOCH)\n                        .unwrap()\n                        .as_secs(),\n                };\n\n                if queue\n                    .clone()\n                    .job(job)\n                    .with_id(\u0026format!(\"job_{}\", job_counter))\n                    .push()\n                    .await\n                    .is_ok()\n                {\n                    job_counter += 1;\n                }\n            }\n\n            job_counter\n        })\n    };\n\n    // Monitor queue depth\n    let mut max_queue_depth = 0usize;\n    let monitor_handle = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            let start_time = SystemTime::now();\n            while start_time.elapsed().unwrap().as_secs() \u003c duration_seconds + 5 {\n                let pending = queue.count(JobStatus::Pending).await.unwrap_or(0);\n                let active = queue.count(JobStatus::Active).await.unwrap_or(0);\n                let depth = pending + active;\n                max_queue_depth = max_queue_depth.max(depth);\n                tokio::time::sleep(Duration::from_millis(100)).await;\n            }\n            max_queue_depth\n        })\n    };\n\n    // Wait for test completion\n    let jobs_pushed = producer_handle.await.unwrap();\n\n    // Give some extra time for jobs to finish processing\n    tokio::time::sleep(Duration::from_secs(2)).await;\n\n    let max_depth = monitor_handle.await.unwrap();\n\n    // Check if queue is sustainable (not building up)\n    let final_pending = queue.count(JobStatus::Pending).await.unwrap_or(0);\n    let final_active = queue.count(JobStatus::Active).await.unwrap_or(0);\n    let is_sustainable = (final_pending + final_active) \u003c jobs_per_second / 2; // Heuristic: backlog \u003c 0.5 seconds of work\n\n    let total_processed = metrics.processed_count();\n    let success_rate = metrics.success_rate();\n    let avg_processing_time = metrics.avg_processing_time_ms();\n\n    println!(\n        \"Load Test Results - {}jobs/s for {}s:\",\n        jobs_per_second, duration_seconds\n    );\n    println!(\"  Jobs pushed: {}\", jobs_pushed);\n    println!(\"  Jobs processed: {}\", total_processed);\n    println!(\"  Success rate: {:.1}%\", success_rate * 100.0);\n    println!(\"  Avg processing time: {:.2}ms\", avg_processing_time);\n    println!(\"  Max queue depth: {}\", max_depth);\n    println!(\"  Final backlog: {}\", final_pending + final_active);\n    println!(\"  Sustainable: {}\", is_sustainable);\n\n    // Cleanup\n    worker_handle.abort();\n    let _: () = redis::cmd(\"DEL\")\n        .arg(format!(\"{}:*\", queue_name))\n        .query_async(\u0026mut redis_conn)\n        .await\n        .unwrap_or(());\n\n    (\n        total_processed,\n        success_rate,\n        avg_processing_time,\n        is_sustainable,\n    )\n}\n\n// Benchmark that finds maximum sustainable throughput\nfn find_max_throughput(c: \u0026mut Criterion) {\n    let rt = Runtime::new().unwrap();\n\n    let mut group = c.benchmark_group(\"twmq_max_throughput\");\n    group.measurement_time(Duration::from_secs(30));\n    group.sample_size(10);\n\n    // Test different throughput levels to find the limit\n    let throughput_levels = vec![50_000, 100_000, 120_000, 150_000, 200_000, 250_000];\n\n    for \u0026jobs_per_second in \u0026throughput_levels {\n        group.throughput(Throughput::Elements(jobs_per_second as u64));\n\n        group.bench_with_input(\n            BenchmarkId::new(\"sustainable_throughput\", jobs_per_second),\n            \u0026jobs_per_second,\n            |b, \u0026jobs_per_second| {\n                b.to_async(\u0026rt).iter(|| async {\n                    let (processed, success_rate, avg_time, sustainable) =\n                        load_test_throughput(jobs_per_second, 10, 0.1).await;\n\n                    // Return metrics for Criterion\n                    black_box((processed, success_rate, avg_time, sustainable))\n                });\n            },\n        );\n    }\n\n    group.finish();\n}\n\n// Benchmark different nack percentages\nfn nack_percentage_impact(c: \u0026mut Criterion) {\n    let rt = Runtime::new().unwrap();\n\n    let mut group = c.benchmark_group(\"twmq_nack_impact\");\n    group.measurement_time(Duration::from_secs(20));\n\n    let nack_percentages = vec![0.0, 0.05, 0.1, 0.2, 0.3, 0.5];\n\n    for \u0026nack_pct in \u0026nack_percentages {\n        group.bench_with_input(\n            BenchmarkId::new(\"nack_percentage\", (nack_pct * 100.0) as u32),\n            \u0026nack_pct,\n            |b, \u0026nack_pct| {\n                b.to_async(\u0026rt).iter(|| async move {\n                    let (processed, success_rate, avg_time, sustainable) =\n                        load_test_throughput(5000, 8, nack_pct).await;\n\n                    black_box((processed, success_rate, avg_time, sustainable))\n                });\n            },\n        );\n    }\n\n    group.finish();\n}\n\n// Quick smoke test benchmark\nfn basic_throughput(c: \u0026mut Criterion) {\n    let rt = Runtime::new().unwrap();\n\n    let mut group = c.benchmark_group(\"twmq_basic_1k_jobs\");\n    group.sample_size(10); // ← Only 5 samples instead of 100\n\n    group.bench_function(\"1k_jobs\", |b| {\n        b.to_async(\u0026rt).iter(|| async {\n            let (processed, success_rate, avg_time, sustainable) =\n                load_test_throughput(1000, 5, 0.1).await;\n\n            black_box((processed, success_rate, avg_time, sustainable))\n        });\n    });\n\n    group.finish();\n}\n\ncriterion_group!(\n    benches,\n    basic_throughput,\n    find_max_throughput,\n    nack_percentage_impact\n);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","src","error.rs"],"content":"#[derive(thiserror::Error, Debug)]\npub enum TwmqError {\n    #[error(\"Redis error: {0}\")]\n    RedisError(#[from] redis::RedisError),\n\n    #[error(\"JSON Serialization error: {0}\")]\n    JsonError(#[from] serde_json::Error),\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","src","hooks.rs"],"content":"use std::time::{SystemTime, UNIX_EPOCH};\n\nuse serde::{Serialize, de::DeserializeOwned};\n\nuse crate::{DurableExecution, error::TwmqError, job::JobBuilder};\n\n// A minimal transaction context that hooks can use\npub struct TransactionContext\u003c'a\u003e {\n    // Redis pipeline for arbitrary commands\n    pipeline: \u0026'a mut redis::Pipeline,\n    // Queue name for context\n    queue_name: String,\n}\n\nimpl\u003c'a\u003e TransactionContext\u003c'a\u003e {\n    pub fn new(pipeline: \u0026'a mut redis::Pipeline, queue_name: String) -\u003e Self {\n        Self {\n            pipeline,\n            queue_name,\n        }\n    }\n\n    // Return pipeline for arbitrary commands\n    pub fn pipeline(\u0026mut self) -\u003e \u0026mut redis::Pipeline {\n        self.pipeline\n    }\n\n    // Queue name accessor\n    pub fn queue_name(\u0026self) -\u003e \u0026str {\n        \u0026self.queue_name\n    }\n\n    // Helper for job scheduling (maintains type safety)\n    pub fn queue_job\u003cT, R, E, C\u003e(\n        \u0026mut self,\n        job_builder: JobBuilder\u003cT, R, E, C\u003e,\n    ) -\u003e Result\u003c\u0026mut Self, TwmqError\u003e\n    where\n        T: Serialize\n            + DeserializeOwned\n            + Send\n            + Sync\n            + DurableExecution\u003cOutput = R, ErrorData = E, ExecutionContext = C\u003e\n            + 'static,\n        R: Serialize + DeserializeOwned + Send + Sync + 'static,\n        E: Serialize + DeserializeOwned + Send + Sync + 'static,\n        C: Send + Clone + Sync + 'static,\n    {\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n        let job_data = serde_json::to_string(\u0026job_builder.options.data)?;\n\n        // Add job to the target queue (could be different from current queue!)\n        self.pipeline\n            .hset(\n                job_builder.queue.job_data_hash_name(),\n                \u0026job_builder.options.id,\n                job_data,\n            )\n            .hset(\n                job_builder\n                    .queue\n                    .job_meta_hash_name(\u0026job_builder.options.id),\n                \"created_at\",\n                now,\n            )\n            .hset(\n                job_builder\n                    .queue\n                    .job_meta_hash_name(\u0026job_builder.options.id),\n                \"attempts\",\n                0,\n            )\n            .sadd(job_builder.queue.dedupe_set_name(), \u0026job_builder.options.id);\n\n        if let Some(delay_options) = job_builder.options.delay {\n            let process_at = now + delay_options.delay.as_secs();\n            self.pipeline\n                .hset(\n                    job_builder\n                        .queue\n                        .job_meta_hash_name(\u0026job_builder.options.id),\n                    \"reentry_position\",\n                    delay_options.position.to_string(),\n                )\n                .zadd(\n                    job_builder.queue.delayed_zset_name(),\n                    \u0026job_builder.options.id,\n                    process_at,\n                );\n        } else {\n            self.pipeline.rpush(\n                job_builder.queue.pending_list_name(),\n                \u0026job_builder.options.id,\n            );\n        }\n\n        Ok(self)\n    }\n}\n","traces":[{"line":16,"address":[],"length":0,"stats":{"Line":29}},{"line":24,"address":[],"length":0,"stats":{"Line":37}},{"line":25,"address":[],"length":0,"stats":{"Line":37}},{"line":29,"address":[],"length":0,"stats":{"Line":0}},{"line":30,"address":[],"length":0,"stats":{"Line":0}},{"line":34,"address":[],"length":0,"stats":{"Line":1}},{"line":49,"address":[],"length":0,"stats":{"Line":1}},{"line":50,"address":[],"length":0,"stats":{"Line":1}},{"line":53,"address":[],"length":0,"stats":{"Line":2}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":60,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[],"length":0,"stats":{"Line":0}},{"line":65,"address":[],"length":0,"stats":{"Line":0}},{"line":67,"address":[],"length":0,"stats":{"Line":0}},{"line":70,"address":[],"length":0,"stats":{"Line":0}},{"line":71,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":76,"address":[],"length":0,"stats":{"Line":0}},{"line":78,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":80,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[],"length":0,"stats":{"Line":0}},{"line":83,"address":[],"length":0,"stats":{"Line":0}},{"line":84,"address":[],"length":0,"stats":{"Line":0}},{"line":86,"address":[],"length":0,"stats":{"Line":0}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":91,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[],"length":0,"stats":{"Line":1}},{"line":95,"address":[],"length":0,"stats":{"Line":1}},{"line":96,"address":[],"length":0,"stats":{"Line":1}},{"line":100,"address":[],"length":0,"stats":{"Line":0}}],"covered":10,"coverable":35},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","src","job.rs"],"content":"use crate::{DurableExecution, Queue, error::TwmqError};\nuse nanoid::nanoid;\nuse serde::{Deserialize, Serialize, de::DeserializeOwned};\nuse std::{fmt::Display, sync::Arc, time::Duration};\n\n// Position for nack operations\n#[derive(Debug, Clone, Copy, Serialize, Deserialize)]\npub enum RequeuePosition {\n    #[serde(rename = \"first\")]\n    First,\n    #[serde(rename = \"last\")]\n    Last,\n}\n\nimpl Display for RequeuePosition {\n    fn fmt(\u0026self, f: \u0026mut std::fmt::Formatter\u003c'_\u003e) -\u003e std::fmt::Result {\n        match self {\n            RequeuePosition::First =\u003e write!(f, \"first\"),\n            RequeuePosition::Last =\u003e write!(f, \"last\"),\n        }\n    }\n}\n\nimpl TryFrom\u003c\u0026str\u003e for RequeuePosition {\n    type Error = ();\n\n    fn try_from(value: \u0026str) -\u003e Result\u003cSelf, Self::Error\u003e {\n        match value {\n            \"first\" =\u003e Ok(RequeuePosition::First),\n            \"last\" =\u003e Ok(RequeuePosition::Last),\n            _ =\u003e Err(()),\n        }\n    }\n}\n\n#[derive(Debug, Clone, Copy, Serialize, Deserialize)]\npub struct DelayOptions {\n    pub delay: Duration,\n    pub position: RequeuePosition,\n}\n\n// Job result type\npub enum JobResult\u003cT, E\u003e {\n    Success(T),\n    Nack {\n        error: E,\n        delay: Option\u003cDuration\u003e,\n        position: RequeuePosition,\n    },\n    Fail(E),\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RequeueOptions {\n    pub delay: Option\u003cDuration\u003e,\n    pub position: RequeuePosition,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum JobErrorType {\n    #[serde(rename = \"nack\")]\n    Nack(RequeueOptions),\n    #[serde(rename = \"fail\")]\n    Fail,\n}\n\nimpl JobErrorType {\n    pub fn nack(delay: Option\u003cDuration\u003e, position: RequeuePosition) -\u003e Self {\n        Self::Nack(RequeueOptions { delay, position })\n    }\n\n    pub fn fail() -\u003e Self {\n        Self::Fail\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct JobErrorRecord\u003cE\u003e {\n    pub error: E,\n    pub attempt: u32,\n    pub details: JobErrorType,\n    pub created_at: u64,\n}\n\n// Job structure\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Job\u003cT\u003e {\n    pub id: String,\n    pub data: T,\n    pub attempts: u32,\n    pub created_at: u64,\n    pub processed_at: Option\u003cu64\u003e,\n    pub finished_at: Option\u003cu64\u003e,\n}\n\n// Job status enum\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum JobStatus {\n    Pending,\n    Active,\n    Delayed,\n    Success,\n    Failed,\n}\n\npub struct JobOptions\u003cT\u003e\nwhere\n    T: Serialize + DeserializeOwned + Send + Sync + 'static,\n{\n    pub data: T,\n    pub id: String,\n    pub delay: Option\u003cDelayOptions\u003e,\n}\n\nimpl\u003cT\u003e JobOptions\u003cT\u003e\nwhere\n    T: Serialize + DeserializeOwned + Send + Sync + 'static,\n{\n    pub fn new(data: T) -\u003e Self {\n        Self {\n            data,\n            id: nanoid!(),\n            delay: None,\n        }\n    }\n\n    // Set custom ID (for deduplication)\n    pub fn with_id(mut self, id: impl Into\u003cString\u003e) -\u003e Self {\n        self.id = id.into();\n        self\n    }\n\n    // Set delay\n    pub fn with_delay(mut self, delay: DelayOptions) -\u003e Self {\n        self.delay = Some(delay);\n        self\n    }\n}\n\npub struct JobBuilder\u003cT, R, E, C\u003e\nwhere\n    T: Serialize\n        + DeserializeOwned\n        + Send\n        + Sync\n        + 'static\n        + DurableExecution\u003cExecutionContext = C, Output = R, ErrorData = E\u003e,\n    R: Serialize + DeserializeOwned + Send + Sync + 'static,\n    E: Serialize + DeserializeOwned + Send + Sync + 'static,\n    C: Send + Sync + 'static,\n{\n    pub options: JobOptions\u003cT\u003e,\n    pub queue: Arc\u003cQueue\u003cT, R, E, C\u003e\u003e,\n}\n\nimpl\u003cT, R, E, C\u003e JobBuilder\u003cT, R, E, C\u003e\nwhere\n    T: Serialize\n        + DeserializeOwned\n        + DurableExecution\u003cOutput = R, ErrorData = E, ExecutionContext = C\u003e\n        + Send\n        + Sync\n        + 'static,\n    R: Serialize + DeserializeOwned + Send + Sync + 'static,\n    E: Serialize + DeserializeOwned + Send + Sync + 'static,\n    C: Send + Sync + Clone + 'static,\n{\n    pub async fn push(self) -\u003e Result\u003cJob\u003cT\u003e, TwmqError\u003e {\n        self.queue.push(self.options).await\n    }\n\n    pub fn with_id(mut self, id: impl Into\u003cString\u003e) -\u003e Self {\n        self.options.id = id.into();\n        self\n    }\n\n    pub fn with_delay(mut self, delay: DelayOptions) -\u003e Self {\n        self.options.delay = Some(delay);\n        self\n    }\n}\n","traces":[{"line":16,"address":[],"length":0,"stats":{"Line":16}},{"line":17,"address":[],"length":0,"stats":{"Line":16}},{"line":18,"address":[],"length":0,"stats":{"Line":1}},{"line":19,"address":[],"length":0,"stats":{"Line":15}},{"line":27,"address":[],"length":0,"stats":{"Line":0}},{"line":28,"address":[],"length":0,"stats":{"Line":0}},{"line":29,"address":[],"length":0,"stats":{"Line":0}},{"line":30,"address":[],"length":0,"stats":{"Line":0}},{"line":31,"address":[],"length":0,"stats":{"Line":0}},{"line":68,"address":[],"length":0,"stats":{"Line":16}},{"line":69,"address":[],"length":0,"stats":{"Line":16}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":73,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":16}},{"line":122,"address":[],"length":0,"stats":{"Line":16}},{"line":128,"address":[],"length":0,"stats":{"Line":0}},{"line":129,"address":[],"length":0,"stats":{"Line":0}},{"line":130,"address":[],"length":0,"stats":{"Line":0}},{"line":134,"address":[],"length":0,"stats":{"Line":0}},{"line":135,"address":[],"length":0,"stats":{"Line":0}},{"line":136,"address":[],"length":0,"stats":{"Line":0}},{"line":168,"address":[],"length":0,"stats":{"Line":30}},{"line":169,"address":[],"length":0,"stats":{"Line":15}},{"line":172,"address":[],"length":0,"stats":{"Line":15}},{"line":173,"address":[],"length":0,"stats":{"Line":15}},{"line":174,"address":[],"length":0,"stats":{"Line":15}},{"line":177,"address":[],"length":0,"stats":{"Line":3}},{"line":178,"address":[],"length":0,"stats":{"Line":3}},{"line":179,"address":[],"length":0,"stats":{"Line":3}}],"covered":16,"coverable":29},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","src","lib.rs"],"content":"pub mod error;\npub mod hooks;\npub mod job;\npub mod queue;\n\nuse std::marker::PhantomData;\nuse std::sync::Arc;\nuse std::time::{Duration, SystemTime, UNIX_EPOCH};\n\nuse error::TwmqError;\nuse hooks::TransactionContext;\nuse job::{\n    DelayOptions, Job, JobBuilder, JobErrorRecord, JobErrorType, JobOptions, JobResult, JobStatus,\n    RequeuePosition,\n};\nuse queue::QueueOptions;\nuse redis::Pipeline;\nuse redis::{AsyncCommands, RedisResult, aio::ConnectionManager};\nuse serde::{Serialize, de::DeserializeOwned};\nuse tokio::sync::Semaphore;\nuse tokio::time::sleep;\n\npub use redis;\nuse tracing::Instrument;\n\npub struct SuccessHookData\u003c'a, O\u003e {\n    pub result: \u0026'a O,\n}\n\npub struct NackHookData\u003c'a, E\u003e {\n    pub error: \u0026'a E,\n    pub delay: Option\u003cDuration\u003e,\n    pub position: RequeuePosition,\n}\n\npub struct FailHookData\u003c'a, E\u003e {\n    pub error: \u0026'a E,\n}\n\n// Main DurableExecution trait\npub trait DurableExecution: Sized {\n    type Output: Serialize + DeserializeOwned + Send + Sync;\n    type ErrorData: Serialize + DeserializeOwned + Send + Sync;\n    type ExecutionContext: Send + Sync + 'static;\n\n    // Required method to process a job\n    fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        c: \u0026Self::ExecutionContext,\n    ) -\u003e impl Future\u003cOutput = JobResult\u003cSelf::Output, Self::ErrorData\u003e\u003e + Send + Sync;\n\n    fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        _d: SuccessHookData\u003cSelf::Output\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _c: \u0026Self::ExecutionContext,\n    ) -\u003e impl Future\u003cOutput = ()\u003e + Send + Sync {\n        std::future::ready(())\n    }\n\n    fn on_nack(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        _d: NackHookData\u003cSelf::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _c: \u0026Self::ExecutionContext,\n    ) -\u003e impl Future\u003cOutput = ()\u003e + Send + Sync {\n        std::future::ready(())\n    }\n\n    fn on_fail(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        _d: FailHookData\u003cSelf::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _c: \u0026Self::ExecutionContext,\n    ) -\u003e impl Future\u003cOutput = ()\u003e + Send + Sync {\n        std::future::ready(())\n    }\n\n    fn on_timeout(\n        \u0026self,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n    ) -\u003e impl Future\u003cOutput = ()\u003e + Send + Sync {\n        std::future::ready(())\n    }\n}\n\n// Main Queue struct\npub struct Queue\u003cT, R, E, C\u003e\nwhere\n    T: Serialize\n        + DeserializeOwned\n        + Send\n        + Sync\n        + 'static\n        + DurableExecution\u003cExecutionContext = C, Output = R, ErrorData = E\u003e,\n    R: Serialize + DeserializeOwned + Send + Sync + 'static,\n    E: Serialize + DeserializeOwned + Send + Sync + 'static,\n    C: Send + Sync + 'static,\n{\n    pub redis: ConnectionManager,\n    options: QueueOptions,\n    // concurrency: usize,\n    name: String,\n    execution_context: C,\n    _phantom: PhantomData\u003c(T, R, E)\u003e,\n}\n\nimpl\u003cT, R, E, C\u003e Queue\u003cT, R, E, C\u003e\nwhere\n    C: Send + Sync + Clone + 'static,\n    T: Serialize\n        + DeserializeOwned\n        + Send\n        + Sync\n        + DurableExecution\u003cOutput = R, ErrorData = E, ExecutionContext = C\u003e\n        + 'static,\n    R: Serialize + DeserializeOwned + Send + Sync + 'static,\n    E: Serialize + DeserializeOwned + Send + Sync + 'static,\n{\n    pub async fn new(\n        redis_url: \u0026str,\n        name: \u0026str,\n        // concurrency: usize,\n        options: Option\u003cQueueOptions\u003e,\n        context: C,\n    ) -\u003e Result\u003cSelf, TwmqError\u003e {\n        let client = redis::Client::open(redis_url)?;\n        let redis = client.get_connection_manager().await?;\n\n        let queue = Self {\n            redis,\n            name: name.to_string(),\n            // concurrency,\n            options: options.unwrap_or_default(),\n            execution_context: context,\n            _phantom: PhantomData,\n        };\n\n        Ok(queue)\n    }\n\n    pub fn job(self: Arc\u003cSelf\u003e, data: T) -\u003e JobBuilder\u003cT, R, E, C\u003e {\n        JobBuilder {\n            options: JobOptions::new(data),\n            queue: self,\n        }\n    }\n\n    // Get queue name\n    pub fn name(\u0026self) -\u003e \u0026str {\n        \u0026self.name\n    }\n\n    pub fn pending_list_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:pending\", self.name())\n    }\n\n    pub fn active_hash_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:active\", self.name)\n    }\n\n    pub fn delayed_zset_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:delayed\", self.name)\n    }\n\n    pub fn success_list_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:success\", self.name)\n    }\n\n    pub fn failed_list_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:failed\", self.name)\n    }\n\n    pub fn job_data_hash_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:jobs:data\", self.name)\n    }\n\n    pub fn job_meta_hash_name(\u0026self, job_id: \u0026str) -\u003e String {\n        format!(\"twmq:{}:job:{}:meta\", self.name, job_id)\n    }\n\n    pub fn job_errors_list_name(\u0026self, job_id: \u0026str) -\u003e String {\n        format!(\"twmq:{}:job:{}:errors\", self.name, job_id)\n    }\n\n    pub fn job_result_hash_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:jobs:result\", self.name)\n    }\n\n    pub fn dedupe_set_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:dedup\", self.name)\n    }\n\n    pub async fn push(\u0026self, job_options: JobOptions\u003cT\u003e) -\u003e Result\u003cJob\u003cT\u003e, TwmqError\u003e {\n        // Check for duplicates and handle job creation with deduplication\n        let script = redis::Script::new(\n            r#\"\n            local job_id = ARGV[1]\n            local job_data = ARGV[2]\n            local now = ARGV[3]\n            local delay = ARGV[4]\n            local reentry_position = ARGV[5]  -- \"first\" or \"last\"\n\n            local queue_id = KEYS[1]\n            local delayed_zset_name = KEYS[2]\n            local pending_list_name = KEYS[3]\n\n            local job_data_hash_name = KEYS[4]\n            local job_meta_hash_name = KEYS[5]\n\n            local dedupe_set_name = KEYS[6]\n\n            -- Check if job already exists in any queue\n            if redis.call('SISMEMBER', dedupe_set_name, job_id) == 1 then\n                -- Job with this ID already exists, skip\n                return { 0, job_id }\n            end\n\n            -- Store job data\n            redis.call('HSET', job_data_hash_name, job_id, job_data)\n\n            -- Store job metadata as a hash\n            redis.call('HSET', job_meta_hash_name, 'created_at', now)\n            redis.call('HSET', job_meta_hash_name, 'attempts', 0)\n\n            -- Add to deduplication set\n            redis.call('SADD', dedupe_set_name, job_id)\n\n            -- Add to appropriate queue based on delay\n            if tonumber(delay) \u003e 0 then\n                local process_at = now + tonumber(delay)\n                -- Store position information for this delayed job\n                redis.call('HSET', job_meta_hash_name, 'reentry_position', reentry_position)\n                redis.call('ZADD', delayed_zset_name, process_at, job_id)\n            else\n                -- Non-delayed job always goes to end of pending\n                redis.call('RPUSH', pending_list_name, job_id)\n            end\n\n            return { 1, job_id }\n            \"#,\n        );\n\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        let job = Job {\n            id: job_options.id.clone(),\n            data: job_options.data,\n            attempts: 0,\n            created_at: now,\n            processed_at: None,\n            finished_at: None,\n        };\n\n        let job_data = serde_json::to_string(\u0026job.data)?;\n\n        let delay = job_options.delay.unwrap_or(DelayOptions {\n            delay: Duration::ZERO,\n            position: RequeuePosition::Last,\n        });\n\n        let delay_secs = delay.delay.as_secs();\n        let position_string = delay.position.to_string();\n\n        let _result: (i32, String) = script\n            .key(\u0026self.name)\n            .key(self.delayed_zset_name())\n            .key(self.pending_list_name())\n            .key(self.job_data_hash_name())\n            .key(self.job_meta_hash_name(\u0026job.id))\n            .key(self.dedupe_set_name())\n            .arg(job_options.id)\n            .arg(job_data)\n            .arg(now)\n            .arg(delay_secs)\n            .arg(position_string)\n            .invoke_async(\u0026mut self.redis.clone())\n            .await?;\n\n        // Return job_id whether new or existing\n        Ok(job)\n    }\n\n    pub async fn get_job(\u0026self, job_id: \u0026str) -\u003e Result\u003cOption\u003cJob\u003cT\u003e\u003e, TwmqError\u003e {\n        let mut conn = self.redis.clone();\n        let job_data_t_json: Option\u003cString\u003e = conn.hget(self.job_data_hash_name(), job_id).await?;\n\n        if let Some(data_json) = job_data_t_json {\n            let data_t: T = serde_json::from_str(\u0026data_json)?;\n\n            // Fetch metadata\n            let meta_map: std::collections::HashMap\u003cString, String\u003e =\n                conn.hgetall(self.job_meta_hash_name(job_id)).await?;\n\n            let attempts: u32 = meta_map\n                .get(\"attempts\")\n                .and_then(|s| s.parse().ok())\n                .unwrap_or(0);\n            let created_at: u64 = meta_map\n                .get(\"created_at\")\n                .and_then(|s| s.parse().ok())\n                .unwrap_or(0); // Consider a more robust default or error\n            let processed_at: Option\u003cu64\u003e =\n                meta_map.get(\"processed_at\").and_then(|s| s.parse().ok());\n            let finished_at: Option\u003cu64\u003e = meta_map.get(\"finished_at\").and_then(|s| s.parse().ok());\n            // reentry_position is also in meta if needed for display\n\n            Ok(Some(Job {\n                id: job_id.to_string(),\n                data: data_t,\n                attempts,\n                created_at,\n                processed_at,\n                finished_at,\n            }))\n        } else {\n            Ok(None)\n        }\n    }\n\n    pub async fn count(\u0026self, status: JobStatus) -\u003e Result\u003cusize, TwmqError\u003e {\n        let mut conn = self.redis.clone();\n\n        let count = match status {\n            JobStatus::Pending =\u003e {\n                let count: usize = conn.llen(self.pending_list_name()).await?;\n                count\n            }\n            JobStatus::Active =\u003e {\n                let count: usize = conn.hlen(self.active_hash_name()).await?;\n                count\n            }\n            JobStatus::Delayed =\u003e {\n                let count: usize = conn.zcard(self.delayed_zset_name()).await?;\n                count\n            }\n            JobStatus::Success =\u003e {\n                let count: usize = conn.llen(self.success_list_name()).await?;\n                count\n            }\n            JobStatus::Failed =\u003e {\n                let count: usize = conn.llen(self.failed_list_name()).await?;\n                count\n            }\n        };\n\n        Ok(count)\n    }\n\n    pub async fn work(self: Arc\u003cSelf\u003e) -\u003e Result\u003c(), TwmqError\u003e {\n        // Local semaphore to limit concurrency per instance\n        let semaphore = Arc::new(Semaphore::new(self.options.local_concurrency));\n\n        // Start worker\n        tokio::spawn(async move {\n            let mut interval = tokio::time::interval(self.options.polling_interval);\n\n            loop {\n                interval.tick().await;\n\n                // Check available permits for batch size\n                let available_permits = semaphore.available_permits();\n                if available_permits == 0 \u0026\u0026 !self.options.always_poll {\n                    tracing::debug!(\"No permits available, waiting...\");\n                    continue;\n                }\n\n                tracing::debug!(\"Available permits: {}\", available_permits);\n                // Try to get multiple jobs - as many as we have permits\n                match self.pop_batch_jobs(available_permits).await {\n                    Ok(jobs) =\u003e {\n                        tracing::debug!(\"Got {} jobs\", jobs.len());\n                        for job in jobs {\n                            let permit = semaphore.clone().acquire_owned().await.unwrap();\n                            let queue_clone = self.clone();\n                            let job_id = job.id.clone();\n                            let execution_context = self.execution_context.clone();\n\n                            tokio::spawn(async move {\n                                // Process job - note we don't pass a context here\n                                let result = DurableExecution::process(\u0026job, \u0026execution_context).await;\n\n                                // Mark job as complete (automatically happens in completion handlers)\n\n                                match result {\n                                    JobResult::Success(output) =\u003e {\n                                        // Create transaction pipeline for atomicity\n                                        let mut pipeline = redis::pipe();\n                                        pipeline.atomic(); // Use MULTI/EXEC\n\n                                        // Create transaction context with mutable access to pipeline\n                                        let mut tx_context = TransactionContext::new(\n                                            \u0026mut pipeline,\n                                            queue_clone.name().to_string(),\n                                        );\n\n                                        let success_hook_data = SuccessHookData {\n                                            result: \u0026output,\n                                        };\n\n                                        // Call success hook to populate transaction context\n                                        job.data.on_success(\u0026job, success_hook_data, \u0026mut tx_context, \u0026execution_context).await;\n\n                                        // Complete job with success and execute transaction\n                                        if let Err(e) = queue_clone\n                                            .complete_job_success(\n                                                \u0026job,\n                                                \u0026output,\n                                                tx_context.pipeline(),\n                                            )\n                                            .await\n                                        {\n                                            tracing::error!(\n                                                \"Failed to complete job {} complete handling successfully: {:?}\",\n                                                job.id,\n                                                e\n                                            );\n                                        }\n                                    }\n                                    JobResult::Nack {\n                                        error,\n                                        delay,\n                                        position,\n                                    } =\u003e {\n                                        // Create transaction pipeline for atomicity\n                                        let mut pipeline = redis::pipe();\n                                        pipeline.atomic(); // Use MULTI/EXEC\n\n                                        // Create transaction context with mutable access to pipeline\n                                        let mut tx_context = TransactionContext::new(\n                                            \u0026mut pipeline,\n                                            queue_clone.name().to_string(),\n                                        );\n\n                                        let nack_hook_data = NackHookData {\n                                            error: \u0026error,\n                                            delay,\n                                            position,\n                                        };\n\n                                        // Call nack hook to populate transaction context\n                                        job.data\n                                            .on_nack(\u0026job, nack_hook_data, \u0026mut tx_context,\u0026execution_context)\n                                            .await;\n\n                                        // Complete job with nack and execute transaction\n                                        if let Err(e) = queue_clone\n                                            .complete_job_nack(\n                                                \u0026job,\n                                                \u0026error,\n                                                delay,\n                                                position,\n                                                tx_context.pipeline(),\n                                            )\n                                            .await\n                                        {\n                                            tracing::error!(\n                                                \"Failed to complete job {} complete nack handling successfully: {:?}\",\n                                                job.id,\n                                                e\n                                            );\n                                        }\n                                    }\n                                    JobResult::Fail(error) =\u003e {\n                                        // Create transaction pipeline for atomicity\n                                        let mut pipeline = redis::pipe();\n                                        pipeline.atomic(); // Use MULTI/EXEC\n\n                                        // Create transaction context with mutable access to pipeline\n                                        let mut tx_context = TransactionContext::new(\n                                            \u0026mut pipeline,\n                                            queue_clone.name.clone(),\n                                        );\n\n                                        let fail_hook_data = FailHookData {\n                                            error: \u0026error\n                                        };\n\n                                        // Call fail hook to populate transaction context\n                                        job.data.on_fail(\u0026job, fail_hook_data, \u0026mut tx_context, \u0026execution_context).await;\n\n                                        // Complete job with fail and execute transaction\n                                        if let Err(e) = queue_clone\n                                            .complete_job_fail(\u0026job, \u0026error, tx_context.pipeline())\n                                            .await\n                                        {\n                                            tracing::error!(\n                                                \"Failed to complete job {} complete fail handling successfully: {:?}\",\n                                                job.id,\n                                                e\n                                            );\n                                        }\n                                    }\n                                }\n\n                                // Release permit when done\n                                drop(permit);\n                            }.instrument(tracing::info_span!(\"twmq_worker\", job_id)));\n                        }\n                    }\n                    Err(e) =\u003e {\n                        // No jobs found, wait a bit\n                        tracing::error!(\"Failed to pop batch jobs: {:?}\", e);\n                        sleep(Duration::from_millis(1000)).await;\n                    }\n                };\n            }\n        });\n\n        Ok(())\n    }\n\n    // Improved batch job popping - gets multiple jobs at once\n    async fn pop_batch_jobs(\u0026self, batch_size: usize) -\u003e RedisResult\u003cVec\u003cJob\u003cT\u003e\u003e\u003e {\n        // Lua script that does:\n        // 1. Process expired delayed jobs\n        // 2. Check for timed out active jobs\n        // 3. Pop up to batch_size jobs from pending\n        let script = redis::Script::new(\n            r#\"\n            local now = tonumber(ARGV[1])\n            local batch_size = tonumber(ARGV[2])\n            local lease_seconds = tonumber(ARGV[3])\n\n            local queue_id = KEYS[1]\n            local delayed_zset_name = KEYS[2]\n            local pending_list_name = KEYS[3]\n            local active_hash_name = KEYS[4]\n            local job_data_hash_name = KEYS[5]\n\n\n            local result_jobs = {}\n\n            local timed_out_jobs = {}\n\n            -- Step 1: Clean up all expired leases\n            -- Get all active jobs\n            local active_jobs = redis.call('HGETALL', active_hash_name)\n\n            -- Process in pairs (job_id, lease_expiry)\n            for i = 1, #active_jobs, 2 do\n                local job_id = active_jobs[i]\n                local lease_expiry = tonumber(active_jobs[i + 1])\n                local job_meta_hash_name = 'twmq:' .. queue_id .. ':job:' .. job_id .. ':meta'\n\n                -- Check if lease has expired\n                if lease_expiry \u003c now then\n                    redis.call('HINCRBY', job_meta_hash_name, 'attempts', 1)\n\n                    -- Move job back to pending\n                    redis.call('HDEL', active_hash_name, job_id)\n                    redis.call('LPUSH', pending_list_name, job_id)\n\n                    -- Add to list of timed out jobs\n                    table.insert(timed_out_jobs, job_id)\n                end\n            end\n\n            -- Step 2: Move expired delayed jobs to pending\n            local delayed_jobs = redis.call('ZRANGEBYSCORE', delayed_zset_name, 0, now)\n            for i, job_id in ipairs(delayed_jobs) do\n                -- Check position information\n\n                local job_meta_hash_name = 'twmq:' .. queue_id .. ':job:' .. job_id .. ':meta'\n                local reentry_position = redis.call('HGET', job_meta_hash_name, 'reentry_position') or 'last'\n\n                -- Remove from delayed\n                redis.call('ZREM', delayed_zset_name, job_id)\n                redis.call('HDEL', job_meta_hash_name, 'reentry_position')\n\n                -- Add to pending based on position\n                if reentry_position == 'first' then\n                    redis.call('LPUSH', pending_list_name, job_id)\n                else\n                    redis.call('RPUSH', pending_list_name, job_id)\n                end\n            end\n\n            -- Finally Step 3: Try to pop jobs from pending (up to batch_size)\n            -- Try to pop jobs from pending (up to batch_size)\n            local popped_job_ids = {}\n            for i = 1, batch_size do\n                local job_id = redis.call('LPOP', pending_list_name)\n                if not job_id then\n                    break\n                end\n\n                table.insert(popped_job_ids, job_id)\n            end\n\n            local result_jobs = {}\n\n            -- Process popped jobs\n            for _, job_id in ipairs(popped_job_ids) do\n                -- Get job data\n                local job_data = redis.call('HGET', job_data_hash_name, job_id)\n\n                -- Only process if we have data\n                if job_data then\n                    -- Update metadata\n                    local job_meta_hash_name = 'twmq:' .. queue_id .. ':job:' .. job_id .. ':meta'\n\n\n                    redis.call('HSET', job_meta_hash_name, 'processed_at', now)\n                    local created_at = redis.call('HGET', job_meta_hash_name, 'created_at') or now\n                    local attempts = redis.call('HINCRBY', job_meta_hash_name, 'attempts', 1)\n\n                    -- Set lease expiration\n                    local lease_expiry = now + lease_seconds\n\n                    -- Add to active set with lease expiry as score\n                    redis.call('HSET', active_hash_name, job_id, lease_expiry)\n\n                    -- Add to result with both id and data\n                    table.insert(result_jobs, {job_id, job_data, tostring(attempts), tostring(created_at), tostring(now)})\n                end\n            end\n\n            return result_jobs\n            \"#,\n        );\n\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        let results_from_lua: Vec\u003c(String, String, String, String, String)\u003e = script\n            .key(self.name())\n            .key(self.delayed_zset_name())\n            .key(self.pending_list_name())\n            .key(self.active_hash_name())\n            .key(self.job_data_hash_name())\n            .arg(now)\n            .arg(batch_size)\n            .arg(self.options.lease_duration.as_secs())\n            .invoke_async(\u0026mut self.redis.clone())\n            .await?;\n\n        let mut jobs = Vec::new();\n        for (job_id_str, job_data_t_json, attempts_str, created_at_str, processed_at_str) in\n            results_from_lua\n        {\n            match serde_json::from_str::\u003cT\u003e(\u0026job_data_t_json) {\n                Ok(data_t) =\u003e {\n                    let attempts: u32 = attempts_str.parse().unwrap_or(1); // Default or handle error\n                    let created_at: u64 = created_at_str.parse().unwrap_or(now); // Default or handle error\n                    let processed_at: u64 = processed_at_str.parse().unwrap_or(now); // Default or handle error\n\n                    jobs.push(Job {\n                        id: job_id_str,\n                        data: data_t,\n                        attempts,\n                        created_at,\n                        processed_at: Some(processed_at),\n                        finished_at: None, // Not finished yet\n                    });\n                }\n                Err(e) =\u003e {\n                    // Log error: failed to deserialize job data T for job_id_str\n                    tracing::error!(\n                        \"Failed to deserialize job data for job {}: {}\",\n                        job_id_str,\n                        e\n                    );\n                    // TODO: Potentially move this job_id to a failed state or log for investigation\n                }\n            }\n        }\n\n        Ok(jobs)\n    }\n\n    async fn complete_job_success(\n        \u0026self,\n        job: \u0026Job\u003cT\u003e,\n        result: \u0026R,\n        pipeline: \u0026mut Pipeline,\n    ) -\u003e Result\u003c(), TwmqError\u003e {\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        // Add basic job completion operations to pipeline\n        pipeline\n            .hdel(self.active_hash_name(), \u0026job.id)\n            .lpush(self.success_list_name(), \u0026job.id)\n            // Set finished_at in the job's metadata hash\n            .hset(self.job_meta_hash_name(\u0026job.id), \"finished_at\", now);\n\n        let result_json = serde_json::to_string(result)?;\n        pipeline.hset(self.job_result_hash_name(), \u0026job.id, result_json);\n\n        // Execute main pipeline first\n        pipeline.query_async::\u003c()\u003e(\u0026mut self.redis.clone()).await?;\n\n        // Separate call for pruning with data deletion using Lua\n        let trim_script = redis::Script::new(\n            r#\"\n                local queue_id = KEYS[1]\n                local list_name = KEYS[2]\n                local job_data_hash = KEYS[3]\n                local results_hash = KEYS[4] -- e.g., \"myqueue:results\"\n                local max_len = tonumber(ARGV[1])\n\n                local job_ids_to_delete = redis.call('LRANGE', list_name, max_len, -1)\n\n                if #job_ids_to_delete \u003e 0 then\n                    for _, j_id in ipairs(job_ids_to_delete) do\n                        local job_meta_hash = 'twmq:' .. queue_id .. ':job:' .. j_id .. ':meta'\n                        local errors_list_name = 'twmq:' .. queue_id .. ':job:' .. j_id .. ':errors'\n\n                        redis.call('HDEL', job_data_hash, j_id)\n                        redis.call('DEL', job_meta_hash)\n                        redis.call('HDEL', results_hash, j_id)\n                        redis.call('DEL', errors_list_name)\n                    end\n                    redis.call('LTRIM', list_name, 0, max_len - 1)\n                end\n                return #job_ids_to_delete\n            \"#,\n        );\n\n        let _trimmed_count: usize = trim_script\n            .key(self.name())\n            .key(self.success_list_name())\n            .key(self.job_data_hash_name())\n            .key(self.job_result_hash_name()) // results_hash\n            .arg(self.options.max_success) // max_len (LTRIM is 0 to max_success-1)\n            .invoke_async(\u0026mut self.redis.clone())\n            .await?;\n\n        Ok(())\n    }\n\n    async fn complete_job_nack(\n        \u0026self,\n        job: \u0026Job\u003cT\u003e,\n        error: \u0026E,\n        delay: Option\u003cDuration\u003e,\n        position: RequeuePosition,\n        pipeline: \u0026mut Pipeline,\n    ) -\u003e Result\u003c(), TwmqError\u003e {\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        // Remove from active\n        pipeline.hdel(self.active_hash_name(), \u0026job.id);\n\n        let error_record = JobErrorRecord {\n            attempt: job.attempts,\n            error,\n            details: JobErrorType::nack(delay, position),\n            created_at: now,\n        };\n\n        let error_json = serde_json::to_string(\u0026error_record)?;\n\n        pipeline.lpush(self.job_errors_list_name(\u0026job.id), error_json);\n\n        // Add to proper queue based on delay and position\n        if let Some(delay_duration) = delay {\n            let delay_until = now + delay_duration.as_secs();\n\n            // Store position for when delay expires\n            let pos_str = position.to_string();\n\n            pipeline\n                .hset(\n                    self.job_meta_hash_name(\u0026job.id),\n                    \"reentry_position\",\n                    pos_str,\n                )\n                .zadd(self.delayed_zset_name(), \u0026job.id, delay_until);\n        } else {\n            match position {\n                RequeuePosition::First =\u003e {\n                    pipeline.lpush(self.pending_list_name(), \u0026job.id);\n                }\n                RequeuePosition::Last =\u003e {\n                    pipeline.rpush(self.pending_list_name(), \u0026job.id);\n                }\n            }\n        }\n\n        // Execute pipeline\n        pipeline.query_async::\u003c()\u003e(\u0026mut self.redis.clone()).await?;\n\n        Ok(())\n    }\n\n    async fn complete_job_fail(\n        \u0026self,\n        job: \u0026Job\u003cT\u003e,\n        error: \u0026E,\n        pipeline: \u0026mut Pipeline,\n    ) -\u003e Result\u003c(), TwmqError\u003e {\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        // Remove from active, add to failed\n        pipeline\n            .hdel(self.active_hash_name(), \u0026job.id)\n            .lpush(self.failed_list_name(), \u0026job.id)\n            // Set finished_at in the job's metadata hash\n            .hset(self.job_meta_hash_name(\u0026job.id), \"finished_at\", now);\n\n        // Store error\n        let error_record = JobErrorRecord {\n            attempt: job.attempts,\n            error,\n            details: JobErrorType::fail(),\n            created_at: now,\n        };\n        let error_json = serde_json::to_string(\u0026error_record)?;\n\n        pipeline.lpush(self.job_errors_list_name(\u0026job.id), error_json);\n        pipeline.query_async::\u003c()\u003e(\u0026mut self.redis.clone()).await?;\n\n        // Separate call for pruning with data deletion using Lua\n        let trim_script = redis::Script::new(\n            r#\"\n                local queue_id = KEYS[1]\n                local list_name = KEYS[2]\n                local job_data_hash = KEYS[3]\n                local max_len = tonumber(ARGV[1])\n\n\n                local job_ids_to_delete = redis.call('LRANGE', list_name, max_len, -1)\n\n                if #job_ids_to_delete \u003e 0 then\n                    for _, j_id in ipairs(job_ids_to_delete) do\n                        local errors_list_name = 'twmq:' .. queue_id .. ':job:' .. j_id .. ':errors'\n                        local job_meta_hash = 'twmq:' .. queue_id .. ':job:' .. j_id .. ':meta'\n\n                        redis.call('HDEL', job_data_hash, j_id)\n                        redis.call('DEL', job_meta_hash)\n                        redis.call('DEL', errors_list_name)\n                    end\n                    redis.call('LTRIM', list_name, 0, max_len - 1)\n                end\n                return #job_ids_to_delete\n            \"#,\n        );\n\n        let _trimmed_count: usize = trim_script\n            .key(self.name())\n            .key(self.failed_list_name())\n            .key(self.job_data_hash_name())\n            .arg(self.options.max_failed) // max_len (LTRIM is 0 to max_failed-1)\n            .invoke_async(\u0026mut self.redis.clone())\n            .await?;\n\n        Ok(())\n    }\n}\n","traces":[{"line":52,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":69,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[],"length":0,"stats":{"Line":0}},{"line":86,"address":[],"length":0,"stats":{"Line":0}},{"line":123,"address":[],"length":0,"stats":{"Line":13}},{"line":130,"address":[],"length":0,"stats":{"Line":26}},{"line":131,"address":[],"length":0,"stats":{"Line":13}},{"line":135,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":145,"address":[],"length":0,"stats":{"Line":16}},{"line":147,"address":[],"length":0,"stats":{"Line":16}},{"line":153,"address":[],"length":0,"stats":{"Line":589}},{"line":154,"address":[],"length":0,"stats":{"Line":589}},{"line":157,"address":[],"length":0,"stats":{"Line":296}},{"line":158,"address":[],"length":0,"stats":{"Line":296}},{"line":161,"address":[],"length":0,"stats":{"Line":288}},{"line":162,"address":[],"length":0,"stats":{"Line":288}},{"line":165,"address":[],"length":0,"stats":{"Line":271}},{"line":166,"address":[],"length":0,"stats":{"Line":271}},{"line":169,"address":[],"length":0,"stats":{"Line":46}},{"line":170,"address":[],"length":0,"stats":{"Line":46}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[],"length":0,"stats":{"Line":0}},{"line":177,"address":[],"length":0,"stats":{"Line":288}},{"line":178,"address":[],"length":0,"stats":{"Line":288}},{"line":181,"address":[],"length":0,"stats":{"Line":38}},{"line":182,"address":[],"length":0,"stats":{"Line":38}},{"line":185,"address":[],"length":0,"stats":{"Line":16}},{"line":186,"address":[],"length":0,"stats":{"Line":16}},{"line":189,"address":[],"length":0,"stats":{"Line":29}},{"line":190,"address":[],"length":0,"stats":{"Line":29}},{"line":193,"address":[],"length":0,"stats":{"Line":17}},{"line":194,"address":[],"length":0,"stats":{"Line":17}},{"line":197,"address":[],"length":0,"stats":{"Line":32}},{"line":247,"address":[],"length":0,"stats":{"Line":16}},{"line":248,"address":[],"length":0,"stats":{"Line":16}},{"line":253,"address":[],"length":0,"stats":{"Line":16}},{"line":254,"address":[],"length":0,"stats":{"Line":16}},{"line":261,"address":[],"length":0,"stats":{"Line":32}},{"line":263,"address":[],"length":0,"stats":{"Line":0}},{"line":264,"address":[],"length":0,"stats":{"Line":0}},{"line":265,"address":[],"length":0,"stats":{"Line":0}},{"line":268,"address":[],"length":0,"stats":{"Line":0}},{"line":269,"address":[],"length":0,"stats":{"Line":0}},{"line":271,"address":[],"length":0,"stats":{"Line":16}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":273,"address":[],"length":0,"stats":{"Line":0}},{"line":274,"address":[],"length":0,"stats":{"Line":0}},{"line":275,"address":[],"length":0,"stats":{"Line":0}},{"line":276,"address":[],"length":0,"stats":{"Line":0}},{"line":277,"address":[],"length":0,"stats":{"Line":0}},{"line":278,"address":[],"length":0,"stats":{"Line":0}},{"line":279,"address":[],"length":0,"stats":{"Line":0}},{"line":280,"address":[],"length":0,"stats":{"Line":0}},{"line":281,"address":[],"length":0,"stats":{"Line":0}},{"line":282,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":284,"address":[],"length":0,"stats":{"Line":0}},{"line":287,"address":[],"length":0,"stats":{"Line":0}},{"line":290,"address":[],"length":0,"stats":{"Line":14}},{"line":291,"address":[],"length":0,"stats":{"Line":7}},{"line":292,"address":[],"length":0,"stats":{"Line":14}},{"line":294,"address":[],"length":0,"stats":{"Line":7}},{"line":295,"address":[],"length":0,"stats":{"Line":7}},{"line":298,"address":[],"length":0,"stats":{"Line":7}},{"line":299,"address":[],"length":0,"stats":{"Line":0}},{"line":301,"address":[],"length":0,"stats":{"Line":0}},{"line":303,"address":[],"length":0,"stats":{"Line":7}},{"line":305,"address":[],"length":0,"stats":{"Line":0}},{"line":307,"address":[],"length":0,"stats":{"Line":7}},{"line":309,"address":[],"length":0,"stats":{"Line":0}},{"line":310,"address":[],"length":0,"stats":{"Line":7}},{"line":311,"address":[],"length":0,"stats":{"Line":6}},{"line":314,"address":[],"length":0,"stats":{"Line":0}},{"line":315,"address":[],"length":0,"stats":{"Line":0}},{"line":316,"address":[],"length":0,"stats":{"Line":0}},{"line":317,"address":[],"length":0,"stats":{"Line":0}},{"line":318,"address":[],"length":0,"stats":{"Line":0}},{"line":319,"address":[],"length":0,"stats":{"Line":0}},{"line":320,"address":[],"length":0,"stats":{"Line":0}},{"line":323,"address":[],"length":0,"stats":{"Line":0}},{"line":327,"address":[],"length":0,"stats":{"Line":88}},{"line":328,"address":[],"length":0,"stats":{"Line":44}},{"line":330,"address":[],"length":0,"stats":{"Line":88}},{"line":331,"address":[],"length":0,"stats":{"Line":0}},{"line":332,"address":[],"length":0,"stats":{"Line":24}},{"line":333,"address":[],"length":0,"stats":{"Line":0}},{"line":335,"address":[],"length":0,"stats":{"Line":0}},{"line":336,"address":[],"length":0,"stats":{"Line":16}},{"line":337,"address":[],"length":0,"stats":{"Line":0}},{"line":339,"address":[],"length":0,"stats":{"Line":0}},{"line":340,"address":[],"length":0,"stats":{"Line":8}},{"line":341,"address":[],"length":0,"stats":{"Line":0}},{"line":343,"address":[],"length":0,"stats":{"Line":0}},{"line":344,"address":[],"length":0,"stats":{"Line":40}},{"line":345,"address":[],"length":0,"stats":{"Line":0}},{"line":347,"address":[],"length":0,"stats":{"Line":0}},{"line":348,"address":[],"length":0,"stats":{"Line":0}},{"line":349,"address":[],"length":0,"stats":{"Line":0}},{"line":353,"address":[],"length":0,"stats":{"Line":0}},{"line":356,"address":[],"length":0,"stats":{"Line":26}},{"line":358,"address":[],"length":0,"stats":{"Line":13}},{"line":361,"address":[],"length":0,"stats":{"Line":26}},{"line":362,"address":[],"length":0,"stats":{"Line":13}},{"line":364,"address":[],"length":0,"stats":{"Line":0}},{"line":365,"address":[],"length":0,"stats":{"Line":264}},{"line":368,"address":[],"length":0,"stats":{"Line":251}},{"line":369,"address":[],"length":0,"stats":{"Line":339}},{"line":370,"address":[],"length":0,"stats":{"Line":0}},{"line":371,"address":[],"length":0,"stats":{"Line":0}},{"line":374,"address":[],"length":0,"stats":{"Line":382}},{"line":376,"address":[],"length":0,"stats":{"Line":0}},{"line":377,"address":[],"length":0,"stats":{"Line":251}},{"line":378,"address":[],"length":0,"stats":{"Line":382}},{"line":379,"address":[],"length":0,"stats":{"Line":317}},{"line":380,"address":[],"length":0,"stats":{"Line":33}},{"line":381,"address":[],"length":0,"stats":{"Line":0}},{"line":382,"address":[],"length":0,"stats":{"Line":0}},{"line":383,"address":[],"length":0,"stats":{"Line":0}},{"line":385,"address":[],"length":0,"stats":{"Line":33}},{"line":387,"address":[],"length":0,"stats":{"Line":62}},{"line":391,"address":[],"length":0,"stats":{"Line":29}},{"line":392,"address":[],"length":0,"stats":{"Line":13}},{"line":394,"address":[],"length":0,"stats":{"Line":13}},{"line":395,"address":[],"length":0,"stats":{"Line":13}},{"line":398,"address":[],"length":0,"stats":{"Line":13}},{"line":399,"address":[],"length":0,"stats":{"Line":13}},{"line":400,"address":[],"length":0,"stats":{"Line":13}},{"line":403,"address":[],"length":0,"stats":{"Line":13}},{"line":404,"address":[],"length":0,"stats":{"Line":13}},{"line":408,"address":[],"length":0,"stats":{"Line":13}},{"line":411,"address":[],"length":0,"stats":{"Line":13}},{"line":412,"address":[],"length":0,"stats":{"Line":13}},{"line":413,"address":[],"length":0,"stats":{"Line":13}},{"line":414,"address":[],"length":0,"stats":{"Line":13}},{"line":415,"address":[],"length":0,"stats":{"Line":13}},{"line":417,"address":[],"length":0,"stats":{"Line":13}},{"line":419,"address":[],"length":0,"stats":{"Line":0}},{"line":420,"address":[],"length":0,"stats":{"Line":0}},{"line":421,"address":[],"length":0,"stats":{"Line":0}},{"line":422,"address":[],"length":0,"stats":{"Line":0}},{"line":426,"address":[],"length":0,"stats":{"Line":0}},{"line":427,"address":[],"length":0,"stats":{"Line":16}},{"line":428,"address":[],"length":0,"stats":{"Line":16}},{"line":429,"address":[],"length":0,"stats":{"Line":16}},{"line":430,"address":[],"length":0,"stats":{"Line":16}},{"line":432,"address":[],"length":0,"stats":{"Line":16}},{"line":433,"address":[],"length":0,"stats":{"Line":16}},{"line":436,"address":[],"length":0,"stats":{"Line":16}},{"line":437,"address":[],"length":0,"stats":{"Line":16}},{"line":438,"address":[],"length":0,"stats":{"Line":16}},{"line":441,"address":[],"length":0,"stats":{"Line":16}},{"line":442,"address":[],"length":0,"stats":{"Line":16}},{"line":443,"address":[],"length":0,"stats":{"Line":16}},{"line":444,"address":[],"length":0,"stats":{"Line":16}},{"line":448,"address":[],"length":0,"stats":{"Line":16}},{"line":449,"address":[],"length":0,"stats":{"Line":16}},{"line":450,"address":[],"length":0,"stats":{"Line":16}},{"line":453,"address":[],"length":0,"stats":{"Line":16}},{"line":454,"address":[],"length":0,"stats":{"Line":16}},{"line":455,"address":[],"length":0,"stats":{"Line":16}},{"line":456,"address":[],"length":0,"stats":{"Line":16}},{"line":457,"address":[],"length":0,"stats":{"Line":16}},{"line":458,"address":[],"length":0,"stats":{"Line":16}},{"line":459,"address":[],"length":0,"stats":{"Line":16}},{"line":461,"address":[],"length":0,"stats":{"Line":16}},{"line":463,"address":[],"length":0,"stats":{"Line":0}},{"line":464,"address":[],"length":0,"stats":{"Line":0}},{"line":465,"address":[],"length":0,"stats":{"Line":0}},{"line":466,"address":[],"length":0,"stats":{"Line":0}},{"line":470,"address":[],"length":0,"stats":{"Line":0}},{"line":472,"address":[],"length":0,"stats":{"Line":0}},{"line":473,"address":[],"length":0,"stats":{"Line":0}},{"line":476,"address":[],"length":0,"stats":{"Line":0}},{"line":477,"address":[],"length":0,"stats":{"Line":0}},{"line":478,"address":[],"length":0,"stats":{"Line":0}},{"line":481,"address":[],"length":0,"stats":{"Line":0}},{"line":482,"address":[],"length":0,"stats":{"Line":0}},{"line":486,"address":[],"length":0,"stats":{"Line":0}},{"line":489,"address":[],"length":0,"stats":{"Line":0}},{"line":490,"address":[],"length":0,"stats":{"Line":0}},{"line":491,"address":[],"length":0,"stats":{"Line":0}},{"line":493,"address":[],"length":0,"stats":{"Line":0}},{"line":494,"address":[],"length":0,"stats":{"Line":0}},{"line":495,"address":[],"length":0,"stats":{"Line":0}},{"line":496,"address":[],"length":0,"stats":{"Line":0}},{"line":503,"address":[],"length":0,"stats":{"Line":29}},{"line":504,"address":[],"length":0,"stats":{"Line":29}},{"line":507,"address":[],"length":0,"stats":{"Line":0}},{"line":509,"address":[],"length":0,"stats":{"Line":0}},{"line":510,"address":[],"length":0,"stats":{"Line":0}},{"line":516,"address":[],"length":0,"stats":{"Line":13}},{"line":520,"address":[],"length":0,"stats":{"Line":502}},{"line":629,"address":[],"length":0,"stats":{"Line":251}},{"line":630,"address":[],"length":0,"stats":{"Line":251}},{"line":634,"address":[],"length":0,"stats":{"Line":502}},{"line":635,"address":[],"length":0,"stats":{"Line":251}},{"line":636,"address":[],"length":0,"stats":{"Line":251}},{"line":637,"address":[],"length":0,"stats":{"Line":251}},{"line":638,"address":[],"length":0,"stats":{"Line":251}},{"line":639,"address":[],"length":0,"stats":{"Line":251}},{"line":640,"address":[],"length":0,"stats":{"Line":251}},{"line":641,"address":[],"length":0,"stats":{"Line":251}},{"line":642,"address":[],"length":0,"stats":{"Line":251}},{"line":643,"address":[],"length":0,"stats":{"Line":251}},{"line":644,"address":[],"length":0,"stats":{"Line":251}},{"line":646,"address":[],"length":0,"stats":{"Line":0}},{"line":647,"address":[],"length":0,"stats":{"Line":33}},{"line":648,"address":[],"length":0,"stats":{"Line":284}},{"line":650,"address":[],"length":0,"stats":{"Line":0}},{"line":651,"address":[],"length":0,"stats":{"Line":33}},{"line":652,"address":[],"length":0,"stats":{"Line":33}},{"line":653,"address":[],"length":0,"stats":{"Line":33}},{"line":654,"address":[],"length":0,"stats":{"Line":33}},{"line":656,"address":[],"length":0,"stats":{"Line":33}},{"line":657,"address":[],"length":0,"stats":{"Line":33}},{"line":658,"address":[],"length":0,"stats":{"Line":33}},{"line":659,"address":[],"length":0,"stats":{"Line":33}},{"line":660,"address":[],"length":0,"stats":{"Line":33}},{"line":661,"address":[],"length":0,"stats":{"Line":33}},{"line":662,"address":[],"length":0,"stats":{"Line":33}},{"line":665,"address":[],"length":0,"stats":{"Line":0}},{"line":667,"address":[],"length":0,"stats":{"Line":0}},{"line":668,"address":[],"length":0,"stats":{"Line":0}},{"line":669,"address":[],"length":0,"stats":{"Line":0}},{"line":670,"address":[],"length":0,"stats":{"Line":0}},{"line":677,"address":[],"length":0,"stats":{"Line":0}},{"line":680,"address":[],"length":0,"stats":{"Line":13}},{"line":686,"address":[],"length":0,"stats":{"Line":13}},{"line":687,"address":[],"length":0,"stats":{"Line":13}},{"line":692,"address":[],"length":0,"stats":{"Line":13}},{"line":693,"address":[],"length":0,"stats":{"Line":13}},{"line":694,"address":[],"length":0,"stats":{"Line":13}},{"line":696,"address":[],"length":0,"stats":{"Line":13}},{"line":698,"address":[],"length":0,"stats":{"Line":26}},{"line":699,"address":[],"length":0,"stats":{"Line":0}},{"line":702,"address":[],"length":0,"stats":{"Line":0}},{"line":731,"address":[],"length":0,"stats":{"Line":13}},{"line":732,"address":[],"length":0,"stats":{"Line":0}},{"line":733,"address":[],"length":0,"stats":{"Line":0}},{"line":734,"address":[],"length":0,"stats":{"Line":0}},{"line":735,"address":[],"length":0,"stats":{"Line":0}},{"line":736,"address":[],"length":0,"stats":{"Line":0}},{"line":737,"address":[],"length":0,"stats":{"Line":0}},{"line":738,"address":[],"length":0,"stats":{"Line":0}},{"line":740,"address":[],"length":0,"stats":{"Line":0}},{"line":743,"address":[],"length":0,"stats":{"Line":16}},{"line":751,"address":[],"length":0,"stats":{"Line":16}},{"line":752,"address":[],"length":0,"stats":{"Line":16}},{"line":757,"address":[],"length":0,"stats":{"Line":16}},{"line":760,"address":[],"length":0,"stats":{"Line":16}},{"line":762,"address":[],"length":0,"stats":{"Line":16}},{"line":766,"address":[],"length":0,"stats":{"Line":32}},{"line":768,"address":[],"length":0,"stats":{"Line":0}},{"line":771,"address":[],"length":0,"stats":{"Line":0}},{"line":772,"address":[],"length":0,"stats":{"Line":0}},{"line":775,"address":[],"length":0,"stats":{"Line":0}},{"line":777,"address":[],"length":0,"stats":{"Line":0}},{"line":779,"address":[],"length":0,"stats":{"Line":0}},{"line":781,"address":[],"length":0,"stats":{"Line":0}},{"line":783,"address":[],"length":0,"stats":{"Line":0}},{"line":785,"address":[],"length":0,"stats":{"Line":16}},{"line":786,"address":[],"length":0,"stats":{"Line":0}},{"line":787,"address":[],"length":0,"stats":{"Line":0}},{"line":789,"address":[],"length":0,"stats":{"Line":16}},{"line":790,"address":[],"length":0,"stats":{"Line":16}},{"line":796,"address":[],"length":0,"stats":{"Line":0}},{"line":798,"address":[],"length":0,"stats":{"Line":16}},{"line":801,"address":[],"length":0,"stats":{"Line":0}},{"line":807,"address":[],"length":0,"stats":{"Line":0}},{"line":808,"address":[],"length":0,"stats":{"Line":0}},{"line":813,"address":[],"length":0,"stats":{"Line":0}},{"line":814,"address":[],"length":0,"stats":{"Line":0}},{"line":815,"address":[],"length":0,"stats":{"Line":0}},{"line":817,"address":[],"length":0,"stats":{"Line":0}},{"line":821,"address":[],"length":0,"stats":{"Line":0}},{"line":823,"address":[],"length":0,"stats":{"Line":0}},{"line":826,"address":[],"length":0,"stats":{"Line":0}},{"line":828,"address":[],"length":0,"stats":{"Line":0}},{"line":829,"address":[],"length":0,"stats":{"Line":0}},{"line":857,"address":[],"length":0,"stats":{"Line":0}},{"line":858,"address":[],"length":0,"stats":{"Line":0}},{"line":859,"address":[],"length":0,"stats":{"Line":0}},{"line":860,"address":[],"length":0,"stats":{"Line":0}},{"line":861,"address":[],"length":0,"stats":{"Line":0}},{"line":862,"address":[],"length":0,"stats":{"Line":0}},{"line":863,"address":[],"length":0,"stats":{"Line":0}},{"line":865,"address":[],"length":0,"stats":{"Line":0}}],"covered":153,"coverable":293},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","src","queue.rs"],"content":"use std::time::Duration;\n\n#[derive(Clone, Debug)]\npub struct QueueOptions {\n    pub max_success: usize,\n    pub max_failed: usize,\n    pub lease_duration: Duration,\n    pub local_concurrency: usize,\n\n    pub polling_interval: Duration,\n\n    /// If true, always poll for jobs even if there are no available permits\n    /// This is important, because polling is how delayed and timed out jobs are handled\n    /// If you have a horiztonally scaled deployment, this can be set to the default of false\n    /// But if there's only one node, you can set this to true to avoid the local concurrency from blocking queue housekeeping\n    pub always_poll: bool,\n}\n\nimpl Default for QueueOptions {\n    fn default() -\u003e Self {\n        Self {\n            max_success: 1000,\n            max_failed: 10000,\n            local_concurrency: 100,\n            polling_interval: Duration::from_millis(100),\n            lease_duration: Duration::from_secs(30),\n            always_poll: false,\n        }\n    }\n}\n","traces":[{"line":20,"address":[],"length":0,"stats":{"Line":10}},{"line":25,"address":[],"length":0,"stats":{"Line":10}},{"line":26,"address":[],"length":0,"stats":{"Line":10}}],"covered":3,"coverable":3},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","tests","basic.rs"],"content":"mod fixtures;\nuse fixtures::*;\nuse redis::AsyncCommands;\nuse tracing_subscriber::EnvFilter;\nuse tracing_subscriber::layer::SubscriberExt;\nuse tracing_subscriber::util::SubscriberInitExt;\nuse twmq::Queue;\n\nuse std::sync::atomic::Ordering;\n// Or use specific imports if they are in a different module.\nuse std::sync::Arc;\nuse std::time::Duration;\nuse twmq::job::{JobOptions, JobStatus}; // Assuming JobStatus is in twmq::job\nuse twmq::redis::aio::ConnectionManager; // For cleanup utility\n\nconst REDIS_URL: \u0026str = \"redis://127.0.0.1:6379/\";\n\n// Helper to clean up Redis keys for a given queue name pattern\nasync fn cleanup_redis_keys(conn_manager: \u0026ConnectionManager, queue_name: \u0026str) {\n    let mut conn = conn_manager.clone();\n    let keys_pattern = format!(\"twmq:{}:*\", queue_name);\n\n    let keys: Vec\u003cString\u003e = redis::cmd(\"KEYS\")\n        .arg(\u0026keys_pattern)\n        .query_async(\u0026mut conn)\n        .await\n        .unwrap_or_default();\n    if !keys.is_empty() {\n        redis::cmd(\"DEL\")\n            .arg(keys)\n            .query_async::\u003c()\u003e(\u0026mut conn)\n            .await\n            .unwrap_or_default();\n    }\n    println!(\"Cleaned up keys for pattern: {}\", keys_pattern);\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_queue_push_and_process_job() {\n    tracing_subscriber::registry()\n        .with(EnvFilter::try_from_default_env().unwrap_or_else(|_| {\n            // Default to info level if RUST_LOG environment variable is not set\n            \"thirdweb_engine=debug,tower_http=debug,axum=debug\".into()\n        }))\n        .with(tracing_subscriber::fmt::layer())\n        .init();\n\n    let test_job_id = \"test_job_001\".to_string();\n    let queue_name = format!(\"test_q_simple_{}\", nanoid::nanoid!(6));\n\n    // Reset the flag before each test run\n    TEST_JOB_PROCESSED_SUCCESSFULLY.store(false, Ordering::SeqCst);\n\n    println!(\"Creating queue: {}\", queue_name);\n    let queue = Arc::new(\n        Queue::\u003cTestJobPayload, TestJobOutput, TestJobErrorData, ()\u003e::new(\n            REDIS_URL,\n            \u0026queue_name,\n            None, // Default QueueOptions\n            (),\n        )\n        .await\n        .expect(\"Failed to create queue\"),\n    );\n\n    // Cleanup Redis before starting, in case of previous failed test\n    cleanup_redis_keys(\u0026queue.redis.clone(), \u0026queue_name).await;\n\n    let job_payload = TestJobPayload {\n        message: \"hello from test\".to_string(),\n        id_to_check: test_job_id.clone(),\n    };\n\n    println!(\"Pushing job with ID: {}\", test_job_id);\n    let job_options = JobOptions {\n        data: job_payload,\n        id: test_job_id.clone(),\n        delay: None,\n    };\n\n    let pushed_job_details = queue.push(job_options).await.expect(\"Failed to push job\");\n    assert_eq!(pushed_job_details.id, test_job_id);\n\n    let pending_count = queue\n        .count(JobStatus::Pending)\n        .await\n        .expect(\"Failed to count pending jobs\");\n    assert_eq!(\n        pending_count, 1,\n        \"There should be 1 job in the pending list\"\n    );\n\n    println!(\"Starting worker for queue: {}\", queue_name);\n\n    let queue_name_clone = queue_name.clone();\n    let worker_queue_ref = Arc::clone(\u0026queue);\n    let worker_handle = tokio::spawn(async move {\n        // The worker will loop internally, so we expect it to pick up the job\n        if let Err(e) = worker_queue_ref.work().await {\n            eprintln!(\"Worker for queue {} failed: {:?}\", queue_name_clone, e);\n        }\n    });\n\n    // Wait for the job to be processed\n    // Poll the flag, with a timeout\n    let mut processed = false;\n    for _ in 0..50 {\n        // Max wait 5 seconds (50 * 100ms)\n        if TEST_JOB_PROCESSED_SUCCESSFULLY.load(Ordering::SeqCst) {\n            processed = true;\n            println!(\"Job processed flag is true.\");\n            break;\n        }\n        tokio::time::sleep(Duration::from_millis(100)).await;\n    }\n    assert!(processed, \"Job was not processed by the worker in time\");\n\n    // Verify job moved to success list\n    let success_count = queue\n        .count(JobStatus::Success)\n        .await\n        .expect(\"Failed to count successful jobs\");\n    assert_eq!(\n        success_count, 1,\n        \"There should be 1 job in the success list\"\n    );\n\n    // Verify pending and active lists are empty\n    let pending_after_processing = queue\n        .count(JobStatus::Pending)\n        .await\n        .expect(\"Failed to count pending jobs after processing\");\n    assert_eq!(\n        pending_after_processing, 0,\n        \"Pending list should be empty after processing\"\n    );\n\n    let active_count = queue\n        .count(JobStatus::Active)\n        .await\n        .expect(\"Failed to count active jobs\");\n    assert_eq!(active_count, 0, \"Active list should be empty\");\n\n    // Verify the job result was stored\n    let mut redis_conn = queue.redis.clone();\n    let result_json_opt: Option\u003cString\u003e = redis_conn\n        .hget(queue.job_result_hash_name(), \u0026test_job_id)\n        .await\n        .expect(\"Redis HGET for result failed\");\n\n    assert!(\n        result_json_opt.is_some(),\n        \"Job result should be stored in Redis\"\n    );\n    let result_json = result_json_opt.unwrap();\n    let job_output: TestJobOutput =\n        serde_json::from_str(\u0026result_json).expect(\"Failed to deserialize job output\");\n    assert_eq!(job_output.reply, \"Successfully processed 'hello from test'\");\n\n    println!(\"Test completed for queue: {}\", queue_name.clone());\n\n    // The worker task runs in a loop. For a clean test exit,\n    // you might want to abort it or implement a shutdown signal for the worker.\n    // For this simple test, we'll let it be.\n    worker_handle.abort(); // Or a more graceful shutdown if implemented\n\n    // Cleanup Redis keys after test\n    cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n}\n","traces":[{"line":19,"address":[],"length":0,"stats":{"Line":4}},{"line":20,"address":[],"length":0,"stats":{"Line":2}},{"line":21,"address":[],"length":0,"stats":{"Line":2}},{"line":23,"address":[],"length":0,"stats":{"Line":4}},{"line":24,"address":[],"length":0,"stats":{"Line":2}},{"line":25,"address":[],"length":0,"stats":{"Line":2}},{"line":26,"address":[],"length":0,"stats":{"Line":2}},{"line":28,"address":[],"length":0,"stats":{"Line":2}},{"line":29,"address":[],"length":0,"stats":{"Line":1}},{"line":30,"address":[],"length":0,"stats":{"Line":1}},{"line":31,"address":[],"length":0,"stats":{"Line":1}},{"line":32,"address":[],"length":0,"stats":{"Line":1}},{"line":35,"address":[],"length":0,"stats":{"Line":2}}],"covered":13,"coverable":13},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","tests","basic_hook.rs"],"content":"// Add this to tests/basic.rs\nmod fixtures;\nuse fixtures::{TestJobErrorData, TestJobOutput};\nuse redis::aio::ConnectionManager;\nuse tracing_subscriber::{EnvFilter, layer::SubscriberExt, util::SubscriberInitExt};\n\nuse std::{\n    sync::{\n        Arc,\n        atomic::{AtomicBool, Ordering},\n    },\n    time::Duration,\n};\n\nuse serde::{Deserialize, Serialize};\nuse twmq::{\n    DurableExecution, Queue, SuccessHookData,\n    hooks::TransactionContext,\n    job::{Job, JobResult, JobStatus},\n    queue::QueueOptions,\n};\n\n// Helper to clean up Redis keys for a given queue name pattern\nasync fn cleanup_redis_keys(conn_manager: \u0026ConnectionManager, queue_name: \u0026str) {\n    let mut conn = conn_manager.clone();\n    let keys_pattern = format!(\"twmq:{}:*\", queue_name);\n\n    let keys: Vec\u003cString\u003e = redis::cmd(\"KEYS\")\n        .arg(\u0026keys_pattern)\n        .query_async(\u0026mut conn)\n        .await\n        .unwrap_or_default();\n    if !keys.is_empty() {\n        redis::cmd(\"DEL\")\n            .arg(keys)\n            .query_async::\u003c()\u003e(\u0026mut conn)\n            .await\n            .unwrap_or_default();\n    }\n    println!(\"Cleaned up keys for pattern: {}\", keys_pattern);\n}\n\n// Define webhook job types\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct WebhookJobPayload {\n    pub url: String,\n    pub payload: String,\n    pub parent_job_id: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]\npub struct WebhookJobOutput {\n    pub status_code: u16,\n    pub response: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct WebhookJobErrorData {\n    pub error: String,\n}\n\n// Main job that queues webhook jobs\n#[derive(Serialize, Deserialize, Clone)]\npub struct MainJobPayload {\n    pub message: String,\n    pub id_to_check: String,\n    // Pass webhook queue reference as part of job data\n}\n\npub static MAIN_JOB_PROCESSED: AtomicBool = AtomicBool::new(false);\npub static WEBHOOK_JOB_PROCESSED: AtomicBool = AtomicBool::new(false);\n\nconst REDIS_URL: \u0026str = \"redis://127.0.0.1:6379/\";\n\ntype WebhookQueue = Queue\u003cWebhookJobPayload, WebhookJobOutput, WebhookJobErrorData, ()\u003e;\n\nimpl DurableExecution for MainJobPayload {\n    type Output = TestJobOutput;\n    type ErrorData = TestJobErrorData;\n    type ExecutionContext = Arc\u003cWebhookQueue\u003e;\n\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        _: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        println!(\"MAIN_JOB: Processing job with id: {}\", job.id);\n        tokio::time::sleep(Duration::from_millis(50)).await;\n\n        MAIN_JOB_PROCESSED.store(true, Ordering::SeqCst);\n\n        JobResult::Success(TestJobOutput {\n            reply: format!(\"Main job processed: {}\", job.data.message),\n        })\n    }\n\n    async fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: SuccessHookData\u003c'_, Self::Output\u003e,\n        tx: \u0026mut TransactionContext\u003c'_\u003e,\n        ec: \u0026Self::ExecutionContext,\n    ) {\n        println!(\"MAIN_JOB: on_success hook - queuing webhook job\");\n\n        let webhook_job = WebhookJobPayload {\n            url: \"https://api.example.com/webhook\".to_string(),\n            payload: serde_json::to_string(d.result).unwrap(),\n            parent_job_id: self.id_to_check.clone(),\n        };\n\n        // Use the type-safe API!\n        let mut webhook_builder = ec.clone().job(webhook_job);\n\n        webhook_builder.options.id = format!(\"{}_webhook\", self.id_to_check);\n\n        if let Err(e) = tx.queue_job(webhook_builder) {\n            tracing::error!(\"Failed to queue webhook job: {:?}\", e);\n        } else {\n            tracing::info!(\"Successfully queued webhook job!\");\n        }\n    }\n}\n\nimpl DurableExecution for WebhookJobPayload {\n    type Output = WebhookJobOutput;\n    type ErrorData = WebhookJobErrorData;\n    type ExecutionContext = ();\n\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        _: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        println!(\"WEBHOOK_JOB: Sending webhook to: {}\", job.data.url);\n        println!(\"WEBHOOK_JOB: Payload: {}\", job.data.payload);\n        tokio::time::sleep(Duration::from_millis(25)).await;\n\n        WEBHOOK_JOB_PROCESSED.store(true, Ordering::SeqCst);\n\n        // Simulate successful webhook call\n        JobResult::Success(WebhookJobOutput {\n            status_code: 200,\n            response: \"Webhook delivered successfully\".to_string(),\n        })\n    }\n\n    async fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        _d: SuccessHookData\u003c'_, Self::Output\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\n            \"WEBHOOK_JOB: Webhook delivered successfully for parent: {}\",\n            self.parent_job_id\n        );\n    }\n\n    async fn on_timeout(\u0026self, _tx: \u0026mut TransactionContext\u003c'_\u003e) {}\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_cross_queue_job_scheduling() {\n    tracing_subscriber::registry()\n        .with(\n            EnvFilter::try_from_default_env()\n                .unwrap_or_else(|_| \"thirdweb_engine=debug,tower_http=debug,axum=debug\".into()),\n        )\n        .with(tracing_subscriber::fmt::layer())\n        .init();\n\n    let main_queue_name = format!(\"test_main_{}\", nanoid::nanoid!(6));\n    let webhook_queue_name = format!(\"test_webhook_{}\", nanoid::nanoid!(6));\n\n    // Reset flags\n    MAIN_JOB_PROCESSED.store(false, Ordering::SeqCst);\n    WEBHOOK_JOB_PROCESSED.store(false, Ordering::SeqCst);\n\n    println!(\"Creating main queue: {}\", main_queue_name);\n    println!(\"Creating webhook queue: {}\", webhook_queue_name);\n\n    let mut queue_options = QueueOptions::default();\n    queue_options.local_concurrency = 1;\n\n    // Create webhook queue\n    let webhook_queue = Arc::new(\n        Queue::\u003cWebhookJobPayload, WebhookJobOutput, WebhookJobErrorData, ()\u003e::new(\n            REDIS_URL,\n            \u0026webhook_queue_name,\n            Some(queue_options.clone()),\n            (),\n        )\n        .await\n        .expect(\"Failed to create webhook queue\"),\n    );\n\n    // Create main job queue\n    let main_queue = Arc::new(\n        Queue::\u003cMainJobPayload, TestJobOutput, TestJobErrorData, Arc\u003cWebhookQueue\u003e\u003e::new(\n            REDIS_URL,\n            \u0026main_queue_name,\n            Some(queue_options),\n            webhook_queue.clone(),\n        )\n        .await\n        .expect(\"Failed to create main queue\"),\n    );\n\n    // Clean up before test\n    cleanup_redis_keys(\u0026main_queue.redis, \u0026main_queue_name).await;\n    cleanup_redis_keys(\u0026webhook_queue.redis, \u0026webhook_queue_name).await;\n\n    // Create main job with access to webhook queue\n    let main_job = MainJobPayload {\n        message: \"Process transaction #123\".to_string(),\n        id_to_check: \"main_job_001\".to_string(),\n    };\n\n    println!(\"Pushing main job\");\n\n    main_queue\n        .clone()\n        .job(main_job)\n        .with_id(\"main_job_001\")\n        .push()\n        .await\n        .expect(\"Failed to push main job\");\n\n    // Start workers for both queues\n    println!(\"Starting workers\");\n    let main_worker = {\n        let queue = main_queue.clone();\n        tokio::spawn(async move {\n            if let Err(e) = queue.work().await {\n                eprintln!(\"Main worker failed: {:?}\", e);\n            }\n        })\n    };\n\n    let webhook_worker = {\n        let queue = webhook_queue.clone();\n        tokio::spawn(async move {\n            if let Err(e) = queue.work().await {\n                eprintln!(\"Webhook worker failed: {:?}\", e);\n            }\n        })\n    };\n\n    // Wait for main job to complete\n    let mut main_processed = false;\n    for _ in 0..50 {\n        if MAIN_JOB_PROCESSED.load(Ordering::SeqCst) {\n            main_processed = true;\n            println!(\"Main job processed!\");\n            break;\n        }\n        tokio::time::sleep(Duration::from_millis(100)).await;\n    }\n    assert!(main_processed, \"Main job should be processed\");\n\n    // Give a moment for the webhook job to be queued and processed\n    tokio::time::sleep(Duration::from_millis(200)).await;\n\n    // Check that webhook job was queued and processed\n    let webhook_pending = webhook_queue.count(JobStatus::Pending).await.unwrap();\n    let webhook_success = webhook_queue.count(JobStatus::Success).await.unwrap();\n\n    println!(\n        \"Webhook queue - Pending: {}, Success: {}\",\n        webhook_pending, webhook_success\n    );\n\n    // Either the webhook job is still pending or already succeeded\n    assert!(\n        webhook_pending \u003e 0 || webhook_success \u003e 0,\n        \"Webhook job should be queued\"\n    );\n\n    // Wait for webhook job to complete if it's still pending\n    if webhook_pending \u003e 0 {\n        let mut webhook_processed = false;\n        for _ in 0..50 {\n            if WEBHOOK_JOB_PROCESSED.load(Ordering::SeqCst) {\n                webhook_processed = true;\n                println!(\"Webhook job processed!\");\n                break;\n            }\n            tokio::time::sleep(Duration::from_millis(100)).await;\n        }\n        assert!(webhook_processed, \"Webhook job should be processed\");\n    }\n\n    // Verify final state\n    let main_success = main_queue.count(JobStatus::Success).await.unwrap();\n    let webhook_final_success = webhook_queue.count(JobStatus::Success).await.unwrap();\n\n    assert_eq!(main_success, 1, \"Main job should be in success list\");\n    assert_eq!(\n        webhook_final_success, 1,\n        \"Webhook job should be in success list\"\n    );\n\n    println!(\"✅ Cross-queue job scheduling works!\");\n    println!(\"Main job triggered webhook job atomically via transaction hooks\");\n\n    // Cleanup\n    main_worker.abort();\n    webhook_worker.abort();\n    cleanup_redis_keys(\u0026main_queue.redis, \u0026main_queue_name).await;\n    cleanup_redis_keys(\u0026webhook_queue.redis, \u0026webhook_queue_name).await;\n}\n","traces":[{"line":24,"address":[],"length":0,"stats":{"Line":8}},{"line":25,"address":[],"length":0,"stats":{"Line":4}},{"line":26,"address":[],"length":0,"stats":{"Line":4}},{"line":28,"address":[],"length":0,"stats":{"Line":8}},{"line":29,"address":[],"length":0,"stats":{"Line":4}},{"line":30,"address":[],"length":0,"stats":{"Line":4}},{"line":31,"address":[],"length":0,"stats":{"Line":4}},{"line":33,"address":[],"length":0,"stats":{"Line":4}},{"line":34,"address":[],"length":0,"stats":{"Line":2}},{"line":35,"address":[],"length":0,"stats":{"Line":2}},{"line":36,"address":[],"length":0,"stats":{"Line":2}},{"line":37,"address":[],"length":0,"stats":{"Line":2}},{"line":40,"address":[],"length":0,"stats":{"Line":4}},{"line":82,"address":[],"length":0,"stats":{"Line":1}},{"line":86,"address":[],"length":0,"stats":{"Line":1}},{"line":87,"address":[],"length":0,"stats":{"Line":1}},{"line":89,"address":[],"length":0,"stats":{"Line":1}},{"line":91,"address":[],"length":0,"stats":{"Line":1}},{"line":92,"address":[],"length":0,"stats":{"Line":1}},{"line":96,"address":[],"length":0,"stats":{"Line":1}},{"line":103,"address":[],"length":0,"stats":{"Line":1}},{"line":106,"address":[],"length":0,"stats":{"Line":1}},{"line":107,"address":[],"length":0,"stats":{"Line":1}},{"line":108,"address":[],"length":0,"stats":{"Line":1}},{"line":112,"address":[],"length":0,"stats":{"Line":1}},{"line":114,"address":[],"length":0,"stats":{"Line":1}},{"line":116,"address":[],"length":0,"stats":{"Line":1}},{"line":117,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":2}},{"line":129,"address":[],"length":0,"stats":{"Line":1}},{"line":133,"address":[],"length":0,"stats":{"Line":1}},{"line":134,"address":[],"length":0,"stats":{"Line":1}},{"line":135,"address":[],"length":0,"stats":{"Line":1}},{"line":137,"address":[],"length":0,"stats":{"Line":1}},{"line":140,"address":[],"length":0,"stats":{"Line":1}},{"line":141,"address":[],"length":0,"stats":{"Line":1}},{"line":142,"address":[],"length":0,"stats":{"Line":1}},{"line":146,"address":[],"length":0,"stats":{"Line":1}},{"line":153,"address":[],"length":0,"stats":{"Line":1}},{"line":154,"address":[],"length":0,"stats":{"Line":1}},{"line":159,"address":[],"length":0,"stats":{"Line":0}}],"covered":39,"coverable":41},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","tests","delay.rs"],"content":"// tests/delay.rs\n\nuse std::sync::Arc;\nuse std::time::{Duration, SystemTime, UNIX_EPOCH};\n\nuse redis::AsyncCommands;\nuse serde::{Deserialize, Serialize};\nuse tracing_subscriber::{EnvFilter, layer::SubscriberExt, util::SubscriberInitExt};\n\nuse twmq::{\n    DurableExecution, Queue, SuccessHookData,\n    hooks::TransactionContext,\n    job::{DelayOptions, Job, JobResult, JobStatus, RequeuePosition},\n    queue::QueueOptions,\n    redis::aio::ConnectionManager,\n};\n\nconst REDIS_URL: \u0026str = \"redis://127.0.0.1:6379/\";\n\n// Helper to clean up Redis keys\nasync fn cleanup_redis_keys(conn_manager: \u0026ConnectionManager, queue_name: \u0026str) {\n    let mut conn = conn_manager.clone();\n    let keys_pattern = format!(\"twmq:{}:*\", queue_name);\n    let keys: Vec\u003cString\u003e = redis::cmd(\"KEYS\")\n        .arg(\u0026keys_pattern)\n        .query_async(\u0026mut conn)\n        .await\n        .unwrap_or_default();\n    if !keys.is_empty() {\n        redis::cmd(\"DEL\")\n            .arg(keys)\n            .query_async::\u003c()\u003e(\u0026mut conn)\n            .await\n            .unwrap_or_default();\n    }\n    tracing::info!(\"Cleaned up keys for pattern: {}\", keys_pattern);\n}\n\n// Job that tests delay functionality\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct DelayTestJob {\n    pub job_id: String,\n    pub expected_delay_seconds: u64,\n    pub created_at: u64,\n    pub test_id: String, // Unique per test to avoid conflicts\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]\npub struct DelayTestOutput {\n    pub actual_delay_seconds: u64,\n    pub message: String,\n    pub test_id: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct DelayTestErrorData {\n    pub reason: String,\n}\n\nimpl DurableExecution for DelayTestJob {\n    type Output = DelayTestOutput;\n    type ErrorData = DelayTestErrorData;\n    type ExecutionContext = Arc\u003cConnectionManager\u003e;\n\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        _redis_conn: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n        let actual_delay = now - job.data.created_at;\n\n        tracing::info!(\n            \"DELAY_JOB: Processing job {}, expected delay: {}s, actual delay: {}s\",\n            job.data.job_id,\n            job.data.expected_delay_seconds,\n            actual_delay\n        );\n\n        JobResult::Success(DelayTestOutput {\n            actual_delay_seconds: actual_delay,\n            message: format!(\n                \"Job {} processed after {}s delay\",\n                job.data.job_id, actual_delay\n            ),\n            test_id: job.data.test_id.clone(),\n        })\n    }\n\n    async fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: SuccessHookData\u003c'_, Self::Output\u003e,\n        tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _redis_conn: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\"DELAY_JOB: on_success hook - {}\", d.result.message);\n\n        // Store processing order in Redis using test-specific key\n        let order_key = format!(\"test:{}:processing_order\", self.test_id);\n\n        // Use pipeline to add to processing order list\n        tx.pipeline().rpush(\u0026order_key, \u0026self.job_id);\n        tx.pipeline().expire(\u0026order_key, 300); // Expire after 5 minutes\n    }\n\n    async fn on_timeout(\u0026self, _tx: \u0026mut TransactionContext\u003c'_\u003e) {\n        tracing::info!(\"DELAY_JOB: on_timeout hook for job {}\", self.job_id);\n    }\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_job_delay_basic() {\n    tracing_subscriber::registry()\n        .with(EnvFilter::try_from_default_env().unwrap_or_else(|_| \"twmq=debug\".into()))\n        .with(tracing_subscriber::fmt::layer())\n        .init();\n\n    let test_id = nanoid::nanoid!();\n    let queue_name = format!(\"test_delay_{}\", test_id);\n    let job_id = \"delay_job_001\";\n    let delay_duration = Duration::from_secs(2);\n\n    tracing::info!(\n        \"Creating delay test queue: {} (test_id: {})\",\n        queue_name,\n        test_id\n    );\n    tracing::info!(\"Job should be delayed by: {:?}\", delay_duration);\n\n    let queue_options = QueueOptions {\n        local_concurrency: 1,\n        polling_interval: Duration::from_millis(100),\n        always_poll: true,\n        ..Default::default()\n    };\n\n    // Create Redis connection for the execution context\n    let redis_client = redis::Client::open(REDIS_URL).unwrap();\n    let redis_conn = Arc::new(redis_client.get_connection_manager().await.unwrap());\n\n    let queue = Arc::new(\n        Queue::\u003cDelayTestJob, DelayTestOutput, DelayTestErrorData, Arc\u003cConnectionManager\u003e\u003e::new(\n            REDIS_URL,\n            \u0026queue_name,\n            Some(queue_options),\n            redis_conn.clone(),\n        )\n        .await\n        .expect(\"Failed to create delay test queue\"),\n    );\n\n    cleanup_redis_keys(\u0026redis_conn, \u0026queue_name).await;\n\n    let created_at = SystemTime::now()\n        .duration_since(UNIX_EPOCH)\n        .unwrap()\n        .as_secs();\n\n    // Create job with delay\n    let delay_job = DelayTestJob {\n        job_id: job_id.to_string(),\n        expected_delay_seconds: delay_duration.as_secs(),\n        created_at,\n        test_id: test_id.clone(),\n    };\n\n    tracing::info!(\"Pushing delayed job with ID: {}\", job_id);\n\n    let delay_options = DelayOptions {\n        delay: delay_duration,\n        position: RequeuePosition::Last,\n    };\n\n    queue\n        .clone()\n        .job(delay_job)\n        .with_id(job_id)\n        .with_delay(delay_options)\n        .push()\n        .await\n        .expect(\"Failed to push delayed job\");\n\n    // Verify job starts in delayed queue, not pending\n    let initial_pending = queue.count(JobStatus::Pending).await.unwrap();\n    let initial_delayed = queue.count(JobStatus::Delayed).await.unwrap();\n    let initial_active = queue.count(JobStatus::Active).await.unwrap();\n\n    tracing::info!(\n        \"Initial state - Pending: {}, Delayed: {}, Active: {}\",\n        initial_pending,\n        initial_delayed,\n        initial_active\n    );\n\n    assert_eq!(\n        initial_pending, 0,\n        \"Job should not be in pending queue initially\"\n    );\n    assert_eq!(initial_delayed, 1, \"Job should be in delayed queue\");\n    assert_eq!(initial_active, 0, \"No jobs should be active initially\");\n\n    // Start worker\n    tracing::info!(\"Starting worker\");\n    let worker = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            if let Err(e) = queue.work().await {\n                tracing::error!(\"Delay worker failed: {:?}\", e);\n            }\n        })\n    };\n\n    // Job should not be processed immediately (still delayed)\n    tokio::time::sleep(Duration::from_millis(500)).await;\n\n    let early_success_count = queue.count(JobStatus::Success).await.unwrap();\n    assert_eq!(\n        early_success_count, 0,\n        \"Job should not be processed before delay expires\"\n    );\n\n    let mid_pending = queue.count(JobStatus::Pending).await.unwrap();\n    let mid_delayed = queue.count(JobStatus::Delayed).await.unwrap();\n    tracing::info!(\n        \"Mid-delay state - Pending: {}, Delayed: {}\",\n        mid_pending,\n        mid_delayed\n    );\n\n    // Wait for delay to expire and job to be processed\n    let total_wait = delay_duration + Duration::from_secs(2);\n    tracing::info!(\n        \"Waiting {:?} for delay to expire and job to process...\",\n        total_wait\n    );\n\n    let mut job_processed = false;\n    let start_waiting = SystemTime::now();\n\n    while start_waiting.elapsed().unwrap() \u003c total_wait {\n        let success_count = queue.count(JobStatus::Success).await.unwrap();\n        if success_count \u003e 0 {\n            job_processed = true;\n            tracing::info!(\n                \"Job processed after waiting {:?}\",\n                start_waiting.elapsed().unwrap()\n            );\n            break;\n        }\n        tokio::time::sleep(Duration::from_millis(200)).await;\n    }\n\n    assert!(job_processed, \"Job should be processed after delay expires\");\n\n    // Give a moment for final processing\n    tokio::time::sleep(Duration::from_millis(300)).await;\n\n    // Verify final state\n    let final_pending = queue.count(JobStatus::Pending).await.unwrap();\n    let final_delayed = queue.count(JobStatus::Delayed).await.unwrap();\n    let final_active = queue.count(JobStatus::Active).await.unwrap();\n    let final_success = queue.count(JobStatus::Success).await.unwrap();\n\n    tracing::info!(\n        \"Final state - Pending: {}, Delayed: {}, Active: {}, Success: {}\",\n        final_pending,\n        final_delayed,\n        final_active,\n        final_success\n    );\n\n    assert_eq!(final_delayed, 0, \"No jobs should remain in delayed queue\");\n    assert_eq!(final_active, 0, \"No jobs should be active\");\n    assert_eq!(final_success, 1, \"Job should be in success queue\");\n\n    // Verify the job result shows correct delay timing\n    let mut redis_conn_direct = redis_conn.as_ref().clone();\n    let result_json: Option\u003cString\u003e = redis_conn_direct\n        .hget(queue.job_result_hash_name(), job_id)\n        .await\n        .expect(\"Failed to get job result\");\n\n    assert!(result_json.is_some(), \"Job result should be stored\");\n\n    let job_output: DelayTestOutput =\n        serde_json::from_str(\u0026result_json.unwrap()).expect(\"Failed to deserialize job result\");\n\n    tracing::info!(\"Job output: {:?}\", job_output);\n\n    // Allow some tolerance for timing (±1 second)\n    let expected_delay = delay_duration.as_secs();\n    let actual_delay = job_output.actual_delay_seconds;\n\n    assert!(\n        actual_delay \u003e= expected_delay \u0026\u0026 actual_delay \u003c= expected_delay + 2,\n        \"Actual delay ({}) should be close to expected delay ({})\",\n        actual_delay,\n        expected_delay\n    );\n\n    tracing::info!(\"✅ Basic delay mechanism works correctly!\");\n\n    worker.abort();\n    cleanup_redis_keys(\u0026redis_conn, \u0026queue_name).await;\n\n    // Clean up test-specific keys\n    let _: () = redis_conn_direct\n        .del(format!(\"test:{}:processing_order\", test_id))\n        .await\n        .unwrap_or(());\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_delay_position_ordering() {\n    // Test that delayed jobs respect RequeuePosition when they expire\n\n    let test_id = nanoid::nanoid!();\n    let queue_name = format!(\"test_delay_order_{}\", test_id);\n\n    tracing::info!(\n        \"\\n=== Testing delay position ordering (test_id: {}) ===\",\n        test_id\n    );\n\n    let queue_options = QueueOptions {\n        local_concurrency: 1, // Process one job at a time to see clear ordering\n        polling_interval: Duration::from_millis(50),\n        always_poll: true,\n        ..Default::default()\n    };\n\n    // Create Redis connection for the execution context\n    let redis_client = redis::Client::open(REDIS_URL).unwrap();\n    let redis_conn = Arc::new(redis_client.get_connection_manager().await.unwrap());\n\n    let queue = Arc::new(\n        Queue::\u003cDelayTestJob, DelayTestOutput, DelayTestErrorData, Arc\u003cConnectionManager\u003e\u003e::new(\n            REDIS_URL,\n            \u0026queue_name,\n            Some(queue_options),\n            redis_conn.clone(),\n        )\n        .await\n        .expect(\"Failed to create delay order queue\"),\n    );\n\n    cleanup_redis_keys(\u0026redis_conn, \u0026queue_name).await;\n\n    let created_at = SystemTime::now()\n        .duration_since(UNIX_EPOCH)\n        .unwrap()\n        .as_secs();\n    let short_delay = Duration::from_secs(1);\n\n    // Push jobs in specific order:\n    // 1. Normal job (no delay) - should process first\n    // 2. Delayed job with \"First\" position - should process second when delay expires\n    // 3. Delayed job with \"Last\" position - should process third when delay expires\n\n    // Job 1: Normal job (immediate)\n    let immediate_job = DelayTestJob {\n        job_id: \"immediate\".to_string(),\n        expected_delay_seconds: 0,\n        created_at,\n        test_id: test_id.clone(),\n    };\n\n    queue\n        .clone()\n        .job(immediate_job)\n        .with_id(\"immediate\")\n        .push()\n        .await\n        .unwrap();\n\n    // Job 2: Delayed job with First position\n    let delayed_first_job = DelayTestJob {\n        job_id: \"delayed_first\".to_string(),\n        expected_delay_seconds: short_delay.as_secs(),\n        created_at,\n        test_id: test_id.clone(),\n    };\n\n    let first_delay_options = DelayOptions {\n        delay: short_delay,\n        position: RequeuePosition::First,\n    };\n\n    queue\n        .clone()\n        .job(delayed_first_job)\n        .with_id(\"delayed_first\")\n        .with_delay(first_delay_options)\n        .push()\n        .await\n        .unwrap();\n\n    // Job 3: Delayed job with Last position\n    let delayed_last_job = DelayTestJob {\n        job_id: \"delayed_last\".to_string(),\n        expected_delay_seconds: short_delay.as_secs(),\n        created_at,\n        test_id: test_id.clone(),\n    };\n\n    let last_delay_options = DelayOptions {\n        delay: short_delay,\n        position: RequeuePosition::Last,\n    };\n\n    queue\n        .clone()\n        .job(delayed_last_job)\n        .with_id(\"delayed_last\")\n        .with_delay(last_delay_options)\n        .push()\n        .await\n        .unwrap();\n\n    // Verify initial state\n    let pending_count = queue.count(JobStatus::Pending).await.unwrap();\n    let delayed_count = queue.count(JobStatus::Delayed).await.unwrap();\n\n    tracing::info!(\n        \"Initial state - Pending: {}, Delayed: {}\",\n        pending_count,\n        delayed_count\n    );\n    assert_eq!(pending_count, 1, \"Should have 1 immediate job pending\");\n    assert_eq!(delayed_count, 2, \"Should have 2 delayed jobs\");\n\n    // Start worker\n    let worker = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            if let Err(e) = queue.work().await {\n                tracing::error!(\"Delay order worker failed: {:?}\", e);\n            }\n        })\n    };\n\n    // Wait for all jobs to complete\n    let max_wait = short_delay + Duration::from_secs(3);\n    let start_time = SystemTime::now();\n\n    while start_time.elapsed().unwrap() \u003c max_wait {\n        let success_count = queue.count(JobStatus::Success).await.unwrap();\n        if success_count \u003e= 3 {\n            break;\n        }\n        tokio::time::sleep(Duration::from_millis(100)).await;\n    }\n\n    tokio::time::sleep(Duration::from_millis(200)).await;\n\n    // Check processing order from Redis\n    let order_key = format!(\"test:{}:processing_order\", test_id);\n    let mut redis_conn_direct = redis_conn.as_ref().clone();\n    let processing_order: Vec\u003cString\u003e = redis_conn_direct\n        .lrange(\u0026order_key, 0, -1)\n        .await\n        .unwrap_or_default();\n\n    tracing::info!(\"Processing order: {:?}\", processing_order);\n\n    assert_eq!(processing_order.len(), 3, \"All 3 jobs should be processed\");\n    assert_eq!(\n        processing_order[0], \"immediate\",\n        \"Immediate job should process first\"\n    );\n\n    // The delayed jobs should process in position order: First comes before Last\n    assert_eq!(\n        processing_order[1], \"delayed_first\",\n        \"Delayed job with First position should process before Last position\"\n    );\n    assert_eq!(\n        processing_order[2], \"delayed_last\",\n        \"Delayed job with Last position should process last\"\n    );\n\n    tracing::info!(\"✅ Delay position ordering works correctly!\");\n\n    worker.abort();\n    cleanup_redis_keys(\u0026redis_conn, \u0026queue_name).await;\n\n    // Clean up test-specific keys\n    let _: () = redis_conn_direct.del(\u0026order_key).await.unwrap_or(());\n}\n","traces":[{"line":21,"address":[],"length":0,"stats":{"Line":8}},{"line":22,"address":[],"length":0,"stats":{"Line":4}},{"line":23,"address":[],"length":0,"stats":{"Line":4}},{"line":24,"address":[],"length":0,"stats":{"Line":8}},{"line":25,"address":[],"length":0,"stats":{"Line":4}},{"line":26,"address":[],"length":0,"stats":{"Line":4}},{"line":27,"address":[],"length":0,"stats":{"Line":4}},{"line":29,"address":[],"length":0,"stats":{"Line":4}},{"line":30,"address":[],"length":0,"stats":{"Line":2}},{"line":31,"address":[],"length":0,"stats":{"Line":2}},{"line":32,"address":[],"length":0,"stats":{"Line":2}},{"line":33,"address":[],"length":0,"stats":{"Line":2}},{"line":36,"address":[],"length":0,"stats":{"Line":6}},{"line":65,"address":[],"length":0,"stats":{"Line":4}},{"line":69,"address":[],"length":0,"stats":{"Line":4}},{"line":70,"address":[],"length":0,"stats":{"Line":4}},{"line":73,"address":[],"length":0,"stats":{"Line":4}},{"line":75,"address":[],"length":0,"stats":{"Line":4}},{"line":76,"address":[],"length":0,"stats":{"Line":1}},{"line":82,"address":[],"length":0,"stats":{"Line":4}},{"line":83,"address":[],"length":0,"stats":{"Line":4}},{"line":84,"address":[],"length":0,"stats":{"Line":4}},{"line":85,"address":[],"length":0,"stats":{"Line":4}},{"line":86,"address":[],"length":0,"stats":{"Line":4}},{"line":88,"address":[],"length":0,"stats":{"Line":4}},{"line":92,"address":[],"length":0,"stats":{"Line":4}},{"line":99,"address":[],"length":0,"stats":{"Line":5}},{"line":102,"address":[],"length":0,"stats":{"Line":4}},{"line":105,"address":[],"length":0,"stats":{"Line":4}},{"line":106,"address":[],"length":0,"stats":{"Line":4}},{"line":109,"address":[],"length":0,"stats":{"Line":0}},{"line":110,"address":[],"length":0,"stats":{"Line":0}}],"covered":30,"coverable":32},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","tests","fixtures.rs"],"content":"// In your test file or test module\n\nuse std::sync::atomic::{AtomicBool, Ordering};\nuse std::time::Duration;\n\nuse serde::{Deserialize, Serialize};\n\n// Assuming your library's root is `twmq` or you use `crate::` if in lib.rs\nuse twmq::hooks::TransactionContext;\nuse twmq::job::{Job, JobResult};\nuse twmq::{DurableExecution, SuccessHookData};\n\n// --- Test Job Definition ---\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct TestJobPayload {\n    pub message: String,\n    pub id_to_check: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]\npub struct TestJobOutput {\n    pub reply: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct TestJobErrorData {\n    pub reason: String,\n}\n\n// Use a static AtomicBool to signal from the job process to the test\n// In a real scenario, you'd check queue state or results in Redis.\npub static TEST_JOB_PROCESSED_SUCCESSFULLY: AtomicBool = AtomicBool::new(false);\n\n// If DurableExecution is in the root of your crate (lib.rs)\n// use crate::DurableExecution;\n\n// If DurableExecution is in job.rs and job.rs is a module\n// use crate::job::DurableExecution;\n\n// If using async_trait:\n// #[async_trait]\nimpl DurableExecution for TestJobPayload {\n    type Output = TestJobOutput;\n    type ErrorData = TestJobErrorData;\n    type ExecutionContext = ();\n\n    // If not using async_trait, the signature is:\n    // fn process(\u0026self) -\u003e impl std::future::Future\u003cOutput = JobResult\u003cSelf::Output, Self::ErrorData\u003e\u003e + Send + Sync {\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        _: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        println!(\n            \"TEST_JOB: Processing job with id_to_check: {}\",\n            job.data.id_to_check\n        );\n        // Simulate some work\n        tokio::time::sleep(Duration::from_millis(50)).await;\n        TEST_JOB_PROCESSED_SUCCESSFULLY.store(true, Ordering::SeqCst);\n        JobResult::Success(TestJobOutput {\n            reply: format!(\"Successfully processed '{}'\", job.data.message),\n        })\n    }\n\n    async fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        _d: SuccessHookData\u003c'_, Self::Output\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\n            \"TEST_JOB: on_success hook for id_to_check: {}\",\n            self.id_to_check\n        );\n    }\n}\n","traces":[{"line":50,"address":[],"length":0,"stats":{"Line":1}},{"line":54,"address":[],"length":0,"stats":{"Line":1}},{"line":55,"address":[],"length":0,"stats":{"Line":1}},{"line":56,"address":[],"length":0,"stats":{"Line":1}},{"line":59,"address":[],"length":0,"stats":{"Line":1}},{"line":60,"address":[],"length":0,"stats":{"Line":1}},{"line":61,"address":[],"length":0,"stats":{"Line":1}},{"line":62,"address":[],"length":0,"stats":{"Line":1}},{"line":66,"address":[],"length":0,"stats":{"Line":1}},{"line":73,"address":[],"length":0,"stats":{"Line":1}},{"line":74,"address":[],"length":0,"stats":{"Line":1}}],"covered":11,"coverable":11},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","tests","lease_expiry.rs"],"content":"use std::sync::{\n    Arc,\n    atomic::{AtomicBool, Ordering},\n};\nuse std::time::Duration;\n\nuse serde::{Deserialize, Serialize};\nuse tracing_subscriber::{EnvFilter, layer::SubscriberExt, util::SubscriberInitExt};\n\nuse twmq::{\n    DurableExecution, FailHookData, NackHookData, Queue, SuccessHookData,\n    hooks::TransactionContext,\n    job::{Job, JobResult, JobStatus},\n    queue::QueueOptions,\n    redis::aio::ConnectionManager,\n};\n\nconst REDIS_URL: \u0026str = \"redis://127.0.0.1:6379/\";\n\n// Helper to clean up Redis keys\nasync fn cleanup_redis_keys(conn_manager: \u0026ConnectionManager, queue_name: \u0026str) {\n    let mut conn = conn_manager.clone();\n    let keys_pattern = format!(\"twmq:{}:*\", queue_name);\n    let keys: Vec\u003cString\u003e = redis::cmd(\"KEYS\")\n        .arg(\u0026keys_pattern)\n        .query_async(\u0026mut conn)\n        .await\n        .unwrap_or_default();\n    if !keys.is_empty() {\n        redis::cmd(\"DEL\")\n            .arg(keys)\n            .query_async::\u003c()\u003e(\u0026mut conn)\n            .await\n            .unwrap_or_default();\n    }\n    tracing::info!(\"Cleaned up keys for pattern: {}\", keys_pattern);\n}\n\n// Job that sleeps forever to test lease expiry\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct SleepForeverJob {\n    pub id_to_check: String,\n    pub message: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]\npub struct SleepJobOutput {\n    pub message: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct SleepJobErrorData {\n    pub reason: String,\n}\n\npub static MULTI_JOB_STARTED_PROCESSING: AtomicBool = AtomicBool::new(false);\npub static MULTI_JOB_SHOULD_CONTINUE_SLEEPING: AtomicBool = AtomicBool::new(true);\n\n#[derive(Clone)]\npub struct SleepForeverJobExecutionContext {\n    pub started_processing: Arc\u003cAtomicBool\u003e,\n    pub should_continue_sleeping: Arc\u003cAtomicBool\u003e,\n}\n\nimpl DurableExecution for SleepForeverJob {\n    type Output = SleepJobOutput;\n    type ErrorData = SleepJobErrorData;\n    type ExecutionContext = SleepForeverJobExecutionContext;\n\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        ec: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        tracing::info!(\n            \"SLEEP_JOB: Starting to process job {}, attempt {}\",\n            job.id,\n            job.attempts\n        );\n\n        // Signal that we started processing\n        ec.started_processing.store(true, Ordering::SeqCst);\n\n        // Sleep forever (or until test tells us to stop)\n        while ec.should_continue_sleeping.load(Ordering::SeqCst) {\n            tokio::time::sleep(Duration::from_millis(100)).await;\n        }\n\n        tracing::info!(\"SLEEP_JOB: Job {} woke up, finishing\", job.id);\n\n        JobResult::Success(SleepJobOutput {\n            message: format!(\"Job {} completed after sleeping\", job.id),\n        })\n    }\n\n    async fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: SuccessHookData\u003c'_, Self::Output\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\"SLEEP_JOB: on_success hook - {}\", d.result.message);\n    }\n\n    async fn on_nack(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: NackHookData\u003c'_, Self::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\"SLEEP_JOB: on_nack hook - {}\", d.error.reason);\n        if let Some(delay_duration) = d.delay {\n            tracing::info!(\"Will retry after {:?}\", delay_duration);\n        }\n    }\n\n    async fn on_fail(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: FailHookData\u003c'_, Self::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::error!(\"SLEEP_JOB: on_fail hook - {}\", d.error.reason);\n    }\n\n    async fn on_timeout(\u0026self, _tx: \u0026mut TransactionContext\u003c'_\u003e) {\n        tracing::info!(\"SLEEP_JOB: on_timeout hook - job lease expired\");\n    }\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_job_lease_expiry() {\n    tracing_subscriber::registry()\n        .with(EnvFilter::try_from_default_env().unwrap_or_else(|_| \"twmq=debug\".into()))\n        .with(tracing_subscriber::fmt::layer())\n        .init();\n\n    let queue_name = format!(\"test_lease_{}\", nanoid::nanoid!(6));\n    let job_id = \"sleep_job_001\";\n    let lease_duration = Duration::from_secs(3); // Short lease for testing\n\n    let job_started_processing = Arc::new(AtomicBool::new(false));\n    let job_should_continue_sleeping = Arc::new(AtomicBool::new(true));\n\n    // Reset flags\n    job_started_processing.store(false, Ordering::SeqCst);\n    job_should_continue_sleeping.store(true, Ordering::SeqCst);\n\n    tracing::info!(\"Creating lease expiry queue: {}\", queue_name);\n    tracing::info!(\"Lease duration: {:?}\", lease_duration);\n\n    // Create queue with short lease duration\n    let queue_options = QueueOptions {\n        max_success: 1000,\n        max_failed: 1000,\n        lease_duration,\n        polling_interval: Duration::from_millis(100),\n        local_concurrency: 1,\n        always_poll: true,\n    };\n\n    let queue =\n        Arc::new(\n            Queue::\u003c\n                SleepForeverJob,\n                SleepJobOutput,\n                SleepJobErrorData,\n                SleepForeverJobExecutionContext,\n            \u003e::new(\n                REDIS_URL,\n                \u0026queue_name,\n                Some(queue_options),\n                SleepForeverJobExecutionContext {\n                    started_processing: job_started_processing.clone(),\n                    should_continue_sleeping: job_should_continue_sleeping.clone(),\n                },\n            )\n            .await\n            .expect(\"Failed to create lease expiry queue\"),\n        );\n\n    // Clean up before test\n    cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n\n    // Create job that will sleep forever\n    let sleep_job = SleepForeverJob {\n        id_to_check: job_id.to_string(),\n        message: \"I will sleep until my lease expires\".to_string(),\n    };\n\n    tracing::info!(\"Pushing sleep job with ID: {}\", job_id);\n    queue\n        .clone()\n        .job(sleep_job)\n        .with_id(job_id)\n        .push()\n        .await\n        .expect(\"Failed to push sleep job\");\n\n    // Verify job is initially pending\n    let initial_pending = queue.count(JobStatus::Pending).await.unwrap();\n    let initial_active = queue.count(JobStatus::Active).await.unwrap();\n    tracing::info!(\n        \"Initial state - Pending: {}, Active: {}\",\n        initial_pending,\n        initial_active\n    );\n    assert_eq!(initial_pending, 1, \"Job should start in pending\");\n    assert_eq!(initial_active, 0, \"No jobs should be active initially\");\n\n    // Start worker with concurrency 1 (so job won't get picked up again immediately after expiry)\n    tracing::info!(\"Starting worker with concurrency 1\");\n    let worker = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            if let Err(e) = queue.work().await {\n                tracing::error!(\"Lease worker failed: {:?}\", e);\n            }\n        })\n    };\n\n    // Wait for job to start processing\n    let mut job_started = false;\n    for i in 0..50 {\n        if job_started_processing.load(Ordering::SeqCst) {\n            job_started = true;\n            tracing::info!(\"Job started processing after {} polling attempts\", i + 1);\n            break;\n        }\n        tokio::time::sleep(Duration::from_millis(100)).await;\n    }\n    assert!(job_started, \"Job should have started processing\");\n\n    // Job should now be active\n    tokio::time::sleep(Duration::from_millis(200)).await;\n    let active_count = queue.count(JobStatus::Active).await.unwrap();\n    let pending_count = queue.count(JobStatus::Pending).await.unwrap();\n    tracing::info!(\n        \"After job start - Pending: {}, Active: {}\",\n        pending_count,\n        active_count\n    );\n    assert_eq!(active_count, 1, \"Job should be active while processing\");\n    assert_eq!(\n        pending_count, 0,\n        \"No jobs should be pending while one is active\"\n    );\n\n    // Wait for lease to expire (lease duration + some buffer)\n    let wait_time = lease_duration + Duration::from_secs(2);\n    tracing::info!(\"Waiting {:?} for lease to expire...\", wait_time);\n    tokio::time::sleep(wait_time).await;\n\n    // Job should have moved back to pending due to lease expiry\n    let expired_active = queue.count(JobStatus::Active).await.unwrap();\n    let expired_pending = queue.count(JobStatus::Pending).await.unwrap();\n    tracing::info!(\n        \"After lease expiry - Pending: {}, Active: {}\",\n        expired_pending,\n        expired_active\n    );\n\n    assert_eq!(\n        expired_active, 0,\n        \"Job should no longer be active after lease expiry\"\n    );\n    assert_eq!(\n        expired_pending, 1,\n        \"Job should be back in pending after lease expiry\"\n    );\n\n    // Verify the job metadata shows increased attempts\n    let job_after_expiry = queue\n        .get_job(job_id)\n        .await\n        .expect(\"Failed to fetch job\")\n        .expect(\"Job should exist\");\n\n    tracing::info!(\"Job state after lease expiry:\");\n    tracing::info!(\"  Job ID: {}\", job_after_expiry.id);\n    tracing::info!(\"  Attempts: {}\", job_after_expiry.attempts);\n    tracing::info!(\"  Created at: {}\", job_after_expiry.created_at);\n    tracing::info!(\"  Processed at: {:?}\", job_after_expiry.processed_at);\n\n    // Job should have at least 2 attempts (original + after lease expiry)\n    assert!(\n        job_after_expiry.attempts \u003e= 2,\n        \"Job should have at least 2 attempts after lease expiry, but had {}\",\n        job_after_expiry.attempts\n    );\n\n    tracing::info!(\"✅ Lease expiry mechanism works correctly!\");\n    tracing::info!(\"Job moved from active back to pending after lease expired\");\n\n    // Stop the sleeping job and cleanup\n    job_should_continue_sleeping.store(false, Ordering::SeqCst);\n    worker.abort();\n    cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_multiple_job_lease_expiry() {\n    // Test that multiple jobs can have their leases expire independently\n\n    let queue_name = format!(\"test_multi_lease_{}\", nanoid::nanoid!(6));\n    let lease_duration = Duration::from_secs(2);\n\n    let job_started_processing = Arc::new(AtomicBool::new(false));\n    let job_should_continue_sleeping = Arc::new(AtomicBool::new(true));\n\n    // Reset flags\n    job_started_processing.store(false, Ordering::SeqCst);\n    job_should_continue_sleeping.store(true, Ordering::SeqCst);\n\n    tracing::info!(\"\\n=== Testing multiple job lease expiry ===\");\n\n    let queue_options = QueueOptions {\n        max_success: 1000,\n        max_failed: 1000,\n        local_concurrency: 3,\n        lease_duration,\n        polling_interval: Duration::from_millis(100),\n        always_poll: true,\n    };\n\n    let queue =\n        Arc::new(\n            Queue::\u003c\n                SleepForeverJob,\n                SleepJobOutput,\n                SleepJobErrorData,\n                SleepForeverJobExecutionContext,\n            \u003e::new(\n                REDIS_URL,\n                \u0026queue_name,\n                Some(queue_options),\n                SleepForeverJobExecutionContext {\n                    started_processing: job_started_processing.clone(),\n                    should_continue_sleeping: job_should_continue_sleeping.clone(),\n                },\n            )\n            .await\n            .expect(\"Failed to create multi-lease queue\"),\n        );\n\n    cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n\n    // Push multiple jobs\n    let job_ids = vec![\"multi_job_1\", \"multi_job_2\", \"multi_job_3\"];\n    for job_id in job_ids {\n        let sleep_job = SleepForeverJob {\n            id_to_check: job_id.to_string(),\n            message: format!(\"Multi-job test: {}\", job_id),\n        };\n\n        queue\n            .clone()\n            .job(sleep_job)\n            .with_id(job_id)\n            .push()\n            .await\n            .expect(\"Failed to push multi-job\");\n    }\n\n    // Start worker with higher concurrency to process multiple jobs\n    let worker = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            if let Err(e) = queue.work().await {\n                tracing::error!(\"Multi-lease worker failed: {:?}\", e);\n            }\n        })\n    };\n\n    // Wait a bit for jobs to start\n    tokio::time::sleep(Duration::from_millis(500)).await;\n\n    // All jobs should be active\n    let active_count = queue.count(JobStatus::Active).await.unwrap();\n    let pending_count = queue.count(JobStatus::Pending).await.unwrap();\n    tracing::info!(\n        \"During processing - Pending: {}, Active: {}\",\n        pending_count,\n        active_count\n    );\n\n    assert_eq!(pending_count + active_count, 3, \"Total jobs should be 3\");\n\n    // Wait for leases to expire\n    let wait_time = lease_duration + Duration::from_secs(1);\n    tracing::info!(\"Waiting {:?} for leases to expire...\", wait_time);\n    tokio::time::sleep(wait_time).await;\n\n    // All jobs should be back to pending\n    let final_active = queue.count(JobStatus::Active).await.unwrap();\n    let final_pending = queue.count(JobStatus::Pending).await.unwrap();\n    tracing::info!(\n        \"After lease expiry - Pending: {}, Active: {}\",\n        final_pending,\n        final_active\n    );\n\n    assert_eq!(\n        final_active, 0,\n        \"No jobs should be active after lease expiry\"\n    );\n    assert_eq!(final_pending, 3, \"All jobs should be back in pending\");\n\n    tracing::info!(\"✅ Multiple job lease expiry works correctly!\");\n\n    // Cleanup\n    job_should_continue_sleeping.store(false, Ordering::SeqCst);\n    worker.abort();\n    cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n}\n","traces":[{"line":21,"address":[],"length":0,"stats":{"Line":8}},{"line":22,"address":[],"length":0,"stats":{"Line":4}},{"line":23,"address":[],"length":0,"stats":{"Line":4}},{"line":24,"address":[],"length":0,"stats":{"Line":8}},{"line":25,"address":[],"length":0,"stats":{"Line":4}},{"line":26,"address":[],"length":0,"stats":{"Line":4}},{"line":27,"address":[],"length":0,"stats":{"Line":4}},{"line":29,"address":[],"length":0,"stats":{"Line":4}},{"line":30,"address":[],"length":0,"stats":{"Line":2}},{"line":31,"address":[],"length":0,"stats":{"Line":2}},{"line":32,"address":[],"length":0,"stats":{"Line":2}},{"line":33,"address":[],"length":0,"stats":{"Line":2}},{"line":36,"address":[],"length":0,"stats":{"Line":8}},{"line":70,"address":[],"length":0,"stats":{"Line":4}},{"line":74,"address":[],"length":0,"stats":{"Line":4}},{"line":75,"address":[],"length":0,"stats":{"Line":4}},{"line":81,"address":[],"length":0,"stats":{"Line":4}},{"line":84,"address":[],"length":0,"stats":{"Line":158}},{"line":85,"address":[],"length":0,"stats":{"Line":158}},{"line":88,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":102,"address":[],"length":0,"stats":{"Line":0}},{"line":105,"address":[],"length":0,"stats":{"Line":0}},{"line":112,"address":[],"length":0,"stats":{"Line":0}},{"line":113,"address":[],"length":0,"stats":{"Line":0}},{"line":114,"address":[],"length":0,"stats":{"Line":0}},{"line":118,"address":[],"length":0,"stats":{"Line":0}},{"line":125,"address":[],"length":0,"stats":{"Line":0}},{"line":128,"address":[],"length":0,"stats":{"Line":0}},{"line":129,"address":[],"length":0,"stats":{"Line":0}}],"covered":19,"coverable":30},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","tests","nack.rs"],"content":"// tests/nack.rs\n\nuse std::sync::{\n    Arc,\n    atomic::{AtomicBool, Ordering},\n};\nuse std::time::Duration;\n\nuse redis::AsyncCommands;\nuse serde::{Deserialize, Serialize};\nuse tracing_subscriber::{EnvFilter, layer::SubscriberExt, util::SubscriberInitExt};\n\nuse twmq::{\n    DurableExecution, FailHookData, NackHookData, Queue, SuccessHookData,\n    hooks::TransactionContext,\n    job::{Job, JobResult, JobStatus, RequeuePosition},\n    queue::QueueOptions,\n    redis::aio::ConnectionManager,\n};\n\nconst REDIS_URL: \u0026str = \"redis://127.0.0.1:6379/\";\n\n// Helper to clean up Redis keys\nasync fn cleanup_redis_keys(conn_manager: \u0026ConnectionManager, queue_name: \u0026str) {\n    let mut conn = conn_manager.clone();\n    let keys_pattern = format!(\"twmq:{}:*\", queue_name);\n    let keys: Vec\u003cString\u003e = redis::cmd(\"KEYS\")\n        .arg(\u0026keys_pattern)\n        .query_async(\u0026mut conn)\n        .await\n        .unwrap_or_default();\n    if !keys.is_empty() {\n        redis::cmd(\"DEL\")\n            .arg(keys)\n            .query_async::\u003c()\u003e(\u0026mut conn)\n            .await\n            .unwrap_or_default();\n    }\n    tracing::info!(\"Cleaned up keys for pattern: {}\", keys_pattern);\n}\n\n// Job that retries until reaching desired attempts\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct RetryJobPayload {\n    pub id_to_check: String,\n    pub desired_attempts: u32,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]\npub struct RetryJobOutput {\n    pub final_attempt: u32,\n    pub message: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct RetryJobErrorData {\n    pub attempt: u32,\n    pub reason: String,\n}\n\npub static RETRY_JOB_FINAL_SUCCESS: AtomicBool = AtomicBool::new(false);\n\nimpl DurableExecution for RetryJobPayload {\n    type Output = RetryJobOutput;\n    type ErrorData = RetryJobErrorData;\n    type ExecutionContext = ();\n\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        _: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        let current_attempt = job.attempts;\n\n        tracing::info!(\n            \"RETRY_JOB: Processing job {}, attempt {}/{}\",\n            job.id,\n            current_attempt,\n            job.data.desired_attempts\n        );\n\n        if current_attempt \u003c job.data.desired_attempts {\n            // Not enough attempts yet, nack it\n            tracing::info!(\n                \"RETRY_JOB: Nacking job {} (attempt {}/{})\",\n                job.id,\n                current_attempt,\n                job.data.desired_attempts\n            );\n\n            JobResult::Nack {\n                error: RetryJobErrorData {\n                    attempt: current_attempt,\n                    reason: format!(\n                        \"Need {} attempts, only at {}\",\n                        job.data.desired_attempts, current_attempt\n                    ),\n                },\n                delay: None,\n                position: RequeuePosition::Last,\n            }\n        } else {\n            // Reached desired attempts, succeed!\n            tracing::info!(\n                \"RETRY_JOB: Success on attempt {}/{}\",\n                current_attempt,\n                job.data.desired_attempts\n            );\n\n            RETRY_JOB_FINAL_SUCCESS.store(true, Ordering::SeqCst);\n\n            JobResult::Success(RetryJobOutput {\n                final_attempt: current_attempt,\n                message: format!(\"Succeeded after {} attempts\", current_attempt),\n            })\n        }\n    }\n\n    async fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: SuccessHookData\u003c'_, Self::Output\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\n            \"RETRY_JOB: on_success hook - final attempt was {}\",\n            d.result.final_attempt\n        );\n    }\n\n    async fn on_nack(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: NackHookData\u003c'_, Self::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\n            \"RETRY_JOB: on_nack hook - attempt {} failed: {}\",\n            d.error.attempt,\n            d.error.reason\n        );\n        if let Some(delay_duration) = d.delay {\n            tracing::info!(\"Will retry after {:?}\", delay_duration);\n        }\n    }\n\n    async fn on_fail(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: FailHookData\u003c'_, Self::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::error!(\n            \"RETRY_JOB: on_fail hook - permanently failed at attempt {}\",\n            d.error.attempt\n        );\n    }\n\n    async fn on_timeout(\u0026self, _tx: \u0026mut TransactionContext\u003c'_\u003e) {\n        tracing::info!(\"RETRY_JOB: on_timeout hook\");\n    }\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_job_retry_attempts() {\n    tracing_subscriber::registry()\n        .with(\n            EnvFilter::try_from_default_env()\n                .unwrap_or_else(|_| \"thirdweb_engine=debug,tower_http=debug,axum=debug\".into()),\n        )\n        .with(tracing_subscriber::fmt::layer())\n        .init();\n\n    let queue_name = format!(\"test_retry_{}\", nanoid::nanoid!(6));\n    let job_id = \"retry_job_001\";\n    let desired_attempts = 4u32; // Job should nack 3 times, then succeed on 4th attempt\n\n    // Reset counters\n    RETRY_JOB_FINAL_SUCCESS.store(false, Ordering::SeqCst);\n\n    tracing::info!(\"Creating retry queue: {}\", queue_name);\n    tracing::info!(\"Job should succeed after {} attempts\", desired_attempts);\n\n    let mut queue_options = QueueOptions::default();\n    queue_options.local_concurrency = 1;\n\n    let queue = Arc::new(\n        Queue::\u003cRetryJobPayload, RetryJobOutput, RetryJobErrorData, ()\u003e::new(\n            REDIS_URL,\n            \u0026queue_name,\n            Some(queue_options),\n            (),\n        )\n        .await\n        .expect(\"Failed to create retry queue\"),\n    );\n\n    // Clean up before test\n    cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n\n    // Create job that requires multiple attempts\n    let retry_job = RetryJobPayload {\n        id_to_check: job_id.to_string(),\n        desired_attempts,\n    };\n\n    tracing::info!(\"Pushing retry job with ID: {}\", job_id);\n    queue\n        .clone()\n        .job(retry_job)\n        .with_id(job_id)\n        .push()\n        .await\n        .expect(\"Failed to push retry job\");\n\n    // Start worker\n    tracing::info!(\"Starting worker\");\n    let worker = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            if let Err(e) = queue.work().await {\n                tracing::error!(\"Retry worker failed: {:?}\", e);\n            }\n        })\n    };\n\n    // Wait for job to complete (with retries)\n    let mut final_success = false;\n    for i in 0..100 {\n        // Give it plenty of time for retries\n        if RETRY_JOB_FINAL_SUCCESS.load(Ordering::SeqCst) {\n            final_success = true;\n            tracing::info!(\n                \"Retry job finally succeeded after {} polling attempts\",\n                i + 1\n            );\n            break;\n        }\n        tokio::time::sleep(Duration::from_millis(200)).await;\n    }\n\n    assert!(final_success, \"Retry job should eventually succeed\");\n\n    // Give a moment for final processing\n    tokio::time::sleep(Duration::from_millis(300)).await;\n\n    // Verify job is in success list\n    let success_count = queue.count(JobStatus::Success).await.unwrap();\n    assert_eq!(success_count, 1, \"Job should be in success list\");\n\n    // Fetch the completed job and verify attempt count\n    let completed_job = queue\n        .get_job(job_id)\n        .await\n        .expect(\"Failed to fetch job\")\n        .expect(\"Job should exist\");\n\n    tracing::info!(\"Final job state:\");\n    tracing::info!(\"  Job ID: {}\", completed_job.id);\n    tracing::info!(\"  Attempts: {}\", completed_job.attempts);\n    tracing::info!(\"  Created at: {}\", completed_job.created_at);\n    tracing::info!(\"  Processed at: {:?}\", completed_job.processed_at);\n    tracing::info!(\"  Finished at: {:?}\", completed_job.finished_at);\n\n    // Verify the job took exactly the desired number of attempts\n    assert_eq!(\n        completed_job.attempts, desired_attempts,\n        \"Job should have exactly {} attempts, but had {}\",\n        desired_attempts, completed_job.attempts\n    );\n\n    // Verify the job result contains the correct attempt count\n    let mut redis_conn = queue.redis.clone();\n    let result_json: Option\u003cString\u003e = redis_conn\n        .hget(queue.job_result_hash_name(), job_id)\n        .await\n        .expect(\"Failed to get job result\");\n\n    assert!(result_json.is_some(), \"Job result should be stored\");\n\n    let job_output: RetryJobOutput =\n        serde_json::from_str(\u0026result_json.unwrap()).expect(\"Failed to deserialize job result\");\n\n    assert_eq!(\n        job_output.final_attempt, desired_attempts,\n        \"Job result should show final attempt as {}\",\n        desired_attempts\n    );\n\n    tracing::info!(\"✅ Retry mechanism works correctly!\");\n    tracing::info!(\n        \"Job succeeded on attempt {} as expected\",\n        job_output.final_attempt\n    );\n    tracing::info!(\"Result message: {}\", job_output.message);\n\n    // Cleanup\n    worker.abort();\n    cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_different_retry_counts() {\n    // Test multiple different retry counts to ensure it works consistently\n    let test_cases = vec![1, 2, 3, 5, 7];\n\n    for desired_attempts in test_cases {\n        tracing::info!(\"\\n=== Testing {} attempts ===\", desired_attempts);\n\n        let queue_name = format!(\"test_retry_{}_{}\", desired_attempts, nanoid::nanoid!(4));\n        let job_id = format!(\"retry_job_{}\", desired_attempts);\n\n        // Reset counters\n        RETRY_JOB_FINAL_SUCCESS.store(false, Ordering::SeqCst);\n\n        let mut queue_options = QueueOptions::default();\n        queue_options.local_concurrency = 1;\n\n        let queue = Arc::new(\n            Queue::\u003cRetryJobPayload, RetryJobOutput, RetryJobErrorData, ()\u003e::new(\n                REDIS_URL,\n                \u0026queue_name,\n                Some(queue_options),\n                (),\n            )\n            .await\n            .expect(\"Failed to create retry queue\"),\n        );\n\n        cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n\n        let retry_job = RetryJobPayload {\n            id_to_check: job_id.clone(),\n            desired_attempts,\n        };\n\n        queue\n            .clone()\n            .job(retry_job)\n            .with_id(\u0026job_id)\n            .push()\n            .await\n            .expect(\"Failed to push retry job\");\n\n        let worker = {\n            let queue = queue.clone();\n            tokio::spawn(async move {\n                if let Err(e) = queue.work().await {\n                    tracing::error!(\"Worker failed: {:?}\", e);\n                }\n            })\n        };\n\n        // Wait for completion\n        for _ in 0..50 {\n            if RETRY_JOB_FINAL_SUCCESS.load(Ordering::SeqCst) {\n                break;\n            }\n            tokio::time::sleep(Duration::from_millis(200)).await;\n        }\n\n        tokio::time::sleep(Duration::from_millis(200)).await;\n\n        let completed_job = queue.get_job(\u0026job_id).await.unwrap().unwrap();\n        assert_eq!(completed_job.attempts, desired_attempts);\n\n        worker.abort();\n        cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n\n        tracing::info!(\"✅ {} attempts test passed\", desired_attempts);\n    }\n\n    tracing::info!(\"\\n✅ All retry count tests passed!\");\n}\n","traces":[{"line":24,"address":[],"length":0,"stats":{"Line":24}},{"line":25,"address":[],"length":0,"stats":{"Line":12}},{"line":26,"address":[],"length":0,"stats":{"Line":12}},{"line":27,"address":[],"length":0,"stats":{"Line":24}},{"line":28,"address":[],"length":0,"stats":{"Line":12}},{"line":29,"address":[],"length":0,"stats":{"Line":12}},{"line":30,"address":[],"length":0,"stats":{"Line":12}},{"line":32,"address":[],"length":0,"stats":{"Line":12}},{"line":33,"address":[],"length":0,"stats":{"Line":6}},{"line":34,"address":[],"length":0,"stats":{"Line":6}},{"line":35,"address":[],"length":0,"stats":{"Line":6}},{"line":36,"address":[],"length":0,"stats":{"Line":6}},{"line":39,"address":[],"length":0,"stats":{"Line":14}},{"line":68,"address":[],"length":0,"stats":{"Line":22}},{"line":72,"address":[],"length":0,"stats":{"Line":22}},{"line":74,"address":[],"length":0,"stats":{"Line":22}},{"line":75,"address":[],"length":0,"stats":{"Line":4}},{"line":81,"address":[],"length":0,"stats":{"Line":22}},{"line":83,"address":[],"length":0,"stats":{"Line":16}},{"line":84,"address":[],"length":0,"stats":{"Line":3}},{"line":91,"address":[],"length":0,"stats":{"Line":16}},{"line":103,"address":[],"length":0,"stats":{"Line":6}},{"line":104,"address":[],"length":0,"stats":{"Line":1}},{"line":118,"address":[],"length":0,"stats":{"Line":6}},{"line":125,"address":[],"length":0,"stats":{"Line":6}},{"line":126,"address":[],"length":0,"stats":{"Line":1}},{"line":131,"address":[],"length":0,"stats":{"Line":16}},{"line":138,"address":[],"length":0,"stats":{"Line":16}},{"line":139,"address":[],"length":0,"stats":{"Line":3}},{"line":143,"address":[],"length":0,"stats":{"Line":16}},{"line":144,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":162,"address":[],"length":0,"stats":{"Line":0}}],"covered":30,"coverable":36}]};
        var previousData = {"files":[{"path":["/","Users","d4mr","work","thirdweb","engine-core","aa-core","src","account_factory","chained.rs"],"content":"use alloy::{\n    primitives::{Address, Bytes},\n    sol,\n};\nuse engine_core::{\n    chain::Chain,\n    error::{ContractErrorToEngineError, EngineError},\n};\n\nuse super::AccountFactory;\n\npub struct ChainedAccountFactory\u003c'a, C: Chain\u003e {\n    pub chain: \u0026'a C,\n    pub factory_address: Address,\n}\n\nsol! {\n    #[sol(rpc)]\n    contract AccountFactoryContract {\n        function getAddress(address _adminSigner, bytes _data) view returns (address);\n    }\n}\n\nimpl\u003cC: Chain\u003e AccountFactory for ChainedAccountFactory\u003c'_, C\u003e {\n    fn factory_address(\u0026self) -\u003e \u0026Address {\n        \u0026self.factory_address\n    }\n\n    async fn predict_address(\n        \u0026self,\n        signer: \u0026Address,\n        salt_data: \u0026Bytes,\n    ) -\u003e Result\u003cAddress, EngineError\u003e {\n        let account_factory_contract =\n            AccountFactoryContract::new(self.factory_address, self.chain.provider().clone());\n\n        let predicted_address = account_factory_contract\n            .getAddress(signer.to_owned(), salt_data.to_owned())\n            .call()\n            .await\n            .map_err(|e| e.to_engine_error(self.chain.chain_id(), Some(self.factory_address)))?;\n\n        Ok(predicted_address)\n    }\n}\n","traces":[{"line":25,"address":[],"length":0,"stats":{"Line":0}},{"line":26,"address":[],"length":0,"stats":{"Line":0}},{"line":29,"address":[],"length":0,"stats":{"Line":0}},{"line":34,"address":[],"length":0,"stats":{"Line":0}},{"line":35,"address":[],"length":0,"stats":{"Line":0}},{"line":37,"address":[],"length":0,"stats":{"Line":0}},{"line":38,"address":[],"length":0,"stats":{"Line":0}},{"line":40,"address":[],"length":0,"stats":{"Line":0}},{"line":41,"address":[],"length":0,"stats":{"Line":0}},{"line":43,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":10},{"path":["/","Users","d4mr","work","thirdweb","engine-core","aa-core","src","account_factory","default.rs"],"content":"use alloy::primitives::{Address, Bytes};\nuse engine_core::{\n    constants::{\n        DEFAULT_FACTORY_ADDRESS_V0_6, DEFAULT_FACTORY_ADDRESS_V0_7,\n        DEFAULT_IMPLEMENTATION_ADDRESS_V0_6, DEFAULT_IMPLEMENTATION_ADDRESS_V0_7,\n    },\n    error::EngineError,\n};\n\nuse super::{AccountFactory, SyncAccountFactory, utils};\n\npub struct DefaultAccountFactory {\n    pub factory_address: Address,\n    pub implementation_address: Address,\n}\n\nimpl DefaultAccountFactory {\n    /// Create new factory with default address\n    pub fn v0_6() -\u003e Self {\n        Self {\n            factory_address: DEFAULT_FACTORY_ADDRESS_V0_6,\n            implementation_address: DEFAULT_IMPLEMENTATION_ADDRESS_V0_6,\n        }\n    }\n\n    /// Create with custom factory address\n    pub fn v0_7() -\u003e Self {\n        Self {\n            factory_address: DEFAULT_FACTORY_ADDRESS_V0_7,\n            implementation_address: DEFAULT_IMPLEMENTATION_ADDRESS_V0_7,\n        }\n    }\n\n    /// Create with custom factory and implementation addresses\n    pub fn with_addresses(factory_address: Address, implementation_address: Address) -\u003e Self {\n        Self {\n            factory_address,\n            implementation_address,\n        }\n    }\n}\n\nimpl SyncAccountFactory for DefaultAccountFactory {\n    fn predict_address_sync(\u0026self, signer: \u0026Address, salt_data: \u0026Bytes) -\u003e Address {\n        let salt = utils::generate_salt(signer, salt_data);\n        utils::predict_deterministic_address(\n            self.implementation_address,\n            salt,\n            self.factory_address,\n        )\n    }\n}\n\n// Add unified trait implementation for the sync type\nimpl AccountFactory for DefaultAccountFactory {\n    fn factory_address(\u0026self) -\u003e \u0026Address {\n        \u0026self.factory_address\n    }\n\n    fn predict_address(\n        \u0026self,\n        signer: \u0026Address,\n        salt_data: \u0026Bytes,\n    ) -\u003e impl Future\u003cOutput = Result\u003cAddress, EngineError\u003e\u003e {\n        // Use the sync implementation but return as a ready future\n        let address = SyncAccountFactory::predict_address_sync(self, signer, salt_data);\n        std::future::ready(Ok(address))\n    }\n}\n\n#[cfg(test)]\nmod tests {\n\n    use alloy::hex::FromHex;\n    use alloy::primitives::{Bytes, address};\n\n    use super::*;\n\n    #[test]\n    fn test_v07_address_prediction() {\n        // Create factory with default addresses\n        let factory = DefaultAccountFactory::v0_7();\n\n        // Test inputs\n        let signer = address!(\"0xbe2D2B388635D33b0C9C6d60dE9853716e4b51A3\");\n        let empty_salt = Bytes::from_hex(\"0x\").unwrap();\n\n        // Expected output\n        let expected_address = address!(\"0xDA15403AF9690C74f30eCC9cCa686fCAD2C897f8\");\n\n        // Predict address\n        let predicted_address = factory.predict_address_sync(\u0026signer, \u0026empty_salt);\n\n        // Verify result\n        assert_eq!(predicted_address, expected_address);\n    }\n\n    #[test]\n    fn test_v06_address_prediction() {\n        // Create factory with default addresses\n        let factory = DefaultAccountFactory::v0_6();\n\n        // Test inputs\n        let signer = address!(\"0xbe2D2B388635D33b0C9C6d60dE9853716e4b51A3\");\n        let empty_salt = Bytes::from_hex(\"0x\").unwrap();\n\n        // Expected output\n        let expected_address = address!(\"0xB7E052ec0BC8B741Ce7cA7B7dFBaECb4B234ffBE\");\n\n        // Predict address\n        let predicted_address = factory.predict_address_sync(\u0026signer, \u0026empty_salt);\n\n        // Verify result\n        assert_eq!(predicted_address, expected_address);\n    }\n}\n","traces":[{"line":19,"address":[],"length":0,"stats":{"Line":1}},{"line":27,"address":[],"length":0,"stats":{"Line":1}},{"line":35,"address":[],"length":0,"stats":{"Line":0}},{"line":44,"address":[],"length":0,"stats":{"Line":2}},{"line":45,"address":[],"length":0,"stats":{"Line":2}},{"line":47,"address":[],"length":0,"stats":{"Line":2}},{"line":48,"address":[],"length":0,"stats":{"Line":2}},{"line":49,"address":[],"length":0,"stats":{"Line":2}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":60,"address":[],"length":0,"stats":{"Line":0}},{"line":66,"address":[],"length":0,"stats":{"Line":0}},{"line":67,"address":[],"length":0,"stats":{"Line":0}}],"covered":7,"coverable":13},{"path":["/","Users","d4mr","work","thirdweb","engine-core","aa-core","src","account_factory","mod.rs"],"content":"use alloy::{\n    primitives::{Address, Bytes},\n    sol,\n    sol_types::SolCall,\n};\n\nmod default;\nmod utils;\n\npub mod chained;\nuse chained::ChainedAccountFactory;\npub use default::*;\nuse engine_core::{\n    chain::Chain,\n    constants::{DEFAULT_FACTORY_ADDRESS_V0_6, DEFAULT_FACTORY_ADDRESS_V0_7},\n    error::EngineError,\n};\n\n/// Interface for smart account factory implementations\npub trait SyncAccountFactory {\n    /// Predicts the smart account address for a given signer\n    fn predict_address_sync(\u0026self, signer: \u0026Address, salt_data: \u0026Bytes) -\u003e Address;\n}\n\nsol! {\n    function createAccount(address admin, bytes salt) returns (address);\n}\n\npub trait AccountFactory {\n    fn factory_address(\u0026self) -\u003e \u0026Address;\n\n    fn predict_address(\n        \u0026self,\n        signer: \u0026Address,\n        salt_data: \u0026Bytes,\n    ) -\u003e impl Future\u003cOutput = Result\u003cAddress, EngineError\u003e\u003e;\n\n    fn init_calldata(\u0026self, signer: Address, salt_data: Bytes) -\u003e Vec\u003cu8\u003e {\n        createAccountCall {\n            admin: signer,\n            salt: salt_data,\n        }\n        .abi_encode()\n    }\n}\n\n/// A factory that can use either implementation based on the provided addresses\npub enum SmartAccountFactory\u003c'a, C: Chain\u003e {\n    Default(DefaultAccountFactory),\n    Chained(ChainedAccountFactory\u003c'a, C\u003e),\n}\n\n// Implement the AccountFactory trait for our enum\nimpl\u003c'a, C: Chain\u003e AccountFactory for SmartAccountFactory\u003c'a, C\u003e {\n    fn factory_address(\u0026self) -\u003e \u0026Address {\n        match self {\n            Self::Default(factory) =\u003e \u0026factory.factory_address,\n            Self::Chained(factory) =\u003e \u0026factory.factory_address,\n        }\n    }\n    // Assuming your AccountFactory trait has this signature:\n    async fn predict_address(\n        \u0026self,\n        signer: \u0026Address,\n        salt_data: \u0026Bytes,\n    ) -\u003e Result\u003cAddress, EngineError\u003e {\n        match self {\n            Self::Default(factory) =\u003e factory.predict_address(signer, salt_data).await,\n            Self::Chained(factory) =\u003e factory.predict_address(signer, salt_data).await,\n        }\n    }\n}\n\n/// Get the appropriate account factory based on the factory address\npub fn get_account_factory\u003c'a, C: Chain\u003e(\n    chain: \u0026'a C,\n    factory_address: Address,\n    implementation_address: Option\u003cAddress\u003e,\n) -\u003e SmartAccountFactory\u003c'a, C\u003e {\n    // Check if the factory address matches default v0.6\n    if factory_address == DEFAULT_FACTORY_ADDRESS_V0_6 {\n        SmartAccountFactory::Default(DefaultAccountFactory::v0_6())\n    }\n    // Check if the factory address matches default v0.7\n    else if factory_address == DEFAULT_FACTORY_ADDRESS_V0_7 {\n        SmartAccountFactory::Default(DefaultAccountFactory::v0_7())\n    }\n    // For custom factory addresses with known implementation\n    else if let Some(implementation) = implementation_address {\n        SmartAccountFactory::Default(DefaultAccountFactory::with_addresses(\n            factory_address,\n            implementation,\n        ))\n    }\n    // For custom factory addresses with unknown implementation\n    else {\n        SmartAccountFactory::Chained(ChainedAccountFactory {\n            chain,\n            factory_address,\n        })\n    }\n}\n","traces":[{"line":38,"address":[],"length":0,"stats":{"Line":0}},{"line":55,"address":[],"length":0,"stats":{"Line":0}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":67,"address":[],"length":0,"stats":{"Line":0}},{"line":68,"address":[],"length":0,"stats":{"Line":0}},{"line":69,"address":[],"length":0,"stats":{"Line":0}},{"line":75,"address":[],"length":0,"stats":{"Line":0}},{"line":81,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[],"length":0,"stats":{"Line":0}},{"line":85,"address":[],"length":0,"stats":{"Line":0}},{"line":86,"address":[],"length":0,"stats":{"Line":0}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":91,"address":[],"length":0,"stats":{"Line":0}},{"line":92,"address":[],"length":0,"stats":{"Line":0}},{"line":96,"address":[],"length":0,"stats":{"Line":0}},{"line":97,"address":[],"length":0,"stats":{"Line":0}},{"line":98,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":22},{"path":["/","Users","d4mr","work","thirdweb","engine-core","aa-core","src","account_factory","utils.rs"],"content":"use alloy::{\n    primitives::{Address, B256, Bytes, bytes, keccak256},\n    sol_types::SolValue,\n};\n\npub fn generate_salt(admin: \u0026Address, data: \u0026Bytes) -\u003e B256 {\n    keccak256((admin, data).abi_encode_params())\n}\n\n/// Predicts the deterministic address of a contract deployed using CREATE2\n/// Equivalent to the Solidity inline assembly implementation\n///\n/// # Arguments\n///\n/// * `implementation` - The address of the implementation contract\n/// * `salt` - The salt value used in CREATE2\n/// * `deployer` - The address of the deployer contract/EOA\n///\n/// # Returns\n///\n/// * The predicted contract address\npub fn predict_deterministic_address(\n    implementation: Address,\n    salt: B256,\n    deployer: Address,\n) -\u003e Address {\n    let code_prefix = bytes!(\"0x3d602d80600a3d3981f3363d3d373d3d3d363d73\");\n    let code_suffix = bytes!(\"0x5af43d82803e903d91602b57fd5bf3\");\n\n    // Create the exact 55-byte init code that OpenZeppelin uses\n    let mut init_code = Vec::with_capacity(55); // 0x37 bytes\n\n    // Start with prefix, but skip the first 9 bytes\n    init_code.extend_from_slice(\u0026code_prefix);\n\n    // Add the implementation address\n    init_code.extend_from_slice(\u0026implementation.as_slice());\n\n    // Add the suffix, but without the final 'ff' byte\n    // The suffix in your constant is 16 bytes, but OpenZeppelin uses 15 bytes\n    init_code.extend_from_slice(\u0026code_suffix);\n\n    // Calculate init code hash and CREATE2 address\n    let init_code_hash = keccak256(\u0026init_code);\n    deployer.create2(salt, init_code_hash)\n}\n","traces":[{"line":6,"address":[],"length":0,"stats":{"Line":2}},{"line":7,"address":[],"length":0,"stats":{"Line":2}},{"line":22,"address":[],"length":0,"stats":{"Line":2}},{"line":27,"address":[],"length":0,"stats":{"Line":2}},{"line":28,"address":[],"length":0,"stats":{"Line":2}},{"line":31,"address":[],"length":0,"stats":{"Line":2}},{"line":34,"address":[],"length":0,"stats":{"Line":2}},{"line":37,"address":[],"length":0,"stats":{"Line":2}},{"line":41,"address":[],"length":0,"stats":{"Line":2}},{"line":44,"address":[],"length":0,"stats":{"Line":2}},{"line":45,"address":[],"length":0,"stats":{"Line":2}}],"covered":11,"coverable":11},{"path":["/","Users","d4mr","work","thirdweb","engine-core","aa-core","src","lib.rs"],"content":"pub mod account_factory;\npub mod smart_account;\npub mod userop;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","aa-core","src","smart_account","mod.rs"],"content":"use alloy::{\n    primitives::{Address, Bytes},\n    providers::Provider,\n    sol,\n    sol_types::SolCall,\n};\nuse engine_core::{\n    chain::Chain,\n    error::{AlloyRpcErrorToEngineError, EngineError},\n    transaction::InnerTransaction,\n};\n\nuse crate::account_factory::{AccountFactory, get_account_factory};\n\nsol! {\n    function execute(address _target, uint256 _value, bytes _calldata);\n}\n\nsol! {\n    function executeBatch(address[] _target, uint256[] _value, bytes[] _calldata);\n}\n\npub trait SmartAccount {\n    fn address(\u0026self) -\u003e \u0026Address;\n\n    /// Check if the account is deployed\n    #[allow(async_fn_in_trait)]\n    async fn is_deployed(\u0026self, chain: \u0026impl Chain) -\u003e Result\u003cbool, EngineError\u003e {\n        let code = chain\n            .provider()\n            .get_code_at(self.address().clone())\n            .await\n            .map_err(|t| t.to_engine_error(chain))?;\n\n        Ok(code.len() \u003e 0)\n    }\n\n    /// Encode a transaction call to the account\n    fn encode_execute(\u0026self, tx: \u0026InnerTransaction) -\u003e Bytes {\n        executeCall {\n            _target: tx.to,\n            _value: tx.value,\n            _calldata: tx.data.clone(),\n        }\n        .abi_encode()\n        .into()\n    }\n\n    /// Encode a batch transaction call to the account\n    fn encode_execute_batch(\u0026self, batch: \u0026Vec\u003cInnerTransaction\u003e) -\u003e Bytes {\n        executeBatchCall {\n            _target: batch.iter().map(|tx| tx.to).collect(),\n            _value: batch.iter().map(|tx| tx.value).collect(),\n            _calldata: batch.iter().map(|tx| tx.data.clone()).collect(),\n        }\n        .abi_encode()\n        .into()\n    }\n}\n\npub struct SmartAccountFromSalt\u003c'a, C: Chain\u003e {\n    pub salt_data: \u0026'a Bytes,\n    pub factory_address: Address,\n    pub admin_address: Address,\n    pub chain: \u0026'a C,\n}\n\npub struct DeterminedSmartAccount {\n    pub address: Address,\n}\n\nimpl\u003cC: Chain\u003e SmartAccountFromSalt\u003c'_, C\u003e {\n    pub async fn to_determined_smart_account(self) -\u003e Result\u003cDeterminedSmartAccount, EngineError\u003e {\n        let factory = get_account_factory(self.chain, self.factory_address, None);\n        let address = factory\n            .predict_address(\u0026self.admin_address, \u0026self.salt_data)\n            .await?;\n\n        Ok(DeterminedSmartAccount { address })\n    }\n}\n\nimpl SmartAccount for DeterminedSmartAccount {\n    fn address(\u0026self) -\u003e \u0026Address {\n        \u0026self.address\n    }\n}\n","traces":[{"line":28,"address":[],"length":0,"stats":{"Line":0}},{"line":29,"address":[],"length":0,"stats":{"Line":0}},{"line":31,"address":[],"length":0,"stats":{"Line":0}},{"line":32,"address":[],"length":0,"stats":{"Line":0}},{"line":33,"address":[],"length":0,"stats":{"Line":0}},{"line":35,"address":[],"length":0,"stats":{"Line":0}},{"line":39,"address":[],"length":0,"stats":{"Line":0}},{"line":41,"address":[],"length":0,"stats":{"Line":0}},{"line":42,"address":[],"length":0,"stats":{"Line":0}},{"line":43,"address":[],"length":0,"stats":{"Line":0}},{"line":50,"address":[],"length":0,"stats":{"Line":0}},{"line":52,"address":[],"length":0,"stats":{"Line":0}},{"line":53,"address":[],"length":0,"stats":{"Line":0}},{"line":54,"address":[],"length":0,"stats":{"Line":0}},{"line":73,"address":[],"length":0,"stats":{"Line":0}},{"line":74,"address":[],"length":0,"stats":{"Line":0}},{"line":75,"address":[],"length":0,"stats":{"Line":0}},{"line":76,"address":[],"length":0,"stats":{"Line":0}},{"line":77,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":84,"address":[],"length":0,"stats":{"Line":0}},{"line":85,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":22},{"path":["/","Users","d4mr","work","thirdweb","engine-core","aa-core","src","userop","builder.rs"],"content":"use std::sync::Arc;\n\nuse alloy::{\n    hex,\n    primitives::{Address, Bytes, U256},\n    providers::Provider,\n    rpc::types::{PackedUserOperation, UserOperation},\n};\nuse engine_core::{\n    chain::Chain,\n    credentials::SigningCredential,\n    error::{AlloyRpcErrorToEngineError, EngineError},\n    execution_options::aa::{EntrypointAndFactoryDetails, EntrypointVersion},\n    userop::{UserOpSigner, UserOpSignerParams, UserOpVersion},\n};\n\npub struct UserOpBuilderConfig\u003c'a, C: Chain\u003e {\n    pub account_address: Address,\n    pub signer_address: Address,\n    pub entrypoint_and_factory: EntrypointAndFactoryDetails,\n    pub call_data: Bytes,\n    pub init_call_data: Vec\u003cu8\u003e,\n    pub is_deployed: bool,\n    pub nonce: U256,\n    pub credential: SigningCredential,\n    pub chain: \u0026'a C,\n    pub signer: Arc\u003cUserOpSigner\u003e,\n}\n\npub struct UserOpBuilder\u003c'a, C: Chain\u003e {\n    config: UserOpBuilderConfig\u003c'a, C\u003e,\n}\n\nconst DUMMY_SIGNATURE: [u8; 65] = hex!(\n    \"0xfffffffffffffffffffffffffffffff0000000000000000000000000000000007aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa1c\"\n);\n\nimpl\u003c'a, C: Chain\u003e UserOpBuilder\u003c'a, C\u003e {\n    pub fn new(config: UserOpBuilderConfig\u003c'a, C\u003e) -\u003e Self {\n        Self { config }\n    }\n\n    pub async fn build(self) -\u003e Result\u003cUserOpVersion, EngineError\u003e {\n        let mut userop = match self.config.entrypoint_and_factory.version {\n            EntrypointVersion::V0_6 =\u003e UserOpBuilderV0_6::new(\u0026self.config).build().await?,\n            EntrypointVersion::V0_7 =\u003e UserOpBuilderV0_7::new(\u0026self.config).build().await?,\n        };\n\n        tracing::debug!(\"UserOp built, proceeding with signing\");\n\n        let signature = self\n            .config\n            .signer\n            .sign(UserOpSignerParams {\n                credentials: self.config.credential.clone(),\n                entrypoint: self.config.entrypoint_and_factory.entrypoint_address,\n                userop: userop.clone(),\n                signer_address: self.config.signer_address,\n                chain_id: self.config.chain.chain_id(),\n            })\n            .await?;\n\n        match \u0026mut userop {\n            UserOpVersion::V0_6(userop) =\u003e {\n                userop.signature = signature;\n            }\n            UserOpVersion::V0_7(userop) =\u003e {\n                userop.signature = signature;\n            }\n        }\n\n        tracing::debug!(\"UserOp signed succcessfully\");\n\n        Ok(userop)\n    }\n}\n\nstruct UserOpBuilderV0_6\u003c'a, C: Chain\u003e {\n    userop: UserOperation,\n    entrypoint: Address,\n    chain: \u0026'a C,\n}\n\nimpl\u003c'a, C: Chain\u003e UserOpBuilderV0_6\u003c'a, C\u003e {\n    pub fn new(config: \u0026UserOpBuilderConfig\u003c'a, C\u003e) -\u003e Self {\n        let initcode: Bytes = if config.is_deployed {\n            Bytes::default()\n        } else {\n            let mut initcode: Vec\u003cu8\u003e = config\n                .entrypoint_and_factory\n                .factory_address\n                .into_array()\n                .to_vec();\n\n            initcode.extend_from_slice(config.init_call_data.as_slice());\n            Bytes::from(initcode)\n        };\n        Self {\n            userop: UserOperation {\n                sender: config.account_address,\n                nonce: config.nonce,\n                init_code: initcode,\n                call_data: config.call_data.clone(),\n                call_gas_limit: U256::ZERO,\n                verification_gas_limit: U256::ZERO,\n                pre_verification_gas: U256::ZERO,\n                max_fee_per_gas: U256::ZERO,\n                max_priority_fee_per_gas: U256::ZERO,\n                paymaster_and_data: Bytes::default(),\n                signature: Bytes::from(DUMMY_SIGNATURE),\n            },\n            entrypoint: config.entrypoint_and_factory.entrypoint_address,\n            chain: config.chain,\n        }\n    }\n\n    async fn build(mut self) -\u003e Result\u003cUserOpVersion, EngineError\u003e {\n        let prices = self\n            .chain\n            .provider()\n            .estimate_eip1559_fees()\n            .await\n            .map_err(|err| err.to_engine_error(self.chain))?;\n\n        tracing::debug!(\"Gas prices determined\");\n\n        self.userop.max_fee_per_gas = U256::from(prices.max_fee_per_gas);\n        self.userop.max_priority_fee_per_gas = U256::from(prices.max_priority_fee_per_gas);\n\n        let pm_response = self\n            .chain\n            .paymaster_client()\n            .get_user_op_paymaster_and_data_v0_6(\u0026self.userop, self.entrypoint.clone())\n            .await\n            .map_err(|err| err.to_engine_paymaster_error(self.chain))?;\n\n        tracing::debug!(\"v6 Userop paymaster and data determined\");\n\n        self.userop.paymaster_and_data = pm_response.paymaster_and_data;\n\n        let (call_gas_limit, verification_gas_limit, pre_verification_gas) = match (\n            pm_response.call_gas_limit,\n            pm_response.verification_gas_limit,\n            pm_response.pre_verification_gas,\n        ) {\n            (Some(call_gas_limit), Some(verification_gas_limit), Some(pre_verification_gas)) =\u003e {\n                (call_gas_limit, verification_gas_limit, pre_verification_gas)\n            }\n            _ =\u003e {\n                tracing::debug!(\"No paymaster provided gas limits, getting from bundler\");\n\n                let bundler_response = self\n                    .chain\n                    .bundler_client()\n                    .estimate_user_op_gas(\n                        \u0026UserOpVersion::V0_6(self.userop.clone()),\n                        self.entrypoint,\n                        None,\n                    )\n                    .await\n                    .map_err(|err| err.to_engine_bundler_error(self.chain))?;\n\n                (\n                    bundler_response.call_gas_limit,\n                    bundler_response.verification_gas_limit,\n                    bundler_response.pre_verification_gas,\n                )\n            }\n        };\n\n        self.userop.call_gas_limit = call_gas_limit;\n        self.userop.verification_gas_limit = verification_gas_limit;\n        self.userop.pre_verification_gas = pre_verification_gas;\n\n        Ok(UserOpVersion::V0_6(self.userop))\n    }\n}\n\n// New V0.7 Builder Implementation\nstruct UserOpBuilderV0_7\u003c'a, C: Chain\u003e {\n    userop: PackedUserOperation,\n    entrypoint: Address,\n    chain: \u0026'a C,\n}\n\nimpl\u003c'a, C: Chain\u003e UserOpBuilderV0_7\u003c'a, C\u003e {\n    pub fn new(config: \u0026UserOpBuilderConfig\u003c'a, C\u003e) -\u003e Self {\n        let (factory, factory_data) = if !config.is_deployed {\n            // If not deployed, set factory and factory data\n            (\n                Some(config.entrypoint_and_factory.factory_address),\n                Some(Bytes::from(config.init_call_data.clone())),\n            )\n        } else {\n            // If deployed, use None for factory and empty bytes for factory data\n            (None, None)\n        };\n\n        Self {\n            userop: PackedUserOperation {\n                sender: config.account_address,\n                nonce: config.nonce,\n                factory,\n                factory_data,\n                call_data: config.call_data.clone(),\n                call_gas_limit: U256::ZERO,\n                verification_gas_limit: U256::ZERO,\n                pre_verification_gas: U256::ZERO,\n                max_fee_per_gas: U256::ZERO,\n                max_priority_fee_per_gas: U256::ZERO,\n                paymaster: None,\n                paymaster_data: None,\n                paymaster_verification_gas_limit: None,\n                paymaster_post_op_gas_limit: None,\n                signature: Bytes::from(DUMMY_SIGNATURE),\n            },\n            entrypoint: config.entrypoint_and_factory.entrypoint_address,\n            chain: config.chain,\n        }\n    }\n\n    async fn build(mut self) -\u003e Result\u003cUserOpVersion, EngineError\u003e {\n        // Get gas prices, same as v0.6\n        let prices = self\n            .chain\n            .provider()\n            .estimate_eip1559_fees()\n            .await\n            .map_err(|err| err.to_engine_error(self.chain))?;\n\n        tracing::info!(\"Gas prices determined\");\n\n        self.userop.max_fee_per_gas = U256::from(prices.max_fee_per_gas);\n        self.userop.max_priority_fee_per_gas = U256::from(prices.max_priority_fee_per_gas);\n\n        // Get paymaster data\n        let pm_response = self\n            .chain\n            .paymaster_client()\n            .get_user_op_paymaster_and_data_v0_7(\u0026self.userop, self.entrypoint)\n            .await\n            .map_err(|err| err.to_engine_paymaster_error(self.chain))?;\n\n        tracing::debug!(\"v7 Userop paymaster and data determined\");\n\n        // Apply paymaster data\n        self.userop.paymaster = Some(pm_response.paymaster);\n        self.userop.paymaster_data = Some(pm_response.paymaster_data);\n\n        // Determine gas limits - either from paymaster or from bundler\n        let (\n            call_gas_limit,\n            verification_gas_limit,\n            pre_verification_gas,\n            paymaster_verification_gas_limit,\n            paymaster_post_op_gas_limit,\n        ) = match (\n            pm_response.call_gas_limit,\n            pm_response.verification_gas_limit,\n            pm_response.pre_verification_gas,\n            pm_response.paymaster_verification_gas_limit,\n            pm_response.paymaster_post_op_gas_limit,\n        ) {\n            (Some(call), Some(verification), Some(pre), Some(pm_verification), Some(pm_post)) =\u003e {\n                (call, verification, pre, pm_verification, pm_post)\n            }\n            _ =\u003e {\n                // If paymaster didn't provide all gas limits, get them from the bundler\n                tracing::debug!(\"No paymaster provided gas limits, getting from bundler\");\n\n                let bundler_response = self\n                    .chain\n                    .bundler_client()\n                    .estimate_user_op_gas(\n                        \u0026UserOpVersion::V0_7(self.userop.clone()),\n                        self.entrypoint,\n                        None,\n                    )\n                    .await\n                    .map_err(|err| err.to_engine_bundler_error(self.chain))?;\n\n                (\n                    bundler_response.call_gas_limit,\n                    bundler_response.verification_gas_limit,\n                    bundler_response.pre_verification_gas,\n                    bundler_response.paymaster_verification_gas_limit,\n                    bundler_response\n                        .paymaster_post_op_gas_limit\n                        .unwrap_or_default(),\n                )\n            }\n        };\n\n        tracing::debug!(\"Gas limits determined\");\n\n        // Set gas limits\n        self.userop.call_gas_limit = call_gas_limit;\n        self.userop.verification_gas_limit = verification_gas_limit;\n        self.userop.pre_verification_gas = pre_verification_gas;\n        self.userop.paymaster_verification_gas_limit = Some(paymaster_verification_gas_limit);\n        self.userop.paymaster_post_op_gas_limit = Some(paymaster_post_op_gas_limit);\n\n        Ok(UserOpVersion::V0_7(self.userop))\n    }\n}\n","traces":[{"line":39,"address":[],"length":0,"stats":{"Line":0}},{"line":43,"address":[],"length":0,"stats":{"Line":0}},{"line":44,"address":[],"length":0,"stats":{"Line":0}},{"line":45,"address":[],"length":0,"stats":{"Line":0}},{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":49,"address":[],"length":0,"stats":{"Line":0}},{"line":51,"address":[],"length":0,"stats":{"Line":0}},{"line":52,"address":[],"length":0,"stats":{"Line":0}},{"line":53,"address":[],"length":0,"stats":{"Line":0}},{"line":54,"address":[],"length":0,"stats":{"Line":0}},{"line":55,"address":[],"length":0,"stats":{"Line":0}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":61,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[],"length":0,"stats":{"Line":0}},{"line":65,"address":[],"length":0,"stats":{"Line":0}},{"line":67,"address":[],"length":0,"stats":{"Line":0}},{"line":68,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":74,"address":[],"length":0,"stats":{"Line":0}},{"line":85,"address":[],"length":0,"stats":{"Line":0}},{"line":86,"address":[],"length":0,"stats":{"Line":0}},{"line":87,"address":[],"length":0,"stats":{"Line":0}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":91,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":96,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":112,"address":[],"length":0,"stats":{"Line":0}},{"line":113,"address":[],"length":0,"stats":{"Line":0}},{"line":117,"address":[],"length":0,"stats":{"Line":0}},{"line":118,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":0}},{"line":122,"address":[],"length":0,"stats":{"Line":0}},{"line":123,"address":[],"length":0,"stats":{"Line":0}},{"line":125,"address":[],"length":0,"stats":{"Line":0}},{"line":127,"address":[],"length":0,"stats":{"Line":0}},{"line":128,"address":[],"length":0,"stats":{"Line":0}},{"line":130,"address":[],"length":0,"stats":{"Line":0}},{"line":131,"address":[],"length":0,"stats":{"Line":0}},{"line":133,"address":[],"length":0,"stats":{"Line":0}},{"line":134,"address":[],"length":0,"stats":{"Line":0}},{"line":135,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":139,"address":[],"length":0,"stats":{"Line":0}},{"line":141,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":144,"address":[],"length":0,"stats":{"Line":0}},{"line":146,"address":[],"length":0,"stats":{"Line":0}},{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":150,"address":[],"length":0,"stats":{"Line":0}},{"line":152,"address":[],"length":0,"stats":{"Line":0}},{"line":153,"address":[],"length":0,"stats":{"Line":0}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":160,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":164,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[],"length":0,"stats":{"Line":0}},{"line":171,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":175,"address":[],"length":0,"stats":{"Line":0}},{"line":187,"address":[],"length":0,"stats":{"Line":0}},{"line":188,"address":[],"length":0,"stats":{"Line":0}},{"line":191,"address":[],"length":0,"stats":{"Line":0}},{"line":192,"address":[],"length":0,"stats":{"Line":0}},{"line":196,"address":[],"length":0,"stats":{"Line":0}},{"line":200,"address":[],"length":0,"stats":{"Line":0}},{"line":217,"address":[],"length":0,"stats":{"Line":0}},{"line":218,"address":[],"length":0,"stats":{"Line":0}},{"line":222,"address":[],"length":0,"stats":{"Line":0}},{"line":224,"address":[],"length":0,"stats":{"Line":0}},{"line":225,"address":[],"length":0,"stats":{"Line":0}},{"line":228,"address":[],"length":0,"stats":{"Line":0}},{"line":229,"address":[],"length":0,"stats":{"Line":0}},{"line":231,"address":[],"length":0,"stats":{"Line":0}},{"line":233,"address":[],"length":0,"stats":{"Line":0}},{"line":234,"address":[],"length":0,"stats":{"Line":0}},{"line":237,"address":[],"length":0,"stats":{"Line":0}},{"line":238,"address":[],"length":0,"stats":{"Line":0}},{"line":240,"address":[],"length":0,"stats":{"Line":0}},{"line":241,"address":[],"length":0,"stats":{"Line":0}},{"line":242,"address":[],"length":0,"stats":{"Line":0}},{"line":244,"address":[],"length":0,"stats":{"Line":0}},{"line":247,"address":[],"length":0,"stats":{"Line":0}},{"line":248,"address":[],"length":0,"stats":{"Line":0}},{"line":251,"address":[],"length":0,"stats":{"Line":0}},{"line":252,"address":[],"length":0,"stats":{"Line":0}},{"line":253,"address":[],"length":0,"stats":{"Line":0}},{"line":254,"address":[],"length":0,"stats":{"Line":0}},{"line":255,"address":[],"length":0,"stats":{"Line":0}},{"line":256,"address":[],"length":0,"stats":{"Line":0}},{"line":257,"address":[],"length":0,"stats":{"Line":0}},{"line":258,"address":[],"length":0,"stats":{"Line":0}},{"line":259,"address":[],"length":0,"stats":{"Line":0}},{"line":260,"address":[],"length":0,"stats":{"Line":0}},{"line":261,"address":[],"length":0,"stats":{"Line":0}},{"line":262,"address":[],"length":0,"stats":{"Line":0}},{"line":264,"address":[],"length":0,"stats":{"Line":0}},{"line":265,"address":[],"length":0,"stats":{"Line":0}},{"line":267,"address":[],"length":0,"stats":{"Line":0}},{"line":269,"address":[],"length":0,"stats":{"Line":0}},{"line":271,"address":[],"length":0,"stats":{"Line":0}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":275,"address":[],"length":0,"stats":{"Line":0}},{"line":276,"address":[],"length":0,"stats":{"Line":0}},{"line":277,"address":[],"length":0,"stats":{"Line":0}},{"line":279,"address":[],"length":0,"stats":{"Line":0}},{"line":280,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":284,"address":[],"length":0,"stats":{"Line":0}},{"line":285,"address":[],"length":0,"stats":{"Line":0}},{"line":286,"address":[],"length":0,"stats":{"Line":0}},{"line":287,"address":[],"length":0,"stats":{"Line":0}},{"line":288,"address":[],"length":0,"stats":{"Line":0}},{"line":289,"address":[],"length":0,"stats":{"Line":0}},{"line":294,"address":[],"length":0,"stats":{"Line":0}},{"line":297,"address":[],"length":0,"stats":{"Line":0}},{"line":298,"address":[],"length":0,"stats":{"Line":0}},{"line":299,"address":[],"length":0,"stats":{"Line":0}},{"line":300,"address":[],"length":0,"stats":{"Line":0}},{"line":301,"address":[],"length":0,"stats":{"Line":0}},{"line":303,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":132},{"path":["/","Users","d4mr","work","thirdweb","engine-core","aa-core","src","userop","deployment.rs"],"content":"use alloy::primitives::Address;\nuse engine_core::error::EngineError;\nuse std::time::Duration;\n\npub trait DeploymentCache: Send + Sync {\n    /// Check if we know for certain that the account is deployed\n    /// Returns Some(true) if definitely deployed, None if unknown\n    fn is_deployed(\n        \u0026self,\n        chain_id: u64,\n        account_address: \u0026Address,\n    ) -\u003e impl Future\u003cOutput = Option\u003cbool\u003e\u003e + Send + Sync;\n}\n\npub trait DeploymentLock: Send + Sync {\n    /// Check if a deployment lock exists\n    /// Returns Some(Duration) with time since lock was acquired if locked, None if not locked\n    fn check_lock(\n        \u0026self,\n        chain_id: u64,\n        account_address: \u0026Address,\n    ) -\u003e impl Future\u003cOutput = Option\u003cDuration\u003e\u003e + Send + Sync;\n\n    /// Try to acquire a deployment lock\n    /// Returns true if successful, false if already locked\n    fn acquire_lock(\n        \u0026self,\n        chain_id: u64,\n        account_address: \u0026Address,\n    ) -\u003e impl Future\u003cOutput = Result\u003cbool, EngineError\u003e\u003e + Send + Sync;\n}\n\npub enum DeploymentStatus {\n    /// Account is definitely deployed\n    Deployed,\n\n    /// Account is currently being deployed by another process\n    BeingDeployed { stale: bool },\n\n    /// Account is not deployed and we have acquired the lock\n    LockAcquired,\n\n    /// Account is not deployed but we couldn't acquire the lock\n    LockFailed,\n}\n\n// A generic deployment manager\npub struct DeploymentManager\u003cC, L\u003e\nwhere\n    C: DeploymentCache,\n    L: DeploymentLock,\n{\n    pub cache: C,\n    pub lock: L,\n    pub stale_lock_threshold: Duration,\n}\n\nimpl\u003cC, L\u003e DeploymentManager\u003cC, L\u003e\nwhere\n    C: DeploymentCache,\n    L: DeploymentLock,\n{\n    pub fn new(cache: C, lock: L, stale_lock_threshold: Duration) -\u003e Self {\n        Self {\n            cache,\n            lock,\n            stale_lock_threshold,\n        }\n    }\n\n    /// Core function that implements the deployment check logic\n    pub async fn check_deployment_status(\n        \u0026self,\n        chain_id: u64,\n        account_address: \u0026Address,\n        check_chain: impl std::future::Future\u003cOutput = Result\u003cbool, EngineError\u003e\u003e,\n    ) -\u003e Result\u003cDeploymentStatus, EngineError\u003e {\n        // 1. Check cache first for definitive \"deployed\" status\n        if let Some(true) = self.cache.is_deployed(chain_id, account_address).await {\n            return Ok(DeploymentStatus::Deployed);\n        }\n\n        // 2. Check for deployment lock\n        if let Some(locked_duration) = self.lock.check_lock(chain_id, account_address).await {\n            // Check if lock is stale\n            let is_stale = locked_duration \u003e self.stale_lock_threshold;\n\n            // For stale locks, we'll check chain state\n            if is_stale {\n                let is_deployed = check_chain.await?;\n                if is_deployed {\n                    return Ok(DeploymentStatus::Deployed);\n                }\n            }\n\n            // Either fresh lock or stale but not deployed\n            return Ok(DeploymentStatus::BeingDeployed { stale: is_stale });\n        }\n\n        // 3. No lock exists, check chain state\n        let is_deployed = check_chain.await?;\n        if is_deployed {\n            return Ok(DeploymentStatus::Deployed);\n        }\n\n        // 4. Not deployed, try to acquire lock\n        if self.lock.acquire_lock(chain_id, account_address).await? {\n            Ok(DeploymentStatus::LockAcquired)\n        } else {\n            Ok(DeploymentStatus::LockFailed)\n        }\n    }\n}\n","traces":[{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":80,"address":[],"length":0,"stats":{"Line":0}},{"line":84,"address":[],"length":0,"stats":{"Line":0}},{"line":86,"address":[],"length":0,"stats":{"Line":0}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":91,"address":[],"length":0,"stats":{"Line":0}},{"line":92,"address":[],"length":0,"stats":{"Line":0}},{"line":97,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":102,"address":[],"length":0,"stats":{"Line":0}},{"line":103,"address":[],"length":0,"stats":{"Line":0}},{"line":107,"address":[],"length":0,"stats":{"Line":0}},{"line":108,"address":[],"length":0,"stats":{"Line":0}},{"line":110,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":17},{"path":["/","Users","d4mr","work","thirdweb","engine-core","aa-core","src","userop","mod.rs"],"content":"pub mod builder;\npub mod deployment;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","core","src","chain.rs"],"content":"use crate::rpc_clients::{BundlerClient, PaymasterClient};\nuse alloy::{\n    providers::{ProviderBuilder, RootProvider},\n    transports::http::{\n        Http,\n        reqwest::{\n            ClientBuilder as HttpClientBuilder, Url,\n            header::{HeaderMap, HeaderValue},\n        },\n    },\n};\n\nuse crate::error::EngineError;\n\npub trait Chain {\n    fn chain_id(\u0026self) -\u003e u64;\n    fn rpc_url(\u0026self) -\u003e Url;\n    fn bundler_url(\u0026self) -\u003e Url;\n    fn paymaster_url(\u0026self) -\u003e Url;\n\n    fn provider(\u0026self) -\u003e \u0026RootProvider;\n    fn bundler_client(\u0026self) -\u003e \u0026BundlerClient;\n    fn paymaster_client(\u0026self) -\u003e \u0026PaymasterClient;\n}\n\npub struct ThirdwebChainConfig\u003c'a\u003e {\n    pub secret_key: \u0026'a str,\n    pub chain_id: u64,\n    pub rpc_base_url: \u0026'a str,\n    pub bundler_base_url: \u0026'a str,\n    pub paymaster_base_url: \u0026'a str,\n    pub client_id: \u0026'a str,\n}\n\npub struct ThirdwebChain {\n    chain_id: u64,\n    rpc_url: Url,\n    bundler_url: Url,\n    paymaster_url: Url,\n\n    pub bundler_client: BundlerClient,\n    pub paymaster_client: PaymasterClient,\n    pub provider: RootProvider,\n}\n\nimpl Chain for ThirdwebChain {\n    fn chain_id(\u0026self) -\u003e u64 {\n        self.chain_id\n    }\n\n    fn rpc_url(\u0026self) -\u003e Url {\n        self.rpc_url.clone()\n    }\n\n    fn bundler_url(\u0026self) -\u003e Url {\n        self.bundler_url.clone()\n    }\n\n    fn paymaster_url(\u0026self) -\u003e Url {\n        self.paymaster_url.clone()\n    }\n\n    fn provider(\u0026self) -\u003e \u0026RootProvider {\n        \u0026self.provider\n    }\n\n    fn bundler_client(\u0026self) -\u003e \u0026BundlerClient {\n        \u0026self.bundler_client\n    }\n\n    fn paymaster_client(\u0026self) -\u003e \u0026PaymasterClient {\n        \u0026self.paymaster_client\n    }\n}\n\nimpl\u003c'a\u003e ThirdwebChainConfig\u003c'a\u003e {\n    pub fn to_chain(\u0026self) -\u003e Result\u003cThirdwebChain, EngineError\u003e {\n        let rpc_url = Url::parse(\u0026format!(\n            \"https://{chain_id}.{base_url}/{client_id}\",\n            chain_id = self.chain_id,\n            base_url = self.rpc_base_url,\n            client_id = self.client_id,\n        ))\n        .map_err(|e| EngineError::RpcConfigError {\n            message: format!(\"Failed to parse RPC URL: {}\", e.to_string()),\n        })?;\n\n        let bundler_url = Url::parse(\u0026format!(\n            \"https://{chain_id}.{base_url}/v2\",\n            chain_id = self.chain_id,\n            base_url = self.bundler_base_url,\n        ))\n        .map_err(|e| EngineError::RpcConfigError {\n            message: format!(\"Failed to parse Bundler URL: {}\", e.to_string()),\n        })?;\n\n        let paymaster_url = Url::parse(\u0026format!(\n            \"https://{chain_id}.{base_url}/v2\",\n            chain_id = self.chain_id,\n            base_url = self.paymaster_base_url,\n        ))\n        .map_err(|e| EngineError::RpcConfigError {\n            message: format!(\"Failed to parse Paymaster URL: {}\", e.to_string()),\n        })?;\n\n        let mut headers = HeaderMap::new();\n        headers.insert(\n            \"x-client-id\",\n            HeaderValue::from_str(\u0026self.client_id).map_err(|e| EngineError::RpcConfigError {\n                message: format!(\"Unserialisable client-id used: {e}\"),\n            })?,\n        );\n\n        headers.insert(\n            \"x-secret-key\",\n            HeaderValue::from_str(\u0026self.secret_key).map_err(|e| EngineError::RpcConfigError {\n                message: format!(\"Unserialisable secret-key used: {e}\"),\n            })?,\n        );\n\n        let reqwest_client = HttpClientBuilder::new()\n            .default_headers(headers)\n            .build()\n            .map_err(|e| EngineError::RpcConfigError {\n                message: format!(\"Failed to build HTTP client: {e}\"),\n            })?;\n\n        let paymaster_transport = Http::with_client(reqwest_client.clone(), paymaster_url.clone());\n        let bundler_transport = Http::with_client(reqwest_client, bundler_url.clone());\n\n        Ok(ThirdwebChain {\n            chain_id: self.chain_id,\n            rpc_url: rpc_url.clone(),\n            bundler_client: BundlerClient::new(bundler_transport),\n            paymaster_client: PaymasterClient::new(paymaster_transport),\n            provider: ProviderBuilder::new()\n                .disable_recommended_fillers()\n                .connect_http(rpc_url)\n                .into(),\n\n            bundler_url,\n            paymaster_url,\n        })\n    }\n}\n","traces":[{"line":47,"address":[],"length":0,"stats":{"Line":0}},{"line":48,"address":[],"length":0,"stats":{"Line":0}},{"line":51,"address":[],"length":0,"stats":{"Line":0}},{"line":52,"address":[],"length":0,"stats":{"Line":0}},{"line":55,"address":[],"length":0,"stats":{"Line":0}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":60,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[],"length":0,"stats":{"Line":0}},{"line":67,"address":[],"length":0,"stats":{"Line":0}},{"line":68,"address":[],"length":0,"stats":{"Line":0}},{"line":71,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":77,"address":[],"length":0,"stats":{"Line":0}},{"line":78,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":80,"address":[],"length":0,"stats":{"Line":0}},{"line":81,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[],"length":0,"stats":{"Line":0}},{"line":84,"address":[],"length":0,"stats":{"Line":0}},{"line":85,"address":[],"length":0,"stats":{"Line":0}},{"line":88,"address":[],"length":0,"stats":{"Line":0}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":91,"address":[],"length":0,"stats":{"Line":0}},{"line":93,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[],"length":0,"stats":{"Line":0}},{"line":97,"address":[],"length":0,"stats":{"Line":0}},{"line":98,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":100,"address":[],"length":0,"stats":{"Line":0}},{"line":102,"address":[],"length":0,"stats":{"Line":0}},{"line":103,"address":[],"length":0,"stats":{"Line":0}},{"line":106,"address":[],"length":0,"stats":{"Line":0}},{"line":107,"address":[],"length":0,"stats":{"Line":0}},{"line":109,"address":[],"length":0,"stats":{"Line":0}},{"line":110,"address":[],"length":0,"stats":{"Line":0}},{"line":114,"address":[],"length":0,"stats":{"Line":0}},{"line":116,"address":[],"length":0,"stats":{"Line":0}},{"line":117,"address":[],"length":0,"stats":{"Line":0}},{"line":121,"address":[],"length":0,"stats":{"Line":0}},{"line":122,"address":[],"length":0,"stats":{"Line":0}},{"line":124,"address":[],"length":0,"stats":{"Line":0}},{"line":125,"address":[],"length":0,"stats":{"Line":0}},{"line":128,"address":[],"length":0,"stats":{"Line":0}},{"line":129,"address":[],"length":0,"stats":{"Line":0}},{"line":131,"address":[],"length":0,"stats":{"Line":0}},{"line":132,"address":[],"length":0,"stats":{"Line":0}},{"line":133,"address":[],"length":0,"stats":{"Line":0}},{"line":134,"address":[],"length":0,"stats":{"Line":0}},{"line":135,"address":[],"length":0,"stats":{"Line":0}},{"line":136,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":139,"address":[],"length":0,"stats":{"Line":0}},{"line":141,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":58},{"path":["/","Users","d4mr","work","thirdweb","engine-core","core","src","constants.rs"],"content":"use alloy::primitives::{Address, address};\n\npub const DEFAULT_FACTORY_ADDRESS_V0_7: Address =\n    address!(\"0x4bE0ddfebcA9A5A4a617dee4DeCe99E7c862dceb\");\n\npub const DEFAULT_IMPLEMENTATION_ADDRESS_V0_7: Address =\n    address!(\"0x94eC38a5d2EDA5A543Ab4c08D998338D4082beb2\");\n\npub const DEFAULT_FACTORY_ADDRESS_V0_6: Address =\n    address!(\"0x85e23b94e7F5E9cC1fF78BCe78cfb15B81f0DF00\");\n\npub const DEFAULT_IMPLEMENTATION_ADDRESS_V0_6: Address =\n    address!(\"0xf22175c80c6e074C171811C59C6c0087e2a6a346\");\n\npub const ENTRYPOINT_ADDRESS_V0_6: Address = address!(\"0x5FF137D4b0FDCD49DcA30c7CF57E578a026d2789\"); // v0.6\n\npub const ENTRYPOINT_ADDRESS_V0_7: Address = address!(\"0x0000000071727De22E5E9d8BAf0edAc6f37da032\"); // v0.7\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","core","src","credentials.rs"],"content":"use vault_types::enclave::auth::Auth;\n\n#[derive(Debug, Clone)]\npub enum SigningCredential {\n    Vault(Auth),\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","core","src","defs.rs"],"content":"use alloy::primitives::Address;\nuse schemars::JsonSchema;\nuse serde::{Deserialize, Serialize};\n\n#[derive(JsonSchema, Serialize, Deserialize, Clone)]\n#[serde(remote = \"Address\", transparent)]\n/// # Address\n/// Used to represent an EVM address. This is a string of length 42 with a `0x` prefix. Non-checksummed addresses are also supported, but will be converted to checksummed.\npub struct AddressDef(pub String);\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","core","src","error.rs"],"content":"use alloy::{\n    primitives::Address,\n    transports::{RpcError as AlloyRpcError, TransportErrorKind},\n};\nuse serde::{Deserialize, Serialize};\nuse thiserror::Error;\n\nuse crate::chain::Chain;\n\n#[derive(Debug, Error, Serialize, Deserialize)]\npub enum RpcErrorKind {\n    /// Server returned an error response.\n    #[error(\"server returned an error response: {0}\")]\n    ErrorResp(RpcErrorResponse),\n\n    /// Server returned a null response when a non-null response was expected.\n    #[error(\"server returned a null response when a non-null response was expected\")]\n    NullResp,\n\n    /// Rpc server returned an unsupported feature.\n    #[error(\"unsupported feature: {0}\")]\n    UnsupportedFeature(String),\n\n    /// Returned when a local pre-processing step fails. This allows custom\n    /// errors from local signers or request pre-processors.\n    #[error(\"local usage error: {0}\")]\n    InternalError(String),\n\n    /// JSON serialization error.\n    #[error(\"serialization error: {message}\")]\n    SerError {\n        /// The underlying serde_json error.\n        // To avoid accidentally confusing ser and deser errors, we do not use\n        // the `#[from]` tag.\n        message: String, // sourced from serde_json::Error\n    },\n    /// JSON deserialization error.\n    #[error(\"deserialization error: {message}, text: {text}\")]\n    DeserError {\n        /// The underlying serde_json error.\n        // To avoid accidentally confusing ser and deser errors, we do not use\n        // the `#[from]` tag.\n        message: String, // sourced from serde_json::Error\n        /// For deser errors, the text that failed to deserialize.\n        text: String,\n    },\n\n    #[error(\"HTTP error {status}\")]\n    TransportHttpError { status: u16, body: String },\n\n    #[error(\"Other transport error: {0}\")]\n    OtherTransportError(String),\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct RpcErrorResponse {\n    /// The error code.\n    pub code: i64,\n    /// The error message (if any).\n    pub message: String,\n    /// The error data (if any).\n    pub data: Option\u003cString\u003e,\n}\n\nimpl RpcErrorResponse {\n    pub fn as_display(\u0026self) -\u003e String {\n        format!(\n            \"code {}: {}{}\",\n            self.code,\n            self.message,\n            self.data\n                .as_ref()\n                .map(|data| format!(\", data: {data}\"))\n                .unwrap_or_default()\n        )\n    }\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct RpcErrorInfo {\n    /// The chain ID where the error occurred\n    pub chain_id: u64,\n\n    /// The provider URL\n    pub provider_url: String,\n\n    /// Human-readable error message\n    pub message: String,\n\n    /// The error response payload as a SerdeValue\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub response: Option\u003cRpcErrorResponse\u003e,\n}\n\n/// A serializable contract interaction error type\n#[derive(Debug, Error, Serialize, Deserialize)]\npub enum ContractInteractionErrorKind {\n    /// Unknown function referenced.\n    #[error(\"unknown function: function {0} does not exist\")]\n    UnknownFunction(String),\n\n    /// Unknown function selector referenced.\n    #[error(\"unknown function: function with selector {0:?} does not exist\")]\n    UnknownSelector(String), // Serialize as string instead of Selector\n\n    /// Called `deploy` with a transaction that is not a deployment transaction.\n    #[error(\"transaction is not a deployment transaction\")]\n    NotADeploymentTransaction,\n\n    /// `contractAddress` was not found in the deployment transaction's receipt.\n    #[error(\"missing `contractAddress` from deployment transaction receipt\")]\n    ContractNotDeployed,\n\n    /// The contract returned no data.\n    #[error(\n        \"contract call to `{function}` returned no data (\\\"0x\\\"); the called address might not be a contract\"\n    )]\n    ZeroData {\n        function: String,\n        message: String, // From AbiError\n    },\n\n    /// An error occurred ABI encoding or decoding.\n    #[error(\"ABI error: {0}\")]\n    AbiError(String),\n\n    /// An error occurred interacting with a contract over RPC.\n    #[error(\"transport error: {0}\")]\n    TransportError(String),\n\n    /// An error occured while waiting for a pending transaction.\n    #[error(\"pending transaction error: {0}\")]\n    PendingTransactionError(String),\n}\n\n#[derive(Error, Debug, Serialize, Deserialize)]\npub enum EngineError {\n    #[error(\"RPC error on chain {chain_id} at {rpc_url}: {message}\")]\n    RpcError {\n        /// Detailed RPC error information\n        chain_id: u64,\n        rpc_url: String,\n        message: String,\n        kind: RpcErrorKind,\n    },\n\n    #[error(\"Paymaster error on chain {chain_id} at {rpc_url}: {message}\")]\n    PaymasterError {\n        /// Detailed RPC error information\n        chain_id: u64,\n        rpc_url: String,\n        message: String,\n        kind: RpcErrorKind,\n    },\n\n    #[error(\"BundlerError error on chain {chain_id} at {rpc_url}: {message}\")]\n    BundlerError {\n        /// Detailed RPC error information\n        chain_id: u64,\n        rpc_url: String,\n        message: String,\n        kind: RpcErrorKind,\n    },\n\n    #[error(\"Error interaction with vault: {message}\")]\n    VaultError { message: String },\n\n    #[error(\"Bad RPC configuration: {message}\")]\n    RpcConfigError { message: String },\n\n    #[error(\"Contract interaction error: {message}\")]\n    ContractInteractionError {\n        /// Contract address\n        contract_address: Option\u003cAddress\u003e,\n        /// Chain ID\n        chain_id: u64,\n        /// Human-readable error message\n        message: String,\n        /// Specific error kind\n        kind: ContractInteractionErrorKind,\n    },\n}\n\npub trait AlloyRpcErrorToEngineError {\n    fn to_engine_error(\u0026self, chain: \u0026impl Chain) -\u003e EngineError;\n    fn to_engine_bundler_error(\u0026self, chain: \u0026impl Chain) -\u003e EngineError;\n    fn to_engine_paymaster_error(\u0026self, chain: \u0026impl Chain) -\u003e EngineError;\n}\n\nfn to_engine_rpc_error_kind(err: \u0026AlloyRpcError\u003cTransportErrorKind\u003e) -\u003e RpcErrorKind {\n    match err {\n        AlloyRpcError::ErrorResp(err) =\u003e RpcErrorKind::ErrorResp(RpcErrorResponse {\n            code: err.code,\n            message: err.message.to_string(),\n            data: err.data.as_ref().map(|data| data.to_string()),\n        }),\n        AlloyRpcError::NullResp =\u003e RpcErrorKind::NullResp,\n        AlloyRpcError::UnsupportedFeature(feature) =\u003e {\n            RpcErrorKind::UnsupportedFeature(feature.to_string())\n        }\n        AlloyRpcError::LocalUsageError(err) =\u003e RpcErrorKind::InternalError(err.to_string()),\n        AlloyRpcError::SerError(err) =\u003e RpcErrorKind::SerError {\n            message: err.to_string(),\n        },\n        AlloyRpcError::DeserError { err, text } =\u003e RpcErrorKind::DeserError {\n            message: err.to_string(),\n            text: text.to_string(),\n        },\n        AlloyRpcError::Transport(err) =\u003e match err {\n            TransportErrorKind::HttpError(err) =\u003e RpcErrorKind::TransportHttpError {\n                status: err.status,\n                body: err.body.to_string(),\n            },\n            TransportErrorKind::Custom(err) =\u003e RpcErrorKind::OtherTransportError(err.to_string()),\n            _ =\u003e RpcErrorKind::OtherTransportError(err.to_string()),\n        },\n    }\n}\n\nimpl AlloyRpcErrorToEngineError for AlloyRpcError\u003cTransportErrorKind\u003e {\n    fn to_engine_error(\u0026self, chain: \u0026impl Chain) -\u003e EngineError {\n        EngineError::RpcError {\n            chain_id: chain.chain_id(),\n            rpc_url: chain.rpc_url().to_string(),\n            message: self.to_string(),\n            kind: to_engine_rpc_error_kind(self),\n        }\n    }\n\n    fn to_engine_bundler_error(\u0026self, chain: \u0026impl Chain) -\u003e EngineError {\n        EngineError::BundlerError {\n            chain_id: chain.chain_id(),\n            rpc_url: chain.bundler_url().to_string(),\n            message: self.to_string(),\n            kind: to_engine_rpc_error_kind(self),\n        }\n    }\n    fn to_engine_paymaster_error(\u0026self, chain: \u0026impl Chain) -\u003e EngineError {\n        EngineError::PaymasterError {\n            chain_id: chain.chain_id(),\n            rpc_url: chain.paymaster_url().to_string(),\n            message: self.to_string(),\n            kind: to_engine_rpc_error_kind(self),\n        }\n    }\n}\n\n// Conversion trait for the original error type\npub trait ContractErrorToEngineError {\n    fn to_engine_error(self, chain_id: u64, contract_address: Option\u003cAddress\u003e) -\u003e EngineError;\n}\n\n// Implementation for the original Error type\nimpl ContractErrorToEngineError for alloy::contract::Error {\n    fn to_engine_error(self, chain_id: u64, contract_address: Option\u003cAddress\u003e) -\u003e EngineError {\n        let (message, kind) = match self {\n            alloy::contract::Error::UnknownFunction(name) =\u003e (\n                format!(\"Unknown function: {}\", name),\n                ContractInteractionErrorKind::UnknownFunction(name),\n            ),\n            alloy::contract::Error::UnknownSelector(selector) =\u003e (\n                format!(\"Unknown selector: {:?}\", selector),\n                ContractInteractionErrorKind::UnknownSelector(format!(\"{:?}\", selector)),\n            ),\n            alloy::contract::Error::NotADeploymentTransaction =\u003e (\n                \"Transaction is not a deployment transaction\".to_string(),\n                ContractInteractionErrorKind::NotADeploymentTransaction,\n            ),\n            alloy::contract::Error::ContractNotDeployed =\u003e (\n                \"Contract not deployed - missing contractAddress in receipt\".to_string(),\n                ContractInteractionErrorKind::ContractNotDeployed,\n            ),\n            alloy::contract::Error::ZeroData(function, err) =\u003e (\n                format!(\"Zero data returned from contract call to {}\", function),\n                ContractInteractionErrorKind::ZeroData {\n                    function,\n                    message: err.to_string(),\n                },\n            ),\n            alloy::contract::Error::AbiError(err) =\u003e (\n                format!(\"ABI error: {}\", err),\n                ContractInteractionErrorKind::AbiError(err.to_string()),\n            ),\n            alloy::contract::Error::TransportError(err) =\u003e (\n                format!(\"Transport error: {}\", err),\n                ContractInteractionErrorKind::TransportError(err.to_string()),\n            ),\n            alloy::contract::Error::PendingTransactionError(err) =\u003e (\n                format!(\"Pending transaction error: {}\", err),\n                ContractInteractionErrorKind::PendingTransactionError(err.to_string()),\n            ),\n        };\n\n        EngineError::ContractInteractionError {\n            contract_address,\n            chain_id,\n            message,\n            kind,\n        }\n    }\n}\n","traces":[{"line":66,"address":[],"length":0,"stats":{"Line":0}},{"line":67,"address":[],"length":0,"stats":{"Line":0}},{"line":69,"address":[],"length":0,"stats":{"Line":0}},{"line":70,"address":[],"length":0,"stats":{"Line":0}},{"line":71,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":73,"address":[],"length":0,"stats":{"Line":0}},{"line":74,"address":[],"length":0,"stats":{"Line":0}},{"line":190,"address":[],"length":0,"stats":{"Line":0}},{"line":191,"address":[],"length":0,"stats":{"Line":0}},{"line":192,"address":[],"length":0,"stats":{"Line":0}},{"line":193,"address":[],"length":0,"stats":{"Line":0}},{"line":194,"address":[],"length":0,"stats":{"Line":0}},{"line":195,"address":[],"length":0,"stats":{"Line":0}},{"line":197,"address":[],"length":0,"stats":{"Line":0}},{"line":198,"address":[],"length":0,"stats":{"Line":0}},{"line":199,"address":[],"length":0,"stats":{"Line":0}},{"line":201,"address":[],"length":0,"stats":{"Line":0}},{"line":203,"address":[],"length":0,"stats":{"Line":0}},{"line":206,"address":[],"length":0,"stats":{"Line":0}},{"line":207,"address":[],"length":0,"stats":{"Line":0}},{"line":209,"address":[],"length":0,"stats":{"Line":0}},{"line":211,"address":[],"length":0,"stats":{"Line":0}},{"line":212,"address":[],"length":0,"stats":{"Line":0}},{"line":214,"address":[],"length":0,"stats":{"Line":0}},{"line":215,"address":[],"length":0,"stats":{"Line":0}},{"line":221,"address":[],"length":0,"stats":{"Line":0}},{"line":223,"address":[],"length":0,"stats":{"Line":0}},{"line":224,"address":[],"length":0,"stats":{"Line":0}},{"line":225,"address":[],"length":0,"stats":{"Line":0}},{"line":226,"address":[],"length":0,"stats":{"Line":0}},{"line":230,"address":[],"length":0,"stats":{"Line":0}},{"line":232,"address":[],"length":0,"stats":{"Line":0}},{"line":233,"address":[],"length":0,"stats":{"Line":0}},{"line":234,"address":[],"length":0,"stats":{"Line":0}},{"line":235,"address":[],"length":0,"stats":{"Line":0}},{"line":238,"address":[],"length":0,"stats":{"Line":0}},{"line":240,"address":[],"length":0,"stats":{"Line":0}},{"line":241,"address":[],"length":0,"stats":{"Line":0}},{"line":242,"address":[],"length":0,"stats":{"Line":0}},{"line":243,"address":[],"length":0,"stats":{"Line":0}},{"line":255,"address":[],"length":0,"stats":{"Line":0}},{"line":256,"address":[],"length":0,"stats":{"Line":0}},{"line":257,"address":[],"length":0,"stats":{"Line":0}},{"line":258,"address":[],"length":0,"stats":{"Line":0}},{"line":259,"address":[],"length":0,"stats":{"Line":0}},{"line":261,"address":[],"length":0,"stats":{"Line":0}},{"line":262,"address":[],"length":0,"stats":{"Line":0}},{"line":263,"address":[],"length":0,"stats":{"Line":0}},{"line":265,"address":[],"length":0,"stats":{"Line":0}},{"line":266,"address":[],"length":0,"stats":{"Line":0}},{"line":267,"address":[],"length":0,"stats":{"Line":0}},{"line":269,"address":[],"length":0,"stats":{"Line":0}},{"line":270,"address":[],"length":0,"stats":{"Line":0}},{"line":271,"address":[],"length":0,"stats":{"Line":0}},{"line":273,"address":[],"length":0,"stats":{"Line":0}},{"line":274,"address":[],"length":0,"stats":{"Line":0}},{"line":275,"address":[],"length":0,"stats":{"Line":0}},{"line":276,"address":[],"length":0,"stats":{"Line":0}},{"line":277,"address":[],"length":0,"stats":{"Line":0}},{"line":280,"address":[],"length":0,"stats":{"Line":0}},{"line":281,"address":[],"length":0,"stats":{"Line":0}},{"line":282,"address":[],"length":0,"stats":{"Line":0}},{"line":284,"address":[],"length":0,"stats":{"Line":0}},{"line":285,"address":[],"length":0,"stats":{"Line":0}},{"line":286,"address":[],"length":0,"stats":{"Line":0}},{"line":288,"address":[],"length":0,"stats":{"Line":0}},{"line":289,"address":[],"length":0,"stats":{"Line":0}},{"line":290,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":69},{"path":["/","Users","d4mr","work","thirdweb","engine-core","core","src","execution_options","aa.rs"],"content":"use crate::{\n    constants::{DEFAULT_FACTORY_ADDRESS_V0_6, ENTRYPOINT_ADDRESS_V0_6},\n    defs::AddressDef,\n};\nuse alloy::primitives::Address;\nuse schemars::JsonSchema;\nuse serde::{Deserialize, Deserializer, Serialize};\n\nuse crate::constants::{DEFAULT_FACTORY_ADDRESS_V0_7, ENTRYPOINT_ADDRESS_V0_7};\n\n#[derive(Deserialize, Serialize, Debug, JsonSchema, Clone, Copy)]\npub enum EntrypointVersion {\n    #[serde(rename = \"0.6\")]\n    V0_6,\n    #[serde(rename = \"0.7\")]\n    V0_7,\n}\n\n#[derive(Serialize, Debug, Clone)]\npub struct EntrypointAndFactoryDetails {\n    #[serde(rename = \"entrypointAddress\")]\n    pub entrypoint_address: Address,\n\n    #[serde(rename = \"entrypointVersion\")]\n    pub version: EntrypointVersion,\n\n    #[serde(rename = \"factoryAddress\")]\n    pub factory_address: Address,\n}\n\n/// # ERC-4337 Execution Options\n/// This struct allows flexible configuration of ERC-4337 execution options,\n/// with intelligent defaults and inferences based on provided values.\n///\n/// ## Field Inference\n/// When fields are omitted, the system uses the following inference rules:\n///\n/// 1. **Version Inference**:\n///    - If `entrypointVersion` is provided, it's used directly\n///    - Otherwise, tries to infer from `entrypointAddress` (if provided)\n///    - If that fails, tries to infer from `factoryAddress` (if provided)\n///    - Defaults to version 0.7 if no inference is possible\n///\n/// 2. **Entrypoint Address Inference**:\n///    - If provided explicitly, it's used as-is\n///    - Otherwise, uses the default address corresponding to the inferred version:\n///      - V0.6: 0x5FF137D4b0FDCD49DcA30c7CF57E578a026d2789\n///      - V0.7: 0x0576a174D229E3cFA37253523E645A78A0C91B57\n///\n/// 3. **Factory Address Inference**:\n///    - If provided explicitly, it's used as-is\n///    - Otherwise, uses the default factory corresponding to the inferred version:\n///      - V0.6: [DEFAULT_FACTORY_ADDRESS_V0_6]\n///      - V0.7: [DEFAULT_FACTORY_ADDRESS_V0_7]\n///\n/// 4. **Account Salt**:\n///    - If provided explicitly, it's used as-is\n///    - Otherwise, defaults to \"0x\" (commonly used as the defauult \"null\" salt for smart accounts)\n///\n/// 5. **Smart Account Address**:\n///    - If provided explicitly, it's used as-is\n///    - Otherwise, it's read from the smart account factory\n///\n/// All optional fields can be omitted for a minimal configuration using version 0.7 defaults.\n#[derive(Deserialize, Serialize, Debug, Clone, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct Erc4337ExecutionOptions {\n    #[schemars(with = \"AddressDef\")]\n    pub signer_address: Address,\n\n    #[serde(flatten)]\n    #[schemars(with = \"EntrypointAndFactoryDetailsDeserHelper\")]\n    pub entrypoint_details: EntrypointAndFactoryDetails,\n\n    #[serde(default = \"default_account_salt\")]\n    pub account_salt: String,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(with = \"Option::\u003cAddressDef\u003e\")]\n    pub smart_account_address: Option\u003cAddress\u003e,\n}\n\npub fn default_factory_address() -\u003e Address {\n    DEFAULT_FACTORY_ADDRESS_V0_7\n}\n\npub fn default_entrypoint_address() -\u003e Address {\n    ENTRYPOINT_ADDRESS_V0_7\n}\n\npub fn default_account_salt() -\u003e String {\n    \"0x\".to_string()\n}\n#[derive(Deserialize, JsonSchema)]\nstruct EntrypointAndFactoryDetailsDeserHelper {\n    /// # Entrypoint Contract Address\n    /// The address of the ERC-4337 entrypoint contract.\n    /// If omitted, defaults to the standard address for the specified/inferred version.\n    /// Known addresses:\n    /// - V0.6: 0x5FF137D4b0FDCD49DcA30c7CF57E578a026d2789\n    /// - V0.7: 0x0000000071727De22E5E9d8BAf0edAc6f37da032\n    #[serde(rename = \"entrypointAddress\")]\n    #[schemars(with = \"AddressDef\")]\n    entrypoint_address: Option\u003cAddress\u003e,\n\n    /// # Entrypoint Version\n    /// The version of the ERC-4337 standard to use.\n    /// If omitted, the version will be inferred from the entrypoint address,\n    /// then from factory address, or defaults to V0.7.\n    #[serde(rename = \"entrypointVersion\")]\n    version: Option\u003cEntrypointVersion\u003e,\n\n    /// # Account Factory Address\n    /// The address of the smart account factory contract.\n    /// If omitted, defaults to the thirweb default account factory for the specified/inferred version.\n    /// Known addresses:\n    /// - V0.6: 0x85e23b94e7F5E9cC1fF78BCe78cfb15B81f0DF00\n    /// - V0.7: 0x4bE0ddfebcA9A5A4a617dee4DeCe99E7c862dceb\n    #[serde(rename = \"factoryAddress\")]\n    #[schemars(with = \"AddressDef\")]\n    factory_address: Option\u003cAddress\u003e,\n}\n\nimpl EntrypointAndFactoryDetails {\n    // Helper function to get default entrypoint address for a version\n    fn default_entrypoint_for_version(version: EntrypointVersion) -\u003e Address {\n        match version {\n            EntrypointVersion::V0_6 =\u003e ENTRYPOINT_ADDRESS_V0_6,\n            EntrypointVersion::V0_7 =\u003e ENTRYPOINT_ADDRESS_V0_7,\n        }\n    }\n\n    // Helper function to get default factory address for a version\n    fn default_factory_for_version(version: EntrypointVersion) -\u003e Address {\n        match version {\n            EntrypointVersion::V0_6 =\u003e DEFAULT_FACTORY_ADDRESS_V0_6,\n            EntrypointVersion::V0_7 =\u003e DEFAULT_FACTORY_ADDRESS_V0_7,\n        }\n    }\n\n    // Helper to infer version from entrypoint address\n    fn infer_version_from_entrypoint(address: \u0026Address) -\u003e Option\u003cEntrypointVersion\u003e {\n        if *address == ENTRYPOINT_ADDRESS_V0_6 {\n            Some(EntrypointVersion::V0_6)\n        } else if *address == ENTRYPOINT_ADDRESS_V0_7 {\n            Some(EntrypointVersion::V0_7)\n        } else {\n            None\n        }\n    }\n\n    // Helper to infer version from factory address\n    fn infer_version_from_factory(address: \u0026Address) -\u003e Option\u003cEntrypointVersion\u003e {\n        if *address == DEFAULT_FACTORY_ADDRESS_V0_6 {\n            Some(EntrypointVersion::V0_6)\n        } else if *address == DEFAULT_FACTORY_ADDRESS_V0_7 {\n            Some(EntrypointVersion::V0_7)\n        } else {\n            None\n        }\n    }\n}\n\n// Define how to convert from the helper to the real struct\nimpl From\u003cEntrypointAndFactoryDetailsDeserHelper\u003e for EntrypointAndFactoryDetails {\n    fn from(helper: EntrypointAndFactoryDetailsDeserHelper) -\u003e Self {\n        // Default version to use if we can't infer anything better\n        let default_version = EntrypointVersion::V0_7;\n\n        // If version is explicitly provided, use it\n        let version = match helper.version {\n            Some(v) =\u003e v,\n            None =\u003e {\n                // Try to infer version from entrypoint address\n                if let Some(entrypoint) = helper.entrypoint_address {\n                    if let Some(v) = Self::infer_version_from_entrypoint(\u0026entrypoint) {\n                        v\n                    } else {\n                        // Try to infer from factory address\n                        helper\n                            .factory_address\n                            .as_ref()\n                            .and_then(Self::infer_version_from_factory)\n                            .unwrap_or(default_version)\n                    }\n                } else {\n                    // Try to infer from factory address\n                    helper\n                        .factory_address\n                        .as_ref()\n                        .and_then(Self::infer_version_from_factory)\n                        .unwrap_or(default_version)\n                }\n            }\n        };\n\n        // Use provided entrypoint address or default for the version\n        let entrypoint_address = helper\n            .entrypoint_address\n            .unwrap_or_else(|| Self::default_entrypoint_for_version(version));\n\n        // Use provided factory address or default for the version\n        let factory_address = helper\n            .factory_address\n            .unwrap_or_else(|| Self::default_factory_for_version(version));\n\n        EntrypointAndFactoryDetails {\n            entrypoint_address,\n            version,\n            factory_address,\n        }\n    }\n}\n\n// Add the custom deserializer\nimpl\u003c'de\u003e Deserialize\u003c'de\u003e for EntrypointAndFactoryDetails {\n    fn deserialize\u003cD\u003e(deserializer: D) -\u003e Result\u003cSelf, D::Error\u003e\n    where\n        D: Deserializer\u003c'de\u003e,\n    {\n        EntrypointAndFactoryDetailsDeserHelper::deserialize(deserializer).map(Into::into)\n    }\n}\n","traces":[{"line":83,"address":[],"length":0,"stats":{"Line":0}},{"line":84,"address":[],"length":0,"stats":{"Line":0}},{"line":87,"address":[],"length":0,"stats":{"Line":0}},{"line":88,"address":[],"length":0,"stats":{"Line":0}},{"line":91,"address":[],"length":0,"stats":{"Line":0}},{"line":92,"address":[],"length":0,"stats":{"Line":0}},{"line":126,"address":[],"length":0,"stats":{"Line":0}},{"line":127,"address":[],"length":0,"stats":{"Line":0}},{"line":128,"address":[],"length":0,"stats":{"Line":0}},{"line":129,"address":[],"length":0,"stats":{"Line":0}},{"line":134,"address":[],"length":0,"stats":{"Line":0}},{"line":135,"address":[],"length":0,"stats":{"Line":0}},{"line":136,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":144,"address":[],"length":0,"stats":{"Line":0}},{"line":145,"address":[],"length":0,"stats":{"Line":0}},{"line":146,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":153,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[],"length":0,"stats":{"Line":0}},{"line":168,"address":[],"length":0,"stats":{"Line":0}},{"line":171,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":175,"address":[],"length":0,"stats":{"Line":0}},{"line":176,"address":[],"length":0,"stats":{"Line":0}},{"line":177,"address":[],"length":0,"stats":{"Line":0}},{"line":180,"address":[],"length":0,"stats":{"Line":0}},{"line":181,"address":[],"length":0,"stats":{"Line":0}},{"line":183,"address":[],"length":0,"stats":{"Line":0}},{"line":184,"address":[],"length":0,"stats":{"Line":0}},{"line":188,"address":[],"length":0,"stats":{"Line":0}},{"line":189,"address":[],"length":0,"stats":{"Line":0}},{"line":191,"address":[],"length":0,"stats":{"Line":0}},{"line":192,"address":[],"length":0,"stats":{"Line":0}},{"line":198,"address":[],"length":0,"stats":{"Line":0}},{"line":199,"address":[],"length":0,"stats":{"Line":0}},{"line":200,"address":[],"length":0,"stats":{"Line":0}},{"line":203,"address":[],"length":0,"stats":{"Line":0}},{"line":204,"address":[],"length":0,"stats":{"Line":0}},{"line":205,"address":[],"length":0,"stats":{"Line":0}},{"line":217,"address":[],"length":0,"stats":{"Line":0}},{"line":221,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":49},{"path":["/","Users","d4mr","work","thirdweb","engine-core","core","src","execution_options","mod.rs"],"content":"use serde::{Deserialize, Serialize};\npub mod aa;\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct BaseExecutionOptions {\n    chain_id: u64,\n    #[serde(default = \"default_idempotency_key\")]\n    idempotency_key: String,\n}\n\nfn default_idempotency_key() -\u003e String {\n    uuid::Uuid::new_v4().to_string()\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(tag = \"type\")]\npub enum SpecificExecutionOptions {\n    ERC4337(aa::Erc4337ExecutionOptions),\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ExecutionOptions {\n    #[serde(flatten)]\n    pub base: BaseExecutionOptions,\n    #[serde(flatten)]\n    pub specific: SpecificExecutionOptions,\n}\n","traces":[{"line":12,"address":[],"length":0,"stats":{"Line":0}},{"line":13,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":2},{"path":["/","Users","d4mr","work","thirdweb","engine-core","core","src","executor","mod.rs"],"content":"","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","core","src","lib.rs"],"content":"pub mod chain;\npub mod constants;\npub mod credentials;\npub mod defs;\npub mod error;\npub mod execution_options;\npub mod rpc_clients;\npub mod transaction;\npub mod userop;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","core","src","rpc_clients","bundler.rs"],"content":"use alloy::primitives::{Address, Bytes, U256};\nuse alloy::rpc::client::RpcClient;\nuse alloy::rpc::types::UserOperationReceipt;\nuse alloy::transports::{IntoBoxTransport, TransportResult};\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\n\nuse crate::userop::UserOpVersion;\n\n// Gas buffer added for managed account factories (matches TypeScript)\npub const MANAGED_ACCOUNT_GAS_BUFFER: U256 = U256::from_limbs([21_000, 0, 0, 0]);\n\n/// A JSON-RPC client for interacting with an ERC-4337 bundler and paymaster\npub struct BundlerClient {\n    inner: RpcClient,\n}\n\n#[derive(Debug, Clone, PartialEq, Eq, Deserialize, Serialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct UseropGasEstimation {\n    pub call_gas_limit: U256,\n    pub verification_gas_limit: U256,\n    pub pre_verification_gas: U256,\n    #[serde(alias = \"paymasterVerificationGas\")]\n    pub paymaster_verification_gas_limit: U256,\n    #[serde(alias = \"paymasterPostOpGas\")]\n    pub paymaster_post_op_gas_limit: Option\u003cU256\u003e,\n}\n\nimpl BundlerClient {\n    /// Create a new bundler client with the given transport\n    pub fn new(transport: impl IntoBoxTransport) -\u003e Self {\n        let client = RpcClient::builder().transport(transport, false);\n\n        Self { inner: client }\n    }\n\n    /// Get a user operation receipt by hash\n    pub async fn get_user_op_receipt(\n        \u0026self,\n        user_op_hash: Bytes,\n    ) -\u003e TransportResult\u003cOption\u003cUserOperationReceipt\u003e\u003e {\n        self.inner\n            .request(\"eth_getUserOperationReceipt\", [user_op_hash])\n            .await\n    }\n\n    /// Estimate the gas for a user operation\n    pub async fn estimate_user_op_gas(\n        \u0026self,\n        user_op: \u0026UserOpVersion,\n        entrypoint: Address,\n        state_overrides: Option\u003cHashMap\u003cString, HashMap\u003cString, String\u003e\u003e\u003e,\n    ) -\u003e TransportResult\u003cUseropGasEstimation\u003e {\n        let state_overrides = state_overrides.unwrap_or_default();\n\n        // Convert the result and apply gas buffer to match TypeScript implementation\n        let result: UseropGasEstimation = self\n            .inner\n            .request(\n                \"eth_estimateUserOperationGas\",\n                (user_op, entrypoint, state_overrides),\n            )\n            .await?;\n\n        Ok(UseropGasEstimation {\n            call_gas_limit: result.call_gas_limit + MANAGED_ACCOUNT_GAS_BUFFER,\n            ..result\n        })\n    }\n}\n","traces":[{"line":32,"address":[],"length":0,"stats":{"Line":0}},{"line":33,"address":[],"length":0,"stats":{"Line":0}},{"line":39,"address":[],"length":0,"stats":{"Line":0}},{"line":43,"address":[],"length":0,"stats":{"Line":0}},{"line":44,"address":[],"length":0,"stats":{"Line":0}},{"line":45,"address":[],"length":0,"stats":{"Line":0}},{"line":49,"address":[],"length":0,"stats":{"Line":0}},{"line":55,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[],"length":0,"stats":{"Line":0}},{"line":66,"address":[],"length":0,"stats":{"Line":0}},{"line":67,"address":[],"length":0,"stats":{"Line":0}},{"line":68,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":15},{"path":["/","Users","d4mr","work","thirdweb","engine-core","core","src","rpc_clients","mod.rs"],"content":"mod bundler;\nmod paymaster;\n\npub use bundler::*;\npub use paymaster::*;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","core","src","rpc_clients","paymaster.rs"],"content":"use alloy::eips::eip1559::Eip1559Estimation;\nuse alloy::primitives::{Address, Bytes, U256};\nuse alloy::rpc::client::RpcClient;\nuse alloy::rpc::types::{PackedUserOperation, UserOperation};\nuse alloy::transports::{IntoBoxTransport, TransportResult};\nuse serde::{Deserialize, Serialize};\n\n/// Paymaster result for v0.6\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct PaymasterResultV06 {\n    pub pre_verification_gas: Option\u003cU256\u003e,\n    pub verification_gas_limit: Option\u003cU256\u003e,\n    pub call_gas_limit: Option\u003cU256\u003e,\n    pub paymaster_and_data: Bytes,\n}\n\n/// Paymaster result for v0.7\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct PaymasterResultV07 {\n    pub pre_verification_gas: Option\u003cU256\u003e,\n    pub verification_gas_limit: Option\u003cU256\u003e,\n    pub call_gas_limit: Option\u003cU256\u003e,\n    pub paymaster: Address,\n    pub paymaster_data: Bytes,\n    pub paymaster_verification_gas_limit: Option\u003cU256\u003e,\n    pub paymaster_post_op_gas_limit: Option\u003cU256\u003e,\n}\n\n/// A JSON-RPC client for interacting with an ERC-4337 bundler and paymaster\npub struct PaymasterClient {\n    inner: RpcClient,\n}\n\nimpl PaymasterClient {\n    /// Create a new bundler client with the given transport\n    pub fn new(transport: impl IntoBoxTransport) -\u003e Self {\n        let client = RpcClient::builder().transport(transport, false);\n        client.next_id();\n        Self { inner: client }\n    }\n\n    /// Get the user operation gas fees\n    pub async fn get_user_op_gas_fees(\u0026self) -\u003e TransportResult\u003cEip1559Estimation\u003e {\n        self.inner\n            .request(\"thirdweb_getUserOperationGasPrice\", ())\n            .await\n    }\n\n    /// Get paymaster and data for a user operation\n    pub async fn get_user_op_paymaster_and_data_v0_6(\n        \u0026self,\n        userop: \u0026UserOperation,\n        entrypoint: Address,\n    ) -\u003e TransportResult\u003cPaymasterResultV06\u003e {\n        self.inner\n            .request(\"pm_sponsorUserOperation\", (userop, entrypoint))\n            .await\n    }\n\n    /// Get paymaster and data for a user operation\n    pub async fn get_user_op_paymaster_and_data_v0_7(\n        \u0026self,\n        userop: \u0026PackedUserOperation,\n        entrypoint: Address,\n    ) -\u003e TransportResult\u003cPaymasterResultV07\u003e {\n        self.inner\n            .request(\"pm_sponsorUserOperation\", (userop, entrypoint))\n            .await\n    }\n}\n","traces":[{"line":38,"address":[],"length":0,"stats":{"Line":0}},{"line":39,"address":[],"length":0,"stats":{"Line":0}},{"line":40,"address":[],"length":0,"stats":{"Line":0}},{"line":45,"address":[],"length":0,"stats":{"Line":0}},{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":47,"address":[],"length":0,"stats":{"Line":0}},{"line":48,"address":[],"length":0,"stats":{"Line":0}},{"line":52,"address":[],"length":0,"stats":{"Line":0}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":68,"address":[],"length":0,"stats":{"Line":0}},{"line":69,"address":[],"length":0,"stats":{"Line":0}},{"line":70,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":15},{"path":["/","Users","d4mr","work","thirdweb","engine-core","core","src","transaction.rs"],"content":"use alloy::primitives::{Address, Bytes, U256};\nuse serde::{Deserialize, Serialize};\n\n#[derive(Deserialize, Serialize, Debug, Clone)]\npub struct InnerTransaction {\n    pub to: Address,\n    pub data: Bytes,\n    pub value: U256,\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","core","src","userop.rs"],"content":"use alloy::{\n    hex::FromHex,\n    primitives::{Address, Bytes, ChainId},\n    rpc::types::{PackedUserOperation, UserOperation},\n};\nuse serde::{Deserialize, Serialize};\nuse vault_sdk::VaultClient;\nuse vault_types::{\n    enclave::encrypted::eoa::StructuredMessageInput,\n    userop::{UserOperationV06Input, UserOperationV07Input},\n};\n\nuse crate::{credentials::SigningCredential, error::EngineError};\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(untagged)]\npub enum UserOpVersion {\n    V0_6(UserOperation),\n    V0_7(PackedUserOperation),\n}\n\npub struct UserOpSigner {\n    pub vault_client: VaultClient,\n}\n\npub struct UserOpSignerParams {\n    pub credentials: SigningCredential,\n    pub entrypoint: Address,\n    pub userop: UserOpVersion,\n    pub signer_address: Address,\n    pub chain_id: ChainId,\n}\n\nimpl UserOpVersion {\n    fn to_vault_input(\u0026self, entrypoint: Address) -\u003e StructuredMessageInput {\n        match self {\n            UserOpVersion::V0_6(userop) =\u003e {\n                StructuredMessageInput::UserOperationV06Input(UserOperationV06Input {\n                    call_data: userop.call_data.clone(),\n                    init_code: userop.init_code.clone(),\n                    nonce: userop.nonce,\n                    pre_verification_gas: userop.pre_verification_gas,\n                    max_fee_per_gas: userop.max_fee_per_gas,\n                    verification_gas_limit: userop.verification_gas_limit,\n                    sender: userop.sender.clone(),\n                    paymaster_and_data: userop.paymaster_and_data.clone(),\n                    signature: userop.signature.clone(),\n                    call_gas_limit: userop.call_gas_limit,\n                    max_priority_fee_per_gas: userop.max_priority_fee_per_gas,\n                    entrypoint,\n                })\n            }\n            UserOpVersion::V0_7(userop) =\u003e {\n                StructuredMessageInput::UserOperationV07Input(UserOperationV07Input {\n                    call_data: userop.call_data.clone(),\n                    nonce: userop.nonce,\n                    pre_verification_gas: userop.pre_verification_gas,\n                    max_fee_per_gas: userop.max_fee_per_gas,\n                    verification_gas_limit: userop.verification_gas_limit,\n                    sender: userop.sender.clone(),\n                    paymaster_data: userop.paymaster_data.clone().unwrap_or_default(),\n                    factory: userop.factory.unwrap_or_default(),\n                    factory_data: userop.factory_data.clone().unwrap_or_default(),\n                    paymaster_post_op_gas_limit: userop\n                        .paymaster_post_op_gas_limit\n                        .unwrap_or_default(),\n                    paymaster_verification_gas_limit: userop\n                        .paymaster_verification_gas_limit\n                        .unwrap_or_default(),\n                    signature: userop.signature.clone(),\n                    call_gas_limit: userop.call_gas_limit,\n                    max_priority_fee_per_gas: userop.max_priority_fee_per_gas,\n                    paymaster: userop.paymaster.unwrap_or_default(),\n                    entrypoint,\n                })\n            }\n        }\n    }\n}\n\nimpl UserOpSigner {\n    pub async fn sign(\u0026self, params: UserOpSignerParams) -\u003e Result\u003cBytes, EngineError\u003e {\n        match \u0026params.credentials {\n            SigningCredential::Vault(auth_method) =\u003e {\n                let vault_result = self\n                    .vault_client\n                    .sign_structured_message(\n                        auth_method.clone(),\n                        params.signer_address,\n                        params.userop.to_vault_input(params.entrypoint),\n                        Some(params.chain_id),\n                    )\n                    .await\n                    .map_err(|e| {\n                        tracing::error!(\"Error signing userop: {:?}\", e);\n                        EngineError::VaultError {\n                            message: e.to_string(),\n                        }\n                    })?;\n\n                Ok(Bytes::from_hex(vault_result.signature).map_err(|_| {\n                    EngineError::VaultError {\n                        message: \"Bad signature received from vault\".to_string(),\n                    }\n                })?)\n            }\n        }\n    }\n}\n","traces":[{"line":35,"address":[],"length":0,"stats":{"Line":0}},{"line":36,"address":[],"length":0,"stats":{"Line":0}},{"line":37,"address":[],"length":0,"stats":{"Line":0}},{"line":38,"address":[],"length":0,"stats":{"Line":0}},{"line":39,"address":[],"length":0,"stats":{"Line":0}},{"line":40,"address":[],"length":0,"stats":{"Line":0}},{"line":41,"address":[],"length":0,"stats":{"Line":0}},{"line":42,"address":[],"length":0,"stats":{"Line":0}},{"line":43,"address":[],"length":0,"stats":{"Line":0}},{"line":44,"address":[],"length":0,"stats":{"Line":0}},{"line":45,"address":[],"length":0,"stats":{"Line":0}},{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":47,"address":[],"length":0,"stats":{"Line":0}},{"line":48,"address":[],"length":0,"stats":{"Line":0}},{"line":49,"address":[],"length":0,"stats":{"Line":0}},{"line":50,"address":[],"length":0,"stats":{"Line":0}},{"line":53,"address":[],"length":0,"stats":{"Line":0}},{"line":54,"address":[],"length":0,"stats":{"Line":0}},{"line":55,"address":[],"length":0,"stats":{"Line":0}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":60,"address":[],"length":0,"stats":{"Line":0}},{"line":61,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[],"length":0,"stats":{"Line":0}},{"line":65,"address":[],"length":0,"stats":{"Line":0}},{"line":66,"address":[],"length":0,"stats":{"Line":0}},{"line":67,"address":[],"length":0,"stats":{"Line":0}},{"line":68,"address":[],"length":0,"stats":{"Line":0}},{"line":69,"address":[],"length":0,"stats":{"Line":0}},{"line":70,"address":[],"length":0,"stats":{"Line":0}},{"line":71,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":73,"address":[],"length":0,"stats":{"Line":0}},{"line":74,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[],"length":0,"stats":{"Line":0}},{"line":83,"address":[],"length":0,"stats":{"Line":0}},{"line":84,"address":[],"length":0,"stats":{"Line":0}},{"line":85,"address":[],"length":0,"stats":{"Line":0}},{"line":86,"address":[],"length":0,"stats":{"Line":0}},{"line":88,"address":[],"length":0,"stats":{"Line":0}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":91,"address":[],"length":0,"stats":{"Line":0}},{"line":93,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":96,"address":[],"length":0,"stats":{"Line":0}},{"line":97,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":102,"address":[],"length":0,"stats":{"Line":0}},{"line":103,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":55},{"path":["/","Users","d4mr","work","thirdweb","engine-core","executors","src","lib.rs"],"content":"pub mod webhook;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","executors","src","webhook","mod.rs"],"content":"use std::collections::HashMap;\nuse std::sync::Arc;\nuse std::time::{Duration, SystemTime, UNIX_EPOCH};\n\nuse hex;\nuse hmac::{Hmac, Mac};\nuse reqwest::header::{HeaderMap, HeaderName, HeaderValue};\nuse serde::{Deserialize, Serialize};\nuse twmq::hooks::TransactionContext;\nuse twmq::job::{Job, JobResult, RequeuePosition};\nuse twmq::{DurableExecution, FailHookData, NackHookData, SuccessHookData};\n\n// --- Configuration ---\n#[derive(Clone, Debug)]\npub struct WebhookRetryConfig {\n    pub max_attempts: u32,\n    pub initial_delay_ms: u64,\n    pub max_delay_ms: u64,\n    pub backoff_factor: f64,\n}\n\nimpl Default for WebhookRetryConfig {\n    fn default() -\u003e Self {\n        Self {\n            max_attempts: 5,\n            initial_delay_ms: 1000, // 1 second\n            max_delay_ms: 30000,    // 30 seconds\n            backoff_factor: 2.0,\n        }\n    }\n}\n\n// --- Execution Context ---\n#[derive(Clone)]\npub struct WebhookExecutionContext {\n    pub http_client: reqwest::Client,\n    pub retry_config: Arc\u003cWebhookRetryConfig\u003e,\n}\n\n// --- Webhook Job Data Structures ---\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct WebhookJobPayload {\n    pub url: String,\n    pub body: String, // Assuming pre-serialized JSON or other content\n    pub headers: Option\u003cHashMap\u003cString, String\u003e\u003e,\n    pub hmac_secret: Option\u003cString\u003e, // Secret key for HMAC-SHA256\n    pub http_method: Option\u003cString\u003e, // e.g., \"POST\", \"PUT\". Defaults to \"POST\"\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]\npub struct WebhookJobOutput {\n    pub status_code: u16,\n    pub response_body: Option\u003cString\u003e,\n}\n\n// --- Webhook Error Enum ---\n#[derive(Serialize, Deserialize, Debug, Clone, thiserror::Error)]\npub enum WebhookError {\n    #[error(\"Network error during webhook dispatch: {0}\")]\n    Network(String),\n\n    #[error(\"Failed to construct webhook request: {0}\")]\n    RequestConstruction(String),\n\n    #[error(\"HMAC signature generation failed: {0}\")]\n    HmacGeneration(String),\n\n    #[error(\"Webhook request timed out: {0}\")]\n    Timeout(String),\n\n    #[error(\"HTTP error from endpoint: status {status}, body: {body_preview}\")]\n    Http {\n        status: u16,\n        body_preview: String, // Store a preview of the raw response body\n    },\n\n    #[error(\"Failed to read or decode webhook response body: {0}\")]\n    ResponseReadError(String),\n\n    #[error(\"Unsupported HTTP method: {0}\")]\n    UnsupportedHttpMethod(String),\n}\n\n// --- DurableExecution Implementation ---\nimpl DurableExecution for WebhookJobPayload {\n    type Output = WebhookJobOutput;\n    type ErrorData = WebhookError;\n    type ExecutionContext = WebhookExecutionContext;\n\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        ec: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        let payload = \u0026job.data;\n        let mut request_headers = HeaderMap::new();\n\n        // Set default Content-Type if body is present, can be overridden by payload.headers\n        if !payload.body.is_empty() {\n            request_headers.insert(\n                reqwest::header::CONTENT_TYPE,\n                HeaderValue::from_static(\"application/json; charset=utf-8\"),\n            );\n        }\n\n        // Apply custom headers\n        if let Some(custom_headers) = \u0026payload.headers {\n            for (key, value) in custom_headers {\n                match HeaderName::from_bytes(key.as_bytes()) {\n                    Ok(header_name) =\u003e match HeaderValue::from_str(value) {\n                        Ok(header_value) =\u003e {\n                            request_headers.insert(header_name, header_value);\n                        }\n                        Err(e) =\u003e {\n                            return JobResult::Fail(WebhookError::RequestConstruction(format!(\n                                \"Invalid header value for '{}': {}\",\n                                key, e\n                            )));\n                        }\n                    },\n                    Err(e) =\u003e {\n                        return JobResult::Fail(WebhookError::RequestConstruction(format!(\n                            \"Invalid header name '{}': {}\",\n                            key, e\n                        )));\n                    }\n                }\n            }\n        }\n\n        // HMAC Signature with Timestamp\n        if let Some(secret) = \u0026payload.hmac_secret {\n            if secret.is_empty() {\n                return JobResult::Fail(WebhookError::HmacGeneration(\n                    \"HMAC secret cannot be empty\".to_string(),\n                ));\n            }\n\n            let timestamp_secs = match SystemTime::now().duration_since(UNIX_EPOCH) {\n                Ok(duration) =\u003e duration.as_secs(),\n                Err(e) =\u003e {\n                    // This is highly unlikely to fail but good to handle\n                    return JobResult::Fail(WebhookError::RequestConstruction(format!(\n                        \"Failed to get system time for timestamp: {}\",\n                        e\n                    )));\n                }\n            };\n            let timestamp_str = timestamp_secs.to_string();\n\n            // Canonical message to sign: \"timestamp.body\"\n            // The dot (.) is a common separator.\n            let message_to_sign = format!(\"{}.{}\", timestamp_str, payload.body);\n\n            type HmacSha256 = Hmac\u003csha2::Sha256\u003e;\n            let mut mac = match HmacSha256::new_from_slice(secret.as_bytes()) {\n                Ok(m) =\u003e m,\n                Err(e) =\u003e {\n                    return JobResult::Fail(WebhookError::HmacGeneration(format!(\n                        \"Failed to initialize HMAC: {}\",\n                        e\n                    )));\n                }\n            };\n            mac.update(message_to_sign.as_bytes());\n            let signature_bytes = mac.finalize().into_bytes();\n            let signature_hex = hex::encode(signature_bytes);\n\n            // Standard header names\n            let signature_header_name = \"X-Signature-SHA256\";\n            let timestamp_header_name = \"X-Request-Timestamp\";\n\n            match HeaderValue::from_str(\u0026signature_hex) {\n                Ok(val) =\u003e {\n                    request_headers.insert(HeaderName::from_static(signature_header_name), val);\n                }\n                Err(_) =\u003e {\n                    // Should not happen with hex string\n                    return JobResult::Fail(WebhookError::HmacGeneration(\n                        \"Generated HMAC signature is not a valid header value\".to_string(),\n                    ));\n                }\n            }\n\n            match HeaderValue::from_str(\u0026timestamp_str) {\n                Ok(val) =\u003e {\n                    request_headers.insert(HeaderName::from_static(timestamp_header_name), val);\n                }\n                Err(_) =\u003e {\n                    // Should not happen with a string representation of u64\n                    return JobResult::Fail(WebhookError::RequestConstruction(\n                        \"Timestamp is not a valid header value\".to_string(),\n                    ));\n                }\n            }\n        }\n\n        let http_method_str = payload\n            .http_method\n            .as_deref()\n            .unwrap_or(\"POST\")\n            .to_uppercase();\n        let method = match reqwest::Method::from_bytes(http_method_str.as_bytes()) {\n            Ok(m) =\u003e m,\n            Err(_) =\u003e {\n                return JobResult::Fail(WebhookError::UnsupportedHttpMethod(http_method_str));\n            }\n        };\n\n        let request_builder = ec\n            .http_client\n            .request(method, \u0026payload.url)\n            .headers(request_headers)\n            .body(payload.body.clone());\n\n        tracing::debug!(\n            job_id = %job.id,\n            url = %payload.url,\n            method = %http_method_str,\n            attempt = %job.attempts,\n            \"Sending webhook request\"\n        );\n\n        match request_builder.send().await {\n            Ok(response) =\u003e {\n                let status = response.status();\n                let response_body_text_result = response.text().await;\n\n                let response_body_text = match response_body_text_result {\n                    Ok(text) =\u003e Some(text),\n                    Err(e) =\u003e {\n                        if status.is_success() {\n                            let err = WebhookError::ResponseReadError(format!(\n                                \"Failed to read response body: {}\",\n                                e\n                            ));\n                            return JobResult::Fail(err);\n                        }\n                        tracing::warn!(job_id = %job.id, \"Failed to read response body for error status {}: {}\", status, e);\n                        None\n                    }\n                };\n\n                if status.is_success() {\n                    tracing::info!(job_id = %job.id, status = %status, \"Webhook delivered successfully\");\n                    JobResult::Success(WebhookJobOutput {\n                        status_code: status.as_u16(),\n                        response_body: response_body_text,\n                    })\n                } else {\n                    let error_body_preview = response_body_text\n                        .map(|s| {\n                            if s.len() \u003e 512 {\n                                format!(\"{}...\", \u0026s[..512])\n                            } else {\n                                s\n                            }\n                        })\n                        .unwrap_or_else(|| \"No body or failed to read body\".to_string());\n\n                    let webhook_error = WebhookError::Http {\n                        status: status.as_u16(),\n                        body_preview: error_body_preview,\n                    };\n\n                    if status.is_server_error() || status.as_u16() == 429 {\n                        if job.attempts \u003c ec.retry_config.max_attempts {\n                            let delay_ms = ec.retry_config.initial_delay_ms as f64\n                                * ec.retry_config.backoff_factor.powi(job.attempts as i32 - 1); // Use current attempts for backoff\n                            let delay_ms =\n                                (delay_ms.min(ec.retry_config.max_delay_ms as f64)) as u64;\n                            let delay = Duration::from_millis(delay_ms);\n\n                            tracing::warn!(\n                                job_id = %job.id,\n                                status = %status,\n                                attempt = %job.attempts,\n                                max_attempts = %ec.retry_config.max_attempts,\n                                delay_ms = %delay.as_millis(),\n                                \"Webhook failed with retryable status, NACKing.\"\n                            );\n                            JobResult::Nack {\n                                error: webhook_error,\n                                delay: Some(delay),\n                                position: RequeuePosition::Last,\n                            }\n                        } else {\n                            tracing::error!(\n                                job_id = %job.id,\n                                status = %status,\n                                attempt = %job.attempts,\n                                \"Webhook failed after max attempts, FAILING.\"\n                            );\n                            JobResult::Fail(webhook_error)\n                        }\n                    } else {\n                        tracing::error!(\n                            job_id = %job.id,\n                            status = %status,\n                            \"Webhook failed with non-retryable client error, FAILING.\"\n                        );\n                        JobResult::Fail(webhook_error)\n                    }\n                }\n            }\n            Err(e) =\u003e {\n                let webhook_error = if e.is_timeout() {\n                    WebhookError::Timeout(e.to_string())\n                } else if e.is_connect() || e.is_request() {\n                    WebhookError::Network(e.to_string())\n                } else {\n                    WebhookError::RequestConstruction(e.to_string())\n                };\n\n                if matches!(webhook_error, WebhookError::RequestConstruction(_))\n                    \u0026\u0026 !e.is_connect()\n                    \u0026\u0026 !e.is_timeout()\n                {\n                    tracing::error!(job_id = %job.id, error = %webhook_error, \"Webhook construction error, FAILING.\");\n                    return JobResult::Fail(webhook_error);\n                }\n\n                if job.attempts \u003c ec.retry_config.max_attempts {\n                    let delay_ms = ec.retry_config.initial_delay_ms as f64\n                        * ec.retry_config.backoff_factor.powi(job.attempts as i32 - 1); // Use current attempts for backoff\n                    let delay_ms = (delay_ms.min(ec.retry_config.max_delay_ms as f64)) as u64;\n                    let delay = Duration::from_millis(delay_ms);\n\n                    tracing::warn!(\n                        job_id = %job.id,\n                        error = %webhook_error,\n                        attempt = %job.attempts,\n                        max_attempts = %ec.retry_config.max_attempts,\n                        delay_ms = %delay.as_millis(),\n                        \"Webhook request failed, NACKing.\"\n                    );\n                    JobResult::Nack {\n                        error: webhook_error,\n                        delay: Some(delay),\n                        position: RequeuePosition::Last,\n                    }\n                } else {\n                    tracing::error!(\n                        job_id = %job.id,\n                        error = %webhook_error,\n                        attempt = %job.attempts,\n                        \"Webhook request failed after max attempts, FAILING.\"\n                    );\n                    JobResult::Fail(webhook_error)\n                }\n            }\n        }\n    }\n\n    async fn on_success(\n        \u0026self,\n        job: \u0026Job\u003cSelf\u003e,\n        d: SuccessHookData\u003c'_, Self::Output\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\n            job_id = %job.id,\n            url = %self.url,\n            status = %d.result.status_code,\n            \"Webhook successfully processed (on_success hook).\"\n        );\n    }\n\n    async fn on_nack(\n        \u0026self,\n        job: \u0026Job\u003cSelf\u003e,\n        d: NackHookData\u003c'_, Self::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::warn!(\n            job_id = %job.id,\n            url = %self.url,\n            attempt = %job.attempts,\n            error = ?d.error,\n            delay_ms = %d.delay.map_or(0, |dur| dur.as_millis()),\n            \"Webhook NACKed (on_nack hook).\"\n        );\n    }\n\n    async fn on_fail(\n        \u0026self,\n        job: \u0026Job\u003cSelf\u003e,\n        d: FailHookData\u003c'_, Self::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::error!(\n            job_id = %job.id,\n            url = %self.url,\n            attempt = %job.attempts,\n            error = ?d.error,\n            \"Webhook FAILED permanently (on_fail hook).\"\n        );\n    }\n}\n","traces":[{"line":23,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":98,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":100,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":106,"address":[],"length":0,"stats":{"Line":0}},{"line":107,"address":[],"length":0,"stats":{"Line":0}},{"line":108,"address":[],"length":0,"stats":{"Line":0}},{"line":109,"address":[],"length":0,"stats":{"Line":0}},{"line":110,"address":[],"length":0,"stats":{"Line":0}},{"line":111,"address":[],"length":0,"stats":{"Line":0}},{"line":113,"address":[],"length":0,"stats":{"Line":0}},{"line":114,"address":[],"length":0,"stats":{"Line":0}},{"line":115,"address":[],"length":0,"stats":{"Line":0}},{"line":116,"address":[],"length":0,"stats":{"Line":0}},{"line":120,"address":[],"length":0,"stats":{"Line":0}},{"line":121,"address":[],"length":0,"stats":{"Line":0}},{"line":122,"address":[],"length":0,"stats":{"Line":0}},{"line":123,"address":[],"length":0,"stats":{"Line":0}},{"line":131,"address":[],"length":0,"stats":{"Line":0}},{"line":132,"address":[],"length":0,"stats":{"Line":0}},{"line":133,"address":[],"length":0,"stats":{"Line":0}},{"line":134,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":139,"address":[],"length":0,"stats":{"Line":0}},{"line":140,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":144,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":152,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":160,"address":[],"length":0,"stats":{"Line":0}},{"line":164,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[],"length":0,"stats":{"Line":0}},{"line":169,"address":[],"length":0,"stats":{"Line":0}},{"line":170,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[],"length":0,"stats":{"Line":0}},{"line":178,"address":[],"length":0,"stats":{"Line":0}},{"line":179,"address":[],"length":0,"stats":{"Line":0}},{"line":184,"address":[],"length":0,"stats":{"Line":0}},{"line":185,"address":[],"length":0,"stats":{"Line":0}},{"line":186,"address":[],"length":0,"stats":{"Line":0}},{"line":190,"address":[],"length":0,"stats":{"Line":0}},{"line":191,"address":[],"length":0,"stats":{"Line":0}},{"line":197,"address":[],"length":0,"stats":{"Line":0}},{"line":198,"address":[],"length":0,"stats":{"Line":0}},{"line":202,"address":[],"length":0,"stats":{"Line":0}},{"line":203,"address":[],"length":0,"stats":{"Line":0}},{"line":205,"address":[],"length":0,"stats":{"Line":0}},{"line":209,"address":[],"length":0,"stats":{"Line":0}},{"line":210,"address":[],"length":0,"stats":{"Line":0}},{"line":211,"address":[],"length":0,"stats":{"Line":0}},{"line":212,"address":[],"length":0,"stats":{"Line":0}},{"line":213,"address":[],"length":0,"stats":{"Line":0}},{"line":215,"address":[],"length":0,"stats":{"Line":0}},{"line":220,"address":[],"length":0,"stats":{"Line":0}},{"line":223,"address":[],"length":0,"stats":{"Line":0}},{"line":224,"address":[],"length":0,"stats":{"Line":0}},{"line":225,"address":[],"length":0,"stats":{"Line":0}},{"line":226,"address":[],"length":0,"stats":{"Line":0}},{"line":228,"address":[],"length":0,"stats":{"Line":0}},{"line":229,"address":[],"length":0,"stats":{"Line":0}},{"line":230,"address":[],"length":0,"stats":{"Line":0}},{"line":231,"address":[],"length":0,"stats":{"Line":0}},{"line":232,"address":[],"length":0,"stats":{"Line":0}},{"line":233,"address":[],"length":0,"stats":{"Line":0}},{"line":234,"address":[],"length":0,"stats":{"Line":0}},{"line":236,"address":[],"length":0,"stats":{"Line":0}},{"line":238,"address":[],"length":0,"stats":{"Line":0}},{"line":239,"address":[],"length":0,"stats":{"Line":0}},{"line":243,"address":[],"length":0,"stats":{"Line":0}},{"line":244,"address":[],"length":0,"stats":{"Line":0}},{"line":245,"address":[],"length":0,"stats":{"Line":0}},{"line":246,"address":[],"length":0,"stats":{"Line":0}},{"line":247,"address":[],"length":0,"stats":{"Line":0}},{"line":250,"address":[],"length":0,"stats":{"Line":0}},{"line":251,"address":[],"length":0,"stats":{"Line":0}},{"line":252,"address":[],"length":0,"stats":{"Line":0}},{"line":253,"address":[],"length":0,"stats":{"Line":0}},{"line":255,"address":[],"length":0,"stats":{"Line":0}},{"line":258,"address":[],"length":0,"stats":{"Line":0}},{"line":261,"address":[],"length":0,"stats":{"Line":0}},{"line":265,"address":[],"length":0,"stats":{"Line":0}},{"line":266,"address":[],"length":0,"stats":{"Line":0}},{"line":267,"address":[],"length":0,"stats":{"Line":0}},{"line":268,"address":[],"length":0,"stats":{"Line":0}},{"line":269,"address":[],"length":0,"stats":{"Line":0}},{"line":270,"address":[],"length":0,"stats":{"Line":0}},{"line":271,"address":[],"length":0,"stats":{"Line":0}},{"line":273,"address":[],"length":0,"stats":{"Line":0}},{"line":277,"address":[],"length":0,"stats":{"Line":0}},{"line":278,"address":[],"length":0,"stats":{"Line":0}},{"line":279,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":287,"address":[],"length":0,"stats":{"Line":0}},{"line":291,"address":[],"length":0,"stats":{"Line":0}},{"line":293,"address":[],"length":0,"stats":{"Line":0}},{"line":296,"address":[],"length":0,"stats":{"Line":0}},{"line":299,"address":[],"length":0,"stats":{"Line":0}},{"line":301,"address":[],"length":0,"stats":{"Line":0}},{"line":305,"address":[],"length":0,"stats":{"Line":0}},{"line":306,"address":[],"length":0,"stats":{"Line":0}},{"line":307,"address":[],"length":0,"stats":{"Line":0}},{"line":308,"address":[],"length":0,"stats":{"Line":0}},{"line":309,"address":[],"length":0,"stats":{"Line":0}},{"line":311,"address":[],"length":0,"stats":{"Line":0}},{"line":314,"address":[],"length":0,"stats":{"Line":0}},{"line":315,"address":[],"length":0,"stats":{"Line":0}},{"line":316,"address":[],"length":0,"stats":{"Line":0}},{"line":318,"address":[],"length":0,"stats":{"Line":0}},{"line":319,"address":[],"length":0,"stats":{"Line":0}},{"line":322,"address":[],"length":0,"stats":{"Line":0}},{"line":323,"address":[],"length":0,"stats":{"Line":0}},{"line":324,"address":[],"length":0,"stats":{"Line":0}},{"line":325,"address":[],"length":0,"stats":{"Line":0}},{"line":326,"address":[],"length":0,"stats":{"Line":0}},{"line":328,"address":[],"length":0,"stats":{"Line":0}},{"line":332,"address":[],"length":0,"stats":{"Line":0}},{"line":333,"address":[],"length":0,"stats":{"Line":0}},{"line":334,"address":[],"length":0,"stats":{"Line":0}},{"line":338,"address":[],"length":0,"stats":{"Line":0}},{"line":342,"address":[],"length":0,"stats":{"Line":0}},{"line":346,"address":[],"length":0,"stats":{"Line":0}},{"line":348,"address":[],"length":0,"stats":{"Line":0}},{"line":354,"address":[],"length":0,"stats":{"Line":0}},{"line":361,"address":[],"length":0,"stats":{"Line":0}},{"line":365,"address":[],"length":0,"stats":{"Line":0}},{"line":369,"address":[],"length":0,"stats":{"Line":0}},{"line":376,"address":[],"length":0,"stats":{"Line":0}},{"line":381,"address":[],"length":0,"stats":{"Line":0}},{"line":382,"address":[],"length":0,"stats":{"Line":0}},{"line":386,"address":[],"length":0,"stats":{"Line":0}},{"line":393,"address":[],"length":0,"stats":{"Line":0}},{"line":398,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":145},{"path":["/","Users","d4mr","work","thirdweb","engine-core","server","src","chains.rs"],"content":"use engine_core::{\n    chain::{Chain, ThirdwebChain, ThirdwebChainConfig},\n    error::EngineError,\n};\n\npub trait ChainService {\n    fn get_chain(\u0026self, chain_id: u64) -\u003e Result\u003cimpl Chain, EngineError\u003e;\n}\n\npub struct ThirdwebChainService {\n    pub client_id: String,\n    pub secret_key: String,\n    pub rpc_base_url: String,\n    pub bundler_base_url: String,\n    pub paymaster_base_url: String,\n}\n\n#[allow(refining_impl_trait)]\nimpl ChainService for ThirdwebChainService {\n    fn get_chain(\u0026self, chain_id: u64) -\u003e Result\u003cThirdwebChain, EngineError\u003e {\n        ThirdwebChainConfig {\n            chain_id,\n            rpc_base_url: \u0026self.rpc_base_url,\n            bundler_base_url: \u0026self.bundler_base_url,\n            paymaster_base_url: \u0026self.paymaster_base_url,\n            client_id: \u0026self.client_id,\n            secret_key: \u0026self.secret_key,\n        }\n        .to_chain()\n    }\n}\n","traces":[{"line":20,"address":[],"length":0,"stats":{"Line":0}},{"line":23,"address":[],"length":0,"stats":{"Line":0}},{"line":24,"address":[],"length":0,"stats":{"Line":0}},{"line":25,"address":[],"length":0,"stats":{"Line":0}},{"line":26,"address":[],"length":0,"stats":{"Line":0}},{"line":27,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":6},{"path":["/","Users","d4mr","work","thirdweb","engine-core","server","src","config.rs"],"content":"use std::env;\n\nuse config::{Config, File};\nuse serde::Deserialize;\n\n#[derive(Debug, Clone, Deserialize)]\npub struct EngineConfig {\n    pub server: ServerConfig,\n    pub thirdweb: ThirdwebConfig,\n}\n\n#[derive(Debug, Clone, Deserialize)]\n#[serde(default)]\npub struct ServerConfig {\n    pub host: String,\n    pub port: u16,\n}\n\n#[derive(Debug, Clone, Deserialize)]\npub struct ThirdwebConfig {\n    pub secret: String,\n    pub client_id: String,\n    pub urls: ThirdwebUrls,\n}\n\n#[derive(Debug, Clone, Deserialize)]\npub struct ThirdwebUrls {\n    pub rpc: String,\n    pub bundler: String,\n    pub vault: String,\n    pub paymaster: String,\n}\n\nimpl Default for ServerConfig {\n    fn default() -\u003e Self {\n        Self {\n            port: 3000,\n            host: \"0.0.0.0\".into(),\n        }\n    }\n}\n\npub fn get_config() -\u003e EngineConfig {\n    let base_path = env::current_dir().expect(\"Failed to determine the current directory\");\n    let configuration_directory = base_path.join(\"configuration\");\n\n    // Detect the running environment\n    let environment: Environment = env::var(\"APP_ENVIRONMENT\")\n        .unwrap_or_else(|_| \"local\".into())\n        .try_into()\n        .expect(\"Failed to parse APP_ENVIRONMENT\");\n\n    let environment_filename = format!(\"server_{}.yaml\", environment.as_str());\n\n    // Load configuration from files\n    let config = Config::builder()\n        .add_source(File::from(configuration_directory.join(\"server_base.yaml\")))\n        .add_source(File::from(\n            configuration_directory.join(environment_filename),\n        ))\n        .add_source(config::Environment::with_prefix(\"app\").separator(\"__\"))\n        .build()\n        .unwrap_or_else(|e| {\n            eprintln!(\"Configuration error: {}\", e);\n            panic!(\"Failed to build configuration\");\n        });\n\n    // Deserialize the configuration\n    config.try_deserialize::\u003cEngineConfig\u003e()\n        .unwrap_or_else(|e| {\n            eprintln!(\"Configuration error: {}\", e);\n            eprintln!(\"Make sure all required fields are set correctly in your configuration files or environment variables.\");\n            panic!(\"Failed to deserialize configuration\");\n        })\n}\n\n/// The possible runtime environment for our application.\npub enum Environment {\n    Local,\n    Development,\n    Production,\n}\n\nimpl Environment {\n    pub fn as_str(\u0026self) -\u003e \u0026'static str {\n        match self {\n            Environment::Local =\u003e \"local\",\n            Environment::Development =\u003e \"development\",\n            Environment::Production =\u003e \"production\",\n        }\n    }\n}\n\nimpl TryFrom\u003cString\u003e for Environment {\n    type Error = String;\n\n    fn try_from(s: String) -\u003e Result\u003cSelf, Self::Error\u003e {\n        match s.to_lowercase().as_str() {\n            \"local\" =\u003e Ok(Self::Local),\n            \"development\" =\u003e Ok(Self::Development),\n            \"production\" =\u003e Ok(Self::Production),\n            other =\u003e Err(format!(\n                \"{} is not a supported environment. Use either `local`, `development`, or `production`.\",\n                other\n            )),\n        }\n    }\n}\n","traces":[{"line":35,"address":[],"length":0,"stats":{"Line":0}},{"line":38,"address":[],"length":0,"stats":{"Line":0}},{"line":43,"address":[],"length":0,"stats":{"Line":0}},{"line":44,"address":[],"length":0,"stats":{"Line":0}},{"line":45,"address":[],"length":0,"stats":{"Line":0}},{"line":48,"address":[],"length":0,"stats":{"Line":0}},{"line":49,"address":[],"length":0,"stats":{"Line":0}},{"line":53,"address":[],"length":0,"stats":{"Line":0}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":61,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[],"length":0,"stats":{"Line":0}},{"line":65,"address":[],"length":0,"stats":{"Line":0}},{"line":69,"address":[],"length":0,"stats":{"Line":0}},{"line":70,"address":[],"length":0,"stats":{"Line":0}},{"line":71,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":73,"address":[],"length":0,"stats":{"Line":0}},{"line":85,"address":[],"length":0,"stats":{"Line":0}},{"line":86,"address":[],"length":0,"stats":{"Line":0}},{"line":87,"address":[],"length":0,"stats":{"Line":0}},{"line":88,"address":[],"length":0,"stats":{"Line":0}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":97,"address":[],"length":0,"stats":{"Line":0}},{"line":98,"address":[],"length":0,"stats":{"Line":0}},{"line":99,"address":[],"length":0,"stats":{"Line":0}},{"line":100,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":102,"address":[],"length":0,"stats":{"Line":0}},{"line":103,"address":[],"length":0,"stats":{"Line":0}},{"line":104,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":34},{"path":["/","Users","d4mr","work","thirdweb","engine-core","server","src","http","error.rs"],"content":"use axum::{Json, http::StatusCode, response::IntoResponse};\nuse engine_core::error::{ContractInteractionErrorKind, EngineError, RpcErrorKind};\nuse serde_json::json;\n\n// Extension trait that lets you pair an error with a status code\n/// Extension trait for EngineError to add HTTP response conversion\npub struct ApiEngineError(pub EngineError);\n\n// 2. Allow automatic conversion from EngineError\nimpl From\u003cEngineError\u003e for ApiEngineError {\n    fn from(error: EngineError) -\u003e Self {\n        ApiEngineError(error)\n    }\n}\n\nimpl IntoResponse for ApiEngineError {\n    fn into_response(self) -\u003e axum::response::Response {\n        let code = self.status_code();\n\n        self.with_status(code)\n    }\n}\n\nimpl ApiEngineError {\n    fn with_status(self, status: StatusCode) -\u003e axum::response::Response {\n        (\n            status,\n            Json(json!({\n                \"error\": {\n                    \"message\": self.0.to_string(),\n                    \"details\": self.0\n                }\n            })),\n        )\n            .into_response()\n    }\n\n    fn status_code(\u0026self) -\u003e StatusCode {\n        match \u0026self.0 {\n            EngineError::RpcError { kind, .. } =\u003e match kind {\n                RpcErrorKind::NullResp =\u003e StatusCode::BAD_GATEWAY,\n                RpcErrorKind::ErrorResp(_) =\u003e StatusCode::BAD_GATEWAY,\n                RpcErrorKind::UnsupportedFeature(_) =\u003e StatusCode::NOT_IMPLEMENTED,\n                RpcErrorKind::TransportHttpError { status, .. } =\u003e {\n                    StatusCode::from_u16(*status).unwrap_or(StatusCode::BAD_GATEWAY)\n                }\n                _ =\u003e StatusCode::SERVICE_UNAVAILABLE,\n            },\n            EngineError::RpcConfigError { .. } =\u003e StatusCode::INTERNAL_SERVER_ERROR,\n            EngineError::ContractInteractionError { kind, .. } =\u003e match kind {\n                ContractInteractionErrorKind::UnknownFunction(_)\n                | ContractInteractionErrorKind::UnknownSelector(_)\n                | ContractInteractionErrorKind::AbiError(_) =\u003e StatusCode::BAD_REQUEST,\n\n                ContractInteractionErrorKind::ZeroData { .. } =\u003e StatusCode::NOT_FOUND,\n\n                _ =\u003e StatusCode::INTERNAL_SERVER_ERROR,\n            },\n            EngineError::VaultError { .. } =\u003e StatusCode::INTERNAL_SERVER_ERROR,\n            EngineError::BundlerError { .. } =\u003e StatusCode::BAD_REQUEST,\n            EngineError::PaymasterError { .. } =\u003e StatusCode::BAD_REQUEST,\n        }\n    }\n}\n\n// 5. Result extension trait for more ergonomic usage\npub trait EngineResult\u003cT, E\u003e {\n    fn api_error(self) -\u003e Result\u003cT, ApiEngineError\u003e;\n}\n\nimpl\u003cT, E: Into\u003cEngineError\u003e\u003e EngineResult\u003cT, E\u003e for Result\u003cT, E\u003e {\n    fn api_error(self) -\u003e Result\u003cT, ApiEngineError\u003e {\n        self.map_err(|e| ApiEngineError(e.into()))\n    }\n}\n","traces":[{"line":11,"address":[],"length":0,"stats":{"Line":0}},{"line":12,"address":[],"length":0,"stats":{"Line":0}},{"line":17,"address":[],"length":0,"stats":{"Line":0}},{"line":18,"address":[],"length":0,"stats":{"Line":0}},{"line":20,"address":[],"length":0,"stats":{"Line":0}},{"line":25,"address":[],"length":0,"stats":{"Line":0}},{"line":27,"address":[],"length":0,"stats":{"Line":0}},{"line":28,"address":[],"length":0,"stats":{"Line":0}},{"line":29,"address":[],"length":0,"stats":{"Line":0}},{"line":30,"address":[],"length":0,"stats":{"Line":0}},{"line":31,"address":[],"length":0,"stats":{"Line":0}},{"line":38,"address":[],"length":0,"stats":{"Line":0}},{"line":39,"address":[],"length":0,"stats":{"Line":0}},{"line":40,"address":[],"length":0,"stats":{"Line":0}},{"line":41,"address":[],"length":0,"stats":{"Line":0}},{"line":42,"address":[],"length":0,"stats":{"Line":0}},{"line":43,"address":[],"length":0,"stats":{"Line":0}},{"line":44,"address":[],"length":0,"stats":{"Line":0}},{"line":45,"address":[],"length":0,"stats":{"Line":0}},{"line":47,"address":[],"length":0,"stats":{"Line":0}},{"line":49,"address":[],"length":0,"stats":{"Line":0}},{"line":50,"address":[],"length":0,"stats":{"Line":0}},{"line":53,"address":[],"length":0,"stats":{"Line":0}},{"line":55,"address":[],"length":0,"stats":{"Line":0}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":60,"address":[],"length":0,"stats":{"Line":0}},{"line":61,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":73,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":30},{"path":["/","Users","d4mr","work","thirdweb","engine-core","server","src","http","mod.rs"],"content":"pub mod error;\npub mod routes;\npub mod server;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","server","src","http","routes","create_userop.rs"],"content":"use alloy::{\n    hex::{self, FromHex},\n    primitives::{Address, Bytes, U256},\n};\nuse axum::{\n    extract::State,\n    http::StatusCode,\n    response::{IntoResponse, Json, Result},\n};\nuse engine_aa_core::{\n    account_factory::{AccountFactory, get_account_factory},\n    smart_account::{DeterminedSmartAccount, SmartAccount, SmartAccountFromSalt},\n    userop::builder::{UserOpBuilder, UserOpBuilderConfig},\n};\nuse engine_core::{\n    credentials::SigningCredential, execution_options::aa::Erc4337ExecutionOptions,\n    transaction::InnerTransaction, userop::UserOpVersion,\n};\nuse rand::Rng;\nuse serde::{Deserialize, Serialize};\nuse vault_types::enclave::auth::Auth;\n\nuse crate::{\n    chains::ChainService,\n    http::{error::EngineResult, server::EngineServerState},\n};\n\n// Request model for the test route\n#[derive(Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct CreateUserOpRequest {\n    // Chain identifier\n    chain_id: u64,\n\n    // ERC-4337 execution options\n    #[serde(flatten)]\n    execution_options: Erc4337ExecutionOptions,\n\n    // Inner transactions to execute\n    transactions: Vec\u003cInnerTransaction\u003e,\n\n    // Authentication for signing\n    #[serde(rename = \"auth\")]\n    credential: Auth,\n}\n\n// Response model\n#[derive(Serialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct CreateUserOpResponse {\n    user_op: UserOpVersion,\n    account_address: Address,\n}\n\n/// Test route that creates a signed UserOp from execution options and transactions\n#[axum::debug_handler]\npub async fn create_user_op(\n    State(state): State\u003cEngineServerState\u003e,\n    Json(request): Json\u003cCreateUserOpRequest\u003e,\n) -\u003e Result\u003cimpl IntoResponse\u003e {\n    // Get nonce for the account\n    let nonce = {\n        let mut rng = rand::rng();\n        let limb1: u64 = rng.random();\n        let limb2: u64 = rng.random();\n        let limb3: u64 = rng.random();\n        U256::from_limbs([0, limb1, limb2, limb3])\n    };\n\n    // Get the chain service for the specified chain ID\n    let chain = state.chains.get_chain(request.chain_id).map_err(|err| {\n        (\n            StatusCode::NOT_FOUND,\n            Json(serde_json::json!({ \"error\": format!(\"Chain not found: {}\", err) })),\n        )\n    })?;\n\n    // Encode transactions into call data using the appropriate encoder\n    let salt_data = if request.execution_options.account_salt.starts_with(\"0x\") {\n        Bytes::from_hex(request.execution_options.account_salt).map_err(|err| {\n            (\n                StatusCode::BAD_REQUEST,\n                Json(serde_json::json!({ \"error\": format!(\"Failed to parse account salt: {}\", err) })),\n            )\n        })?\n    } else {\n        let vec = hex::encode(request.execution_options.account_salt);\n        Bytes::from_hex(vec).expect(\"unreachable\")\n    };\n\n    // Determine if the account is deployed\n    let smart_account = match request.execution_options.smart_account_address {\n        Some(address) =\u003e DeterminedSmartAccount { address },\n        None =\u003e SmartAccountFromSalt {\n            admin_address: request.execution_options.signer_address,\n            chain: \u0026chain,\n            factory_address: request.execution_options.entrypoint_details.factory_address,\n            salt_data: \u0026salt_data,\n        }\n        .to_determined_smart_account()\n        .await\n        .api_error()?,\n    };\n    tracing::info!(\"Smarto account determined\");\n\n    let calldata = if request.transactions.len() == 1 {\n        smart_account.encode_execute(\u0026request.transactions[0])\n    } else {\n        smart_account.encode_execute_batch(\u0026request.transactions)\n    };\n\n    let is_deployed = smart_account.is_deployed(\u0026chain).await.api_error()?;\n\n    tracing::info!(\"Deployment status determined\");\n\n    let init_call_data = get_account_factory(\n        \u0026chain,\n        request.execution_options.entrypoint_details.factory_address,\n        None,\n    )\n    .init_calldata(request.execution_options.signer_address, salt_data);\n\n    // Convert Auth to SigningCredential\n    let signing_credential = SigningCredential::Vault(request.credential);\n\n    // Create UserOpBuilder configuration\n    let builder_config = UserOpBuilderConfig {\n        account_address: smart_account.address,\n        signer_address: request.execution_options.signer_address,\n        entrypoint_and_factory: request.execution_options.entrypoint_details,\n        call_data: calldata,\n        init_call_data,\n        is_deployed,\n        nonce,\n        credential: signing_credential,\n        chain: \u0026chain,\n        signer: state.signer.clone(),\n    };\n\n    // Build and sign the UserOp\n    let user_op = UserOpBuilder::new(builder_config)\n        .build()\n        .await\n        .api_error()?;\n\n    // Prepare the response\n    let response = CreateUserOpResponse {\n        user_op,\n        account_address: smart_account.address,\n    };\n\n    Ok((StatusCode::OK, Json(response)))\n}\n","traces":[{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[],"length":0,"stats":{"Line":0}},{"line":65,"address":[],"length":0,"stats":{"Line":0}},{"line":66,"address":[],"length":0,"stats":{"Line":0}},{"line":67,"address":[],"length":0,"stats":{"Line":0}},{"line":71,"address":[],"length":0,"stats":{"Line":0}},{"line":73,"address":[],"length":0,"stats":{"Line":0}},{"line":74,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":80,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[],"length":0,"stats":{"Line":0}},{"line":83,"address":[],"length":0,"stats":{"Line":0}},{"line":87,"address":[],"length":0,"stats":{"Line":0}},{"line":88,"address":[],"length":0,"stats":{"Line":0}},{"line":92,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":96,"address":[],"length":0,"stats":{"Line":0}},{"line":97,"address":[],"length":0,"stats":{"Line":0}},{"line":98,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":104,"address":[],"length":0,"stats":{"Line":0}},{"line":106,"address":[],"length":0,"stats":{"Line":0}},{"line":107,"address":[],"length":0,"stats":{"Line":0}},{"line":109,"address":[],"length":0,"stats":{"Line":0}},{"line":112,"address":[],"length":0,"stats":{"Line":0}},{"line":114,"address":[],"length":0,"stats":{"Line":0}},{"line":117,"address":[],"length":0,"stats":{"Line":0}},{"line":118,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":0}},{"line":121,"address":[],"length":0,"stats":{"Line":0}},{"line":124,"address":[],"length":0,"stats":{"Line":0}},{"line":128,"address":[],"length":0,"stats":{"Line":0}},{"line":129,"address":[],"length":0,"stats":{"Line":0}},{"line":130,"address":[],"length":0,"stats":{"Line":0}},{"line":136,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":141,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":152,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":42},{"path":["/","Users","d4mr","work","thirdweb","engine-core","server","src","http","routes","mod.rs"],"content":"pub mod create_userop;\npub mod smart_account;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","server","src","http","routes","smart_account.rs"],"content":"use alloy::primitives::{Address, Bytes};\nuse axum::{\n    extract::State,\n    http::StatusCode,\n    response::{IntoResponse, Json, Result},\n};\nuse engine_aa_core::smart_account::{SmartAccount, SmartAccountFromSalt};\nuse engine_core::constants::{DEFAULT_FACTORY_ADDRESS_V0_6, DEFAULT_FACTORY_ADDRESS_V0_7};\nuse serde::{Deserialize, Serialize};\n\nuse crate::{\n    chains::ChainService,\n    http::{error::EngineResult, server::EngineServerState},\n};\n\n// Request model\n#[derive(Deserialize)]\npub struct SmartAccountStatusRequest {\n    signer_address: Address,\n    chain_id: u64,\n}\n\n// Response models\n#[derive(Serialize)]\nstruct SmartAccountStatusResponse {\n    signer_address: Address,\n    chain_id: u64,\n    v6_account: AccountDetails,\n    v7_account: AccountDetails,\n}\n\n#[derive(Serialize)]\nstruct AccountDetails {\n    address: Address,\n    is_deployed: bool,\n}\n\npub async fn smart_account_status(\n    State(state): State\u003cEngineServerState\u003e,\n    Json(request): Json\u003cSmartAccountStatusRequest\u003e,\n) -\u003e Result\u003cimpl IntoResponse\u003e {\n    // Get the chain for the specified chain ID\n    let chain = state.chains.get_chain(request.chain_id).map_err(|err| {\n        (\n            StatusCode::NOT_FOUND,\n            Json(serde_json::json!({ \"error\": format!(\"Chain not found: {}\", err) })),\n        )\n    })?;\n\n    // Empty salt data for this example\n    let empty_salt = Bytes::default();\n\n    // Create the smart account wrappers\n    let v6_account = SmartAccountFromSalt {\n        admin_address: request.signer_address,\n        chain: \u0026chain,\n        factory_address: DEFAULT_FACTORY_ADDRESS_V0_6,\n        salt_data: \u0026empty_salt,\n    }\n    .to_determined_smart_account()\n    .await\n    .api_error()?;\n\n    let v7_account = SmartAccountFromSalt {\n        admin_address: request.signer_address,\n        chain: \u0026chain,\n        factory_address: DEFAULT_FACTORY_ADDRESS_V0_7,\n        salt_data: \u0026empty_salt,\n    }\n    .to_determined_smart_account()\n    .await\n    .api_error()?;\n\n    // Check deployment status\n    let v6_is_deployed = v6_account.is_deployed(\u0026chain).await.api_error()?;\n    let v7_is_deployed = v7_account.is_deployed(\u0026chain).await.api_error()?;\n\n    // Prepare the response\n    let response = SmartAccountStatusResponse {\n        signer_address: request.signer_address,\n        chain_id: request.chain_id,\n        v6_account: AccountDetails {\n            address: v6_account.address().clone(),\n            is_deployed: v6_is_deployed,\n        },\n        v7_account: AccountDetails {\n            address: v7_account.address().clone(),\n            is_deployed: v7_is_deployed,\n        },\n    };\n\n    Ok((StatusCode::OK, Json(response)))\n}\n","traces":[{"line":38,"address":[],"length":0,"stats":{"Line":0}},{"line":43,"address":[],"length":0,"stats":{"Line":0}},{"line":45,"address":[],"length":0,"stats":{"Line":0}},{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":51,"address":[],"length":0,"stats":{"Line":0}},{"line":55,"address":[],"length":0,"stats":{"Line":0}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":61,"address":[],"length":0,"stats":{"Line":0}},{"line":65,"address":[],"length":0,"stats":{"Line":0}},{"line":66,"address":[],"length":0,"stats":{"Line":0}},{"line":68,"address":[],"length":0,"stats":{"Line":0}},{"line":71,"address":[],"length":0,"stats":{"Line":0}},{"line":75,"address":[],"length":0,"stats":{"Line":0}},{"line":76,"address":[],"length":0,"stats":{"Line":0}},{"line":80,"address":[],"length":0,"stats":{"Line":0}},{"line":81,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[],"length":0,"stats":{"Line":0}},{"line":86,"address":[],"length":0,"stats":{"Line":0}},{"line":92,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":20},{"path":["/","Users","d4mr","work","thirdweb","engine-core","server","src","http","server.rs"],"content":"use std::sync::Arc;\n\nuse axum::{Router, routing::post};\nuse engine_core::userop::UserOpSigner;\nuse tokio::{sync::watch, task::JoinHandle};\n\nuse crate::chains::ThirdwebChainService;\nuse tower_http::{\n    cors::{Any, CorsLayer},\n    trace::TraceLayer,\n};\n\nuse super::routes::{create_userop::create_user_op, smart_account::smart_account_status};\n\n#[derive(Clone)]\npub struct EngineServerState {\n    pub chains: Arc\u003cThirdwebChainService\u003e,\n    pub signer: Arc\u003cUserOpSigner\u003e,\n}\n\npub struct EngineServer {\n    handle: Option\u003cJoinHandle\u003cResult\u003c(), std::io::Error\u003e\u003e\u003e,\n    shutdown_tx: Option\u003cwatch::Sender\u003cbool\u003e\u003e,\n    app: Router,\n}\n\nimpl EngineServer {\n    pub async fn new(state: EngineServerState) -\u003e Self {\n        let cors = CorsLayer::new()\n            .allow_origin(Any)\n            .allow_methods(Any)\n            .allow_headers(Any)\n            .allow_credentials(false);\n\n        let router = Router::new()\n            .route(\"/smart-account/status\", post(smart_account_status))\n            .route(\"/userop/create\", post(create_user_op))\n            .layer(cors)\n            .layer(TraceLayer::new_for_http())\n            // Add more routes here\n            .with_state(state);\n\n        Self {\n            handle: None,\n            shutdown_tx: None,\n            app: router,\n        }\n    }\n\n    pub fn start(\u0026mut self, listener: tokio::net::TcpListener) -\u003e Result\u003c(), std::io::Error\u003e {\n        // Create a shutdown channel\n        let (shutdown_tx, shutdown_rx) = watch::channel(false);\n        let app = self.app.clone();\n\n        // Start the HTTP server in a background task\n        let handle = tokio::spawn(async move {\n            tracing::info!(\"HTTP server starting on {}\", listener.local_addr().unwrap());\n\n            axum::serve(listener, app)\n                .with_graceful_shutdown(async move {\n                    let mut rx = shutdown_rx;\n                    while !*rx.borrow() {\n                        if rx.changed().await.is_err() {\n                            break;\n                        }\n                    }\n                    tracing::info!(\"HTTP server shutting down\");\n                })\n                .await\n        });\n\n        self.handle = Some(handle);\n        self.shutdown_tx = Some(shutdown_tx);\n\n        Ok(())\n    }\n\n    pub async fn shutdown(\u0026mut self) -\u003e Result\u003c(), std::io::Error\u003e {\n        if let Some(tx) = self.shutdown_tx.take() {\n            if tx.send(true).is_err() {\n                tracing::error!(\"Failed to send shutdown signal to HTTP server\");\n            }\n        }\n\n        if let Some(handle) = self.handle.take() {\n            match handle.await {\n                Ok(result) =\u003e {\n                    if let Err(e) = result {\n                        tracing::error!(\"HTTP server error during shutdown: {}\", e);\n                        return Err(e);\n                    }\n                }\n                Err(e) =\u003e {\n                    tracing::error!(\"Failed to join HTTP server task: {}\", e);\n                    return Err(std::io::Error::new(\n                        std::io::ErrorKind::Other,\n                        format!(\"Task join error: {}\", e),\n                    ));\n                }\n            }\n        }\n\n        Ok(())\n    }\n}\n","traces":[{"line":28,"address":[],"length":0,"stats":{"Line":0}},{"line":29,"address":[],"length":0,"stats":{"Line":0}},{"line":30,"address":[],"length":0,"stats":{"Line":0}},{"line":31,"address":[],"length":0,"stats":{"Line":0}},{"line":32,"address":[],"length":0,"stats":{"Line":0}},{"line":35,"address":[],"length":0,"stats":{"Line":0}},{"line":36,"address":[],"length":0,"stats":{"Line":0}},{"line":37,"address":[],"length":0,"stats":{"Line":0}},{"line":38,"address":[],"length":0,"stats":{"Line":0}},{"line":39,"address":[],"length":0,"stats":{"Line":0}},{"line":41,"address":[],"length":0,"stats":{"Line":0}},{"line":50,"address":[],"length":0,"stats":{"Line":0}},{"line":52,"address":[],"length":0,"stats":{"Line":0}},{"line":53,"address":[],"length":0,"stats":{"Line":0}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":60,"address":[],"length":0,"stats":{"Line":0}},{"line":61,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[],"length":0,"stats":{"Line":0}},{"line":67,"address":[],"length":0,"stats":{"Line":0}},{"line":69,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":73,"address":[],"length":0,"stats":{"Line":0}},{"line":75,"address":[],"length":0,"stats":{"Line":0}},{"line":78,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":80,"address":[],"length":0,"stats":{"Line":0}},{"line":81,"address":[],"length":0,"stats":{"Line":0}},{"line":85,"address":[],"length":0,"stats":{"Line":0}},{"line":86,"address":[],"length":0,"stats":{"Line":0}},{"line":87,"address":[],"length":0,"stats":{"Line":0}},{"line":88,"address":[],"length":0,"stats":{"Line":0}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":93,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":96,"address":[],"length":0,"stats":{"Line":0}},{"line":97,"address":[],"length":0,"stats":{"Line":0}},{"line":103,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":43},{"path":["/","Users","d4mr","work","thirdweb","engine-core","server","src","lib.rs"],"content":"pub mod chains;\npub mod config;\npub mod http;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","server","src","main.rs"],"content":"use std::sync::Arc;\n\nuse engine_core::userop::UserOpSigner;\nuse thirdweb_engine::{\n    chains::ThirdwebChainService,\n    config,\n    http::server::{EngineServer, EngineServerState},\n};\nuse tracing_subscriber::{filter::EnvFilter, layer::SubscriberExt, util::SubscriberInitExt};\n\n#[tokio::main]\nasync fn main() -\u003e anyhow::Result\u003c()\u003e {\n    tracing_subscriber::registry()\n        .with(EnvFilter::try_from_default_env().unwrap_or_else(|_| {\n            // Default to info level if RUST_LOG environment variable is not set\n            \"thirdweb_engine=debug,tower_http=debug,axum=debug\".into()\n        }))\n        .with(tracing_subscriber::fmt::layer())\n        .init();\n\n    let config = config::get_config();\n\n    let vault_client = vault_sdk::VaultClient::builder(config.thirdweb.urls.vault)\n        .build()\n        .await?;\n\n    tracing::info!(\"Vault client initialized\");\n\n    let chains = Arc::new(ThirdwebChainService {\n        secret_key: config.thirdweb.secret.clone(),\n        client_id: config.thirdweb.client_id.clone(),\n        bundler_base_url: config.thirdweb.urls.bundler,\n        paymaster_base_url: config.thirdweb.urls.paymaster,\n        rpc_base_url: config.thirdweb.urls.rpc,\n    });\n\n    let signer = UserOpSigner { vault_client };\n\n    let mut server = EngineServer::new(EngineServerState {\n        signer: Arc::new(signer),\n        chains,\n    })\n    .await;\n\n    let address = format!(\"{}:{}\", config.server.host, config.server.port);\n    let listener = tokio::net::TcpListener::bind(\u0026address).await?;\n\n    server.start(listener)?;\n\n    tracing::info!(\"Servers started, waiting for shutdown signal\");\n    if let Err(e) = tokio::signal::ctrl_c().await {\n        tracing::warn!(\"Failed to listen for Ctrl+C: {}\", e);\n    }\n    tracing::info!(\"Shutdown signal received\");\n\n    // Orchestrate shutdown with minimal wrapping since internal methods handle their own instrumentation\n    tracing::info!(\"Starting coordinated shutdown\");\n\n    if let Err(e) = server.shutdown().await {\n        tracing::error!(\"Error during coordinated shutdown: {}\", e);\n    } else {\n        tracing::info!(\"All servers shut down successfully\");\n    }\n\n    Ok(())\n}\n","traces":[{"line":12,"address":[],"length":0,"stats":{"Line":0}},{"line":13,"address":[],"length":0,"stats":{"Line":0}},{"line":14,"address":[],"length":0,"stats":{"Line":0}},{"line":16,"address":[],"length":0,"stats":{"Line":0}},{"line":18,"address":[],"length":0,"stats":{"Line":0}},{"line":21,"address":[],"length":0,"stats":{"Line":0}},{"line":23,"address":[],"length":0,"stats":{"Line":0}},{"line":25,"address":[],"length":0,"stats":{"Line":0}},{"line":27,"address":[],"length":0,"stats":{"Line":0}},{"line":29,"address":[],"length":0,"stats":{"Line":0}},{"line":30,"address":[],"length":0,"stats":{"Line":0}},{"line":31,"address":[],"length":0,"stats":{"Line":0}},{"line":32,"address":[],"length":0,"stats":{"Line":0}},{"line":33,"address":[],"length":0,"stats":{"Line":0}},{"line":34,"address":[],"length":0,"stats":{"Line":0}},{"line":37,"address":[],"length":0,"stats":{"Line":0}},{"line":39,"address":[],"length":0,"stats":{"Line":0}},{"line":40,"address":[],"length":0,"stats":{"Line":0}},{"line":41,"address":[],"length":0,"stats":{"Line":0}},{"line":43,"address":[],"length":0,"stats":{"Line":0}},{"line":45,"address":[],"length":0,"stats":{"Line":0}},{"line":46,"address":[],"length":0,"stats":{"Line":0}},{"line":48,"address":[],"length":0,"stats":{"Line":0}},{"line":50,"address":[],"length":0,"stats":{"Line":0}},{"line":51,"address":[],"length":0,"stats":{"Line":0}},{"line":52,"address":[],"length":0,"stats":{"Line":0}},{"line":54,"address":[],"length":0,"stats":{"Line":0}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":60,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":65,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":32},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","benches","throughput.rs"],"content":"use criterion::{BenchmarkId, Criterion, Throughput, criterion_group, criterion_main};\nuse rand::Rng;\nuse serde::{Deserialize, Serialize};\nuse std::hint::black_box;\nuse std::sync::{\n    Arc,\n    atomic::{AtomicU64, Ordering},\n};\nuse std::time::{Duration, SystemTime, UNIX_EPOCH};\nuse tokio::runtime::Runtime;\n\nuse twmq::{\n    DurableExecution, NackHookData, Queue, SuccessHookData,\n    hooks::TransactionContext,\n    job::{Job, JobResult, JobStatus, RequeuePosition},\n    queue::QueueOptions,\n};\n\nconst REDIS_URL: \u0026str = \"redis://127.0.0.1:6379/\";\n\n// Benchmark job that either succeeds immediately or nacks based on probability\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct BenchmarkJob {\n    pub job_id: String,\n    pub nack_probability: f64, // For metrics\n    pub created_at: u64,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct BenchmarkOutput {\n    pub job_id: String,\n    pub processed_at: u64,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct BenchmarkErrorData {\n    pub job_id: String,\n    pub attempt: u32,\n}\n\n// Shared metrics across all benchmark jobs\n#[derive(Clone)]\npub struct BenchmarkMetrics {\n    pub jobs_processed: Arc\u003cAtomicU64\u003e,\n    pub jobs_nacked: Arc\u003cAtomicU64\u003e,\n    pub jobs_succeeded: Arc\u003cAtomicU64\u003e,\n    pub total_processing_time_ms: Arc\u003cAtomicU64\u003e,\n}\n\nimpl BenchmarkMetrics {\n    fn new() -\u003e Self {\n        Self {\n            jobs_processed: Arc::new(AtomicU64::new(0)),\n            jobs_nacked: Arc::new(AtomicU64::new(0)),\n            jobs_succeeded: Arc::new(AtomicU64::new(0)),\n            total_processing_time_ms: Arc::new(AtomicU64::new(0)),\n        }\n    }\n\n    fn reset(\u0026self) {\n        self.jobs_processed.store(0, Ordering::SeqCst);\n        self.jobs_nacked.store(0, Ordering::SeqCst);\n        self.jobs_succeeded.store(0, Ordering::SeqCst);\n        self.total_processing_time_ms.store(0, Ordering::SeqCst);\n    }\n\n    fn processed_count(\u0026self) -\u003e u64 {\n        self.jobs_processed.load(Ordering::SeqCst)\n    }\n\n    fn success_rate(\u0026self) -\u003e f64 {\n        let succeeded = self.jobs_succeeded.load(Ordering::SeqCst) as f64;\n        let total = self.jobs_processed.load(Ordering::SeqCst) as f64;\n        if total \u003e 0.0 { succeeded / total } else { 0.0 }\n    }\n\n    fn avg_processing_time_ms(\u0026self) -\u003e f64 {\n        let total_time = self.total_processing_time_ms.load(Ordering::SeqCst) as f64;\n        let total_jobs = self.jobs_processed.load(Ordering::SeqCst) as f64;\n        if total_jobs \u003e 0.0 {\n            total_time / total_jobs\n        } else {\n            0.0\n        }\n    }\n}\n\nimpl DurableExecution for BenchmarkJob {\n    type Output = BenchmarkOutput;\n    type ErrorData = BenchmarkErrorData;\n    type ExecutionContext = BenchmarkMetrics;\n\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        metrics: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        let start_time = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_millis() as u64;\n\n        // Simulate minimal work (just increment counter)\n        metrics.jobs_processed.fetch_add(1, Ordering::SeqCst);\n\n        let end_time = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_millis() as u64;\n        let processing_time = end_time - start_time;\n        metrics\n            .total_processing_time_ms\n            .fetch_add(processing_time, Ordering::SeqCst);\n\n        // Fresh random decision each processing attempt\n        if rand::thread_rng().gen_bool(job.data.nack_probability) {\n            metrics.jobs_nacked.fetch_add(1, Ordering::SeqCst);\n\n            // Random position for nacks as requested\n            let position = if rand::thread_rng().gen_bool(0.5) {\n                RequeuePosition::First\n            } else {\n                RequeuePosition::Last\n            };\n\n            JobResult::Nack {\n                error: BenchmarkErrorData {\n                    job_id: job.data.job_id.clone(),\n                    attempt: job.attempts,\n                },\n                delay: None, // No delay as requested\n                position,\n            }\n        } else {\n            metrics.jobs_succeeded.fetch_add(1, Ordering::SeqCst);\n\n            JobResult::Success(BenchmarkOutput {\n                job_id: job.data.job_id.clone(),\n                processed_at: end_time,\n            })\n        }\n    }\n\n    async fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        _d: SuccessHookData\u003c'_, Self::Output\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _metrics: \u0026Self::ExecutionContext,\n    ) {\n        // Keep hooks minimal for max performance\n    }\n\n    async fn on_nack(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        _d: NackHookData\u003c'_, Self::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _metrics: \u0026Self::ExecutionContext,\n    ) {\n        // Keep hooks minimal for max performance\n    }\n\n    async fn on_timeout(\u0026self, _tx: \u0026mut TransactionContext\u003c'_\u003e) {\n        // Minimal timeout handling\n    }\n}\n\n// Load test that finds maximum sustainable throughput\nasync fn load_test_throughput(\n    jobs_per_second: usize,\n    duration_seconds: u64,\n    nack_percentage: f64,\n) -\u003e (u64, f64, f64, bool) {\n    let test_id = nanoid::nanoid!(8);\n    let queue_name = format!(\"bench_queue_{}\", test_id);\n\n    let metrics = BenchmarkMetrics::new();\n\n    // Optimize queue for high throughput\n    let queue_options = QueueOptions {\n        local_concurrency: 200,                      // High concurrency\n        polling_interval: Duration::from_millis(10), // Fast polling\n        always_poll: true,                           // Always poll for max responsiveness\n        lease_duration: Duration::from_secs(30),     // Reasonable lease time\n        max_success: 10000,                          // Large success queue\n        max_failed: 1000,                            // Reasonable failed queue\n    };\n\n    let queue = Arc::new(\n        Queue::\u003cBenchmarkJob, BenchmarkOutput, BenchmarkErrorData, BenchmarkMetrics\u003e::new(\n            REDIS_URL,\n            \u0026queue_name,\n            Some(queue_options),\n            metrics.clone(),\n        )\n        .await\n        .expect(\"Failed to create benchmark queue\"),\n    );\n\n    // Clean up any existing data\n    let mut redis_conn = queue.redis.clone();\n    let keys: Vec\u003cString\u003e = redis::cmd(\"KEYS\")\n        .arg(format!(\"{}:*\", queue_name))\n        .query_async(\u0026mut redis_conn)\n        .await\n        .unwrap_or_default();\n    if !keys.is_empty() {\n        let _: () = redis::cmd(\"DEL\")\n            .arg(keys)\n            .query_async(\u0026mut redis_conn)\n            .await\n            .unwrap_or(());\n    }\n\n    // Start workers\n    let worker_handle = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            let _ = queue.work().await;\n        })\n    };\n\n    // Job producer task\n    let producer_handle = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            let mut job_counter = 0u64;\n            let interval_duration = Duration::from_secs_f64(1.0 / jobs_per_second as f64);\n            let mut interval = tokio::time::interval(interval_duration);\n            interval.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Delay);\n\n            let start_time = SystemTime::now();\n\n            loop {\n                if start_time.elapsed().unwrap().as_secs() \u003e= duration_seconds {\n                    break;\n                }\n\n                interval.tick().await;\n\n                let job = BenchmarkJob {\n                    job_id: format!(\"job_{}\", job_counter),\n                    nack_probability: nack_percentage,\n                    created_at: SystemTime::now()\n                        .duration_since(UNIX_EPOCH)\n                        .unwrap()\n                        .as_secs(),\n                };\n\n                if queue\n                    .clone()\n                    .job(job)\n                    .with_id(\u0026format!(\"job_{}\", job_counter))\n                    .push()\n                    .await\n                    .is_ok()\n                {\n                    job_counter += 1;\n                }\n            }\n\n            job_counter\n        })\n    };\n\n    // Monitor queue depth\n    let mut max_queue_depth = 0usize;\n    let monitor_handle = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            let start_time = SystemTime::now();\n            while start_time.elapsed().unwrap().as_secs() \u003c duration_seconds + 5 {\n                let pending = queue.count(JobStatus::Pending).await.unwrap_or(0);\n                let active = queue.count(JobStatus::Active).await.unwrap_or(0);\n                let depth = pending + active;\n                max_queue_depth = max_queue_depth.max(depth);\n                tokio::time::sleep(Duration::from_millis(100)).await;\n            }\n            max_queue_depth\n        })\n    };\n\n    // Wait for test completion\n    let jobs_pushed = producer_handle.await.unwrap();\n\n    // Give some extra time for jobs to finish processing\n    tokio::time::sleep(Duration::from_secs(2)).await;\n\n    let max_depth = monitor_handle.await.unwrap();\n\n    // Check if queue is sustainable (not building up)\n    let final_pending = queue.count(JobStatus::Pending).await.unwrap_or(0);\n    let final_active = queue.count(JobStatus::Active).await.unwrap_or(0);\n    let is_sustainable = (final_pending + final_active) \u003c jobs_per_second / 2; // Heuristic: backlog \u003c 0.5 seconds of work\n\n    let total_processed = metrics.processed_count();\n    let success_rate = metrics.success_rate();\n    let avg_processing_time = metrics.avg_processing_time_ms();\n\n    println!(\n        \"Load Test Results - {}jobs/s for {}s:\",\n        jobs_per_second, duration_seconds\n    );\n    println!(\"  Jobs pushed: {}\", jobs_pushed);\n    println!(\"  Jobs processed: {}\", total_processed);\n    println!(\"  Success rate: {:.1}%\", success_rate * 100.0);\n    println!(\"  Avg processing time: {:.2}ms\", avg_processing_time);\n    println!(\"  Max queue depth: {}\", max_depth);\n    println!(\"  Final backlog: {}\", final_pending + final_active);\n    println!(\"  Sustainable: {}\", is_sustainable);\n\n    // Cleanup\n    worker_handle.abort();\n    let _: () = redis::cmd(\"DEL\")\n        .arg(format!(\"{}:*\", queue_name))\n        .query_async(\u0026mut redis_conn)\n        .await\n        .unwrap_or(());\n\n    (\n        total_processed,\n        success_rate,\n        avg_processing_time,\n        is_sustainable,\n    )\n}\n\n// Benchmark that finds maximum sustainable throughput\nfn find_max_throughput(c: \u0026mut Criterion) {\n    let rt = Runtime::new().unwrap();\n\n    let mut group = c.benchmark_group(\"twmq_max_throughput\");\n    group.measurement_time(Duration::from_secs(30));\n    group.sample_size(10);\n\n    // Test different throughput levels to find the limit\n    let throughput_levels = vec![50_000, 100_000, 120_000, 150_000, 200_000, 250_000];\n\n    for \u0026jobs_per_second in \u0026throughput_levels {\n        group.throughput(Throughput::Elements(jobs_per_second as u64));\n\n        group.bench_with_input(\n            BenchmarkId::new(\"sustainable_throughput\", jobs_per_second),\n            \u0026jobs_per_second,\n            |b, \u0026jobs_per_second| {\n                b.to_async(\u0026rt).iter(|| async {\n                    let (processed, success_rate, avg_time, sustainable) =\n                        load_test_throughput(jobs_per_second, 10, 0.1).await;\n\n                    // Return metrics for Criterion\n                    black_box((processed, success_rate, avg_time, sustainable))\n                });\n            },\n        );\n    }\n\n    group.finish();\n}\n\n// Benchmark different nack percentages\nfn nack_percentage_impact(c: \u0026mut Criterion) {\n    let rt = Runtime::new().unwrap();\n\n    let mut group = c.benchmark_group(\"twmq_nack_impact\");\n    group.measurement_time(Duration::from_secs(20));\n\n    let nack_percentages = vec![0.0, 0.05, 0.1, 0.2, 0.3, 0.5];\n\n    for \u0026nack_pct in \u0026nack_percentages {\n        group.bench_with_input(\n            BenchmarkId::new(\"nack_percentage\", (nack_pct * 100.0) as u32),\n            \u0026nack_pct,\n            |b, \u0026nack_pct| {\n                b.to_async(\u0026rt).iter(|| async move {\n                    let (processed, success_rate, avg_time, sustainable) =\n                        load_test_throughput(5000, 8, nack_pct).await;\n\n                    black_box((processed, success_rate, avg_time, sustainable))\n                });\n            },\n        );\n    }\n\n    group.finish();\n}\n\n// Quick smoke test benchmark\nfn basic_throughput(c: \u0026mut Criterion) {\n    let rt = Runtime::new().unwrap();\n\n    let mut group = c.benchmark_group(\"twmq_basic_1k_jobs\");\n    group.sample_size(10); // ← Only 5 samples instead of 100\n\n    group.bench_function(\"1k_jobs\", |b| {\n        b.to_async(\u0026rt).iter(|| async {\n            let (processed, success_rate, avg_time, sustainable) =\n                load_test_throughput(1000, 5, 0.1).await;\n\n            black_box((processed, success_rate, avg_time, sustainable))\n        });\n    });\n\n    group.finish();\n}\n\ncriterion_group!(\n    benches,\n    basic_throughput,\n    find_max_throughput,\n    nack_percentage_impact\n);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","src","error.rs"],"content":"#[derive(thiserror::Error, Debug)]\npub enum TwmqError {\n    #[error(\"Redis error: {0}\")]\n    RedisError(#[from] redis::RedisError),\n\n    #[error(\"JSON Serialization error: {0}\")]\n    JsonError(#[from] serde_json::Error),\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","src","hooks.rs"],"content":"use std::time::{SystemTime, UNIX_EPOCH};\n\nuse serde::{Serialize, de::DeserializeOwned};\n\nuse crate::{DurableExecution, error::TwmqError, job::JobBuilder};\n\n// A minimal transaction context that hooks can use\npub struct TransactionContext\u003c'a\u003e {\n    // Redis pipeline for arbitrary commands\n    pipeline: \u0026'a mut redis::Pipeline,\n    // Queue name for context\n    queue_name: String,\n}\n\nimpl\u003c'a\u003e TransactionContext\u003c'a\u003e {\n    pub fn new(pipeline: \u0026'a mut redis::Pipeline, queue_name: String) -\u003e Self {\n        Self {\n            pipeline,\n            queue_name,\n        }\n    }\n\n    // Return pipeline for arbitrary commands\n    pub fn pipeline(\u0026mut self) -\u003e \u0026mut redis::Pipeline {\n        self.pipeline\n    }\n\n    // Queue name accessor\n    pub fn queue_name(\u0026self) -\u003e \u0026str {\n        \u0026self.queue_name\n    }\n\n    // Helper for job scheduling (maintains type safety)\n    pub fn queue_job\u003cT, R, E, C\u003e(\n        \u0026mut self,\n        job_builder: JobBuilder\u003cT, R, E, C\u003e,\n    ) -\u003e Result\u003c\u0026mut Self, TwmqError\u003e\n    where\n        T: Serialize\n            + DeserializeOwned\n            + Send\n            + Sync\n            + DurableExecution\u003cOutput = R, ErrorData = E, ExecutionContext = C\u003e\n            + 'static,\n        R: Serialize + DeserializeOwned + Send + Sync + 'static,\n        E: Serialize + DeserializeOwned + Send + Sync + 'static,\n        C: Send + Clone + Sync + 'static,\n    {\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n        let job_data = serde_json::to_string(\u0026job_builder.options.data)?;\n\n        // Add job to the target queue (could be different from current queue!)\n        self.pipeline\n            .hset(\n                job_builder.queue.job_data_hash_name(),\n                \u0026job_builder.options.id,\n                job_data,\n            )\n            .hset(\n                job_builder\n                    .queue\n                    .job_meta_hash_name(\u0026job_builder.options.id),\n                \"created_at\",\n                now,\n            )\n            .hset(\n                job_builder\n                    .queue\n                    .job_meta_hash_name(\u0026job_builder.options.id),\n                \"attempts\",\n                0,\n            )\n            .sadd(job_builder.queue.dedupe_set_name(), \u0026job_builder.options.id);\n\n        if let Some(delay_options) = job_builder.options.delay {\n            let process_at = now + delay_options.delay.as_secs();\n            self.pipeline\n                .hset(\n                    job_builder\n                        .queue\n                        .job_meta_hash_name(\u0026job_builder.options.id),\n                    \"reentry_position\",\n                    delay_options.position.to_string(),\n                )\n                .zadd(\n                    job_builder.queue.delayed_zset_name(),\n                    \u0026job_builder.options.id,\n                    process_at,\n                );\n        } else {\n            self.pipeline.rpush(\n                job_builder.queue.pending_list_name(),\n                \u0026job_builder.options.id,\n            );\n        }\n\n        Ok(self)\n    }\n}\n","traces":[{"line":16,"address":[],"length":0,"stats":{"Line":29}},{"line":24,"address":[],"length":0,"stats":{"Line":37}},{"line":25,"address":[],"length":0,"stats":{"Line":37}},{"line":29,"address":[],"length":0,"stats":{"Line":0}},{"line":30,"address":[],"length":0,"stats":{"Line":0}},{"line":34,"address":[],"length":0,"stats":{"Line":1}},{"line":49,"address":[],"length":0,"stats":{"Line":1}},{"line":50,"address":[],"length":0,"stats":{"Line":1}},{"line":53,"address":[],"length":0,"stats":{"Line":2}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":60,"address":[],"length":0,"stats":{"Line":0}},{"line":63,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[],"length":0,"stats":{"Line":0}},{"line":65,"address":[],"length":0,"stats":{"Line":0}},{"line":67,"address":[],"length":0,"stats":{"Line":0}},{"line":70,"address":[],"length":0,"stats":{"Line":0}},{"line":71,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":76,"address":[],"length":0,"stats":{"Line":0}},{"line":78,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":80,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[],"length":0,"stats":{"Line":0}},{"line":83,"address":[],"length":0,"stats":{"Line":0}},{"line":84,"address":[],"length":0,"stats":{"Line":0}},{"line":86,"address":[],"length":0,"stats":{"Line":0}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":0}},{"line":91,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[],"length":0,"stats":{"Line":1}},{"line":95,"address":[],"length":0,"stats":{"Line":1}},{"line":96,"address":[],"length":0,"stats":{"Line":1}},{"line":100,"address":[],"length":0,"stats":{"Line":0}}],"covered":10,"coverable":35},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","src","job.rs"],"content":"use crate::{DurableExecution, Queue, error::TwmqError};\nuse nanoid::nanoid;\nuse serde::{Deserialize, Serialize, de::DeserializeOwned};\nuse std::{fmt::Display, sync::Arc, time::Duration};\n\n// Position for nack operations\n#[derive(Debug, Clone, Copy, Serialize, Deserialize)]\npub enum RequeuePosition {\n    #[serde(rename = \"first\")]\n    First,\n    #[serde(rename = \"last\")]\n    Last,\n}\n\nimpl Display for RequeuePosition {\n    fn fmt(\u0026self, f: \u0026mut std::fmt::Formatter\u003c'_\u003e) -\u003e std::fmt::Result {\n        match self {\n            RequeuePosition::First =\u003e write!(f, \"first\"),\n            RequeuePosition::Last =\u003e write!(f, \"last\"),\n        }\n    }\n}\n\nimpl TryFrom\u003c\u0026str\u003e for RequeuePosition {\n    type Error = ();\n\n    fn try_from(value: \u0026str) -\u003e Result\u003cSelf, Self::Error\u003e {\n        match value {\n            \"first\" =\u003e Ok(RequeuePosition::First),\n            \"last\" =\u003e Ok(RequeuePosition::Last),\n            _ =\u003e Err(()),\n        }\n    }\n}\n\n#[derive(Debug, Clone, Copy, Serialize, Deserialize)]\npub struct DelayOptions {\n    pub delay: Duration,\n    pub position: RequeuePosition,\n}\n\n// Job result type\npub enum JobResult\u003cT, E\u003e {\n    Success(T),\n    Nack {\n        error: E,\n        delay: Option\u003cDuration\u003e,\n        position: RequeuePosition,\n    },\n    Fail(E),\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RequeueOptions {\n    pub delay: Option\u003cDuration\u003e,\n    pub position: RequeuePosition,\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum JobErrorType {\n    #[serde(rename = \"nack\")]\n    Nack(RequeueOptions),\n    #[serde(rename = \"fail\")]\n    Fail,\n}\n\nimpl JobErrorType {\n    pub fn nack(delay: Option\u003cDuration\u003e, position: RequeuePosition) -\u003e Self {\n        Self::Nack(RequeueOptions { delay, position })\n    }\n\n    pub fn fail() -\u003e Self {\n        Self::Fail\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct JobErrorRecord\u003cE\u003e {\n    pub error: E,\n    pub attempt: u32,\n    pub details: JobErrorType,\n    pub created_at: u64,\n}\n\n// Job structure\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct Job\u003cT\u003e {\n    pub id: String,\n    pub data: T,\n    pub attempts: u32,\n    pub created_at: u64,\n    pub processed_at: Option\u003cu64\u003e,\n    pub finished_at: Option\u003cu64\u003e,\n}\n\n// Job status enum\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum JobStatus {\n    Pending,\n    Active,\n    Delayed,\n    Success,\n    Failed,\n}\n\npub struct JobOptions\u003cT\u003e\nwhere\n    T: Serialize + DeserializeOwned + Send + Sync + 'static,\n{\n    pub data: T,\n    pub id: String,\n    pub delay: Option\u003cDelayOptions\u003e,\n}\n\nimpl\u003cT\u003e JobOptions\u003cT\u003e\nwhere\n    T: Serialize + DeserializeOwned + Send + Sync + 'static,\n{\n    pub fn new(data: T) -\u003e Self {\n        Self {\n            data,\n            id: nanoid!(),\n            delay: None,\n        }\n    }\n\n    // Set custom ID (for deduplication)\n    pub fn with_id(mut self, id: impl Into\u003cString\u003e) -\u003e Self {\n        self.id = id.into();\n        self\n    }\n\n    // Set delay\n    pub fn with_delay(mut self, delay: DelayOptions) -\u003e Self {\n        self.delay = Some(delay);\n        self\n    }\n}\n\npub struct JobBuilder\u003cT, R, E, C\u003e\nwhere\n    T: Serialize\n        + DeserializeOwned\n        + Send\n        + Sync\n        + 'static\n        + DurableExecution\u003cExecutionContext = C, Output = R, ErrorData = E\u003e,\n    R: Serialize + DeserializeOwned + Send + Sync + 'static,\n    E: Serialize + DeserializeOwned + Send + Sync + 'static,\n    C: Send + Sync + 'static,\n{\n    pub options: JobOptions\u003cT\u003e,\n    pub queue: Arc\u003cQueue\u003cT, R, E, C\u003e\u003e,\n}\n\nimpl\u003cT, R, E, C\u003e JobBuilder\u003cT, R, E, C\u003e\nwhere\n    T: Serialize\n        + DeserializeOwned\n        + DurableExecution\u003cOutput = R, ErrorData = E, ExecutionContext = C\u003e\n        + Send\n        + Sync\n        + 'static,\n    R: Serialize + DeserializeOwned + Send + Sync + 'static,\n    E: Serialize + DeserializeOwned + Send + Sync + 'static,\n    C: Send + Sync + Clone + 'static,\n{\n    pub async fn push(self) -\u003e Result\u003cJob\u003cT\u003e, TwmqError\u003e {\n        self.queue.push(self.options).await\n    }\n\n    pub fn with_id(mut self, id: impl Into\u003cString\u003e) -\u003e Self {\n        self.options.id = id.into();\n        self\n    }\n\n    pub fn with_delay(mut self, delay: DelayOptions) -\u003e Self {\n        self.options.delay = Some(delay);\n        self\n    }\n}\n","traces":[{"line":16,"address":[],"length":0,"stats":{"Line":16}},{"line":17,"address":[],"length":0,"stats":{"Line":16}},{"line":18,"address":[],"length":0,"stats":{"Line":1}},{"line":19,"address":[],"length":0,"stats":{"Line":15}},{"line":27,"address":[],"length":0,"stats":{"Line":0}},{"line":28,"address":[],"length":0,"stats":{"Line":0}},{"line":29,"address":[],"length":0,"stats":{"Line":0}},{"line":30,"address":[],"length":0,"stats":{"Line":0}},{"line":31,"address":[],"length":0,"stats":{"Line":0}},{"line":68,"address":[],"length":0,"stats":{"Line":16}},{"line":69,"address":[],"length":0,"stats":{"Line":16}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":73,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":16}},{"line":122,"address":[],"length":0,"stats":{"Line":16}},{"line":128,"address":[],"length":0,"stats":{"Line":0}},{"line":129,"address":[],"length":0,"stats":{"Line":0}},{"line":130,"address":[],"length":0,"stats":{"Line":0}},{"line":134,"address":[],"length":0,"stats":{"Line":0}},{"line":135,"address":[],"length":0,"stats":{"Line":0}},{"line":136,"address":[],"length":0,"stats":{"Line":0}},{"line":168,"address":[],"length":0,"stats":{"Line":30}},{"line":169,"address":[],"length":0,"stats":{"Line":15}},{"line":172,"address":[],"length":0,"stats":{"Line":15}},{"line":173,"address":[],"length":0,"stats":{"Line":15}},{"line":174,"address":[],"length":0,"stats":{"Line":15}},{"line":177,"address":[],"length":0,"stats":{"Line":3}},{"line":178,"address":[],"length":0,"stats":{"Line":3}},{"line":179,"address":[],"length":0,"stats":{"Line":3}}],"covered":16,"coverable":29},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","src","lib.rs"],"content":"pub mod error;\npub mod hooks;\npub mod job;\npub mod queue;\n\nuse std::marker::PhantomData;\nuse std::sync::Arc;\nuse std::time::{Duration, SystemTime, UNIX_EPOCH};\n\nuse error::TwmqError;\nuse hooks::TransactionContext;\nuse job::{\n    DelayOptions, Job, JobBuilder, JobErrorRecord, JobErrorType, JobOptions, JobResult, JobStatus,\n    RequeuePosition,\n};\nuse queue::QueueOptions;\nuse redis::Pipeline;\nuse redis::{AsyncCommands, RedisResult, aio::ConnectionManager};\nuse serde::{Serialize, de::DeserializeOwned};\nuse tokio::sync::Semaphore;\nuse tokio::time::sleep;\n\npub use redis;\nuse tracing::Instrument;\n\npub struct SuccessHookData\u003c'a, O\u003e {\n    pub result: \u0026'a O,\n}\n\npub struct NackHookData\u003c'a, E\u003e {\n    pub error: \u0026'a E,\n    pub delay: Option\u003cDuration\u003e,\n    pub position: RequeuePosition,\n}\n\npub struct FailHookData\u003c'a, E\u003e {\n    pub error: \u0026'a E,\n}\n\n// Main DurableExecution trait\npub trait DurableExecution: Sized {\n    type Output: Serialize + DeserializeOwned + Send + Sync;\n    type ErrorData: Serialize + DeserializeOwned + Send + Sync;\n    type ExecutionContext: Send + Sync + 'static;\n\n    // Required method to process a job\n    fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        c: \u0026Self::ExecutionContext,\n    ) -\u003e impl Future\u003cOutput = JobResult\u003cSelf::Output, Self::ErrorData\u003e\u003e + Send + Sync;\n\n    fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        _d: SuccessHookData\u003cSelf::Output\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _c: \u0026Self::ExecutionContext,\n    ) -\u003e impl Future\u003cOutput = ()\u003e + Send + Sync {\n        std::future::ready(())\n    }\n\n    fn on_nack(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        _d: NackHookData\u003cSelf::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _c: \u0026Self::ExecutionContext,\n    ) -\u003e impl Future\u003cOutput = ()\u003e + Send + Sync {\n        std::future::ready(())\n    }\n\n    fn on_fail(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        _d: FailHookData\u003cSelf::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _c: \u0026Self::ExecutionContext,\n    ) -\u003e impl Future\u003cOutput = ()\u003e + Send + Sync {\n        std::future::ready(())\n    }\n\n    fn on_timeout(\n        \u0026self,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n    ) -\u003e impl Future\u003cOutput = ()\u003e + Send + Sync {\n        std::future::ready(())\n    }\n}\n\n// Main Queue struct\npub struct Queue\u003cT, R, E, C\u003e\nwhere\n    T: Serialize\n        + DeserializeOwned\n        + Send\n        + Sync\n        + 'static\n        + DurableExecution\u003cExecutionContext = C, Output = R, ErrorData = E\u003e,\n    R: Serialize + DeserializeOwned + Send + Sync + 'static,\n    E: Serialize + DeserializeOwned + Send + Sync + 'static,\n    C: Send + Sync + 'static,\n{\n    pub redis: ConnectionManager,\n    options: QueueOptions,\n    // concurrency: usize,\n    name: String,\n    execution_context: C,\n    _phantom: PhantomData\u003c(T, R, E)\u003e,\n}\n\nimpl\u003cT, R, E, C\u003e Queue\u003cT, R, E, C\u003e\nwhere\n    C: Send + Sync + Clone + 'static,\n    T: Serialize\n        + DeserializeOwned\n        + Send\n        + Sync\n        + DurableExecution\u003cOutput = R, ErrorData = E, ExecutionContext = C\u003e\n        + 'static,\n    R: Serialize + DeserializeOwned + Send + Sync + 'static,\n    E: Serialize + DeserializeOwned + Send + Sync + 'static,\n{\n    pub async fn new(\n        redis_url: \u0026str,\n        name: \u0026str,\n        // concurrency: usize,\n        options: Option\u003cQueueOptions\u003e,\n        context: C,\n    ) -\u003e Result\u003cSelf, TwmqError\u003e {\n        let client = redis::Client::open(redis_url)?;\n        let redis = client.get_connection_manager().await?;\n\n        let queue = Self {\n            redis,\n            name: name.to_string(),\n            // concurrency,\n            options: options.unwrap_or_default(),\n            execution_context: context,\n            _phantom: PhantomData,\n        };\n\n        Ok(queue)\n    }\n\n    pub fn job(self: Arc\u003cSelf\u003e, data: T) -\u003e JobBuilder\u003cT, R, E, C\u003e {\n        JobBuilder {\n            options: JobOptions::new(data),\n            queue: self,\n        }\n    }\n\n    // Get queue name\n    pub fn name(\u0026self) -\u003e \u0026str {\n        \u0026self.name\n    }\n\n    pub fn pending_list_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:pending\", self.name())\n    }\n\n    pub fn active_hash_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:active\", self.name)\n    }\n\n    pub fn delayed_zset_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:delayed\", self.name)\n    }\n\n    pub fn success_list_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:success\", self.name)\n    }\n\n    pub fn failed_list_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:failed\", self.name)\n    }\n\n    pub fn job_data_hash_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:jobs:data\", self.name)\n    }\n\n    pub fn job_meta_hash_name(\u0026self, job_id: \u0026str) -\u003e String {\n        format!(\"twmq:{}:job:{}:meta\", self.name, job_id)\n    }\n\n    pub fn job_errors_list_name(\u0026self, job_id: \u0026str) -\u003e String {\n        format!(\"twmq:{}:job:{}:errors\", self.name, job_id)\n    }\n\n    pub fn job_result_hash_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:jobs:result\", self.name)\n    }\n\n    pub fn dedupe_set_name(\u0026self) -\u003e String {\n        format!(\"twmq:{}:dedup\", self.name)\n    }\n\n    pub async fn push(\u0026self, job_options: JobOptions\u003cT\u003e) -\u003e Result\u003cJob\u003cT\u003e, TwmqError\u003e {\n        // Check for duplicates and handle job creation with deduplication\n        let script = redis::Script::new(\n            r#\"\n            local job_id = ARGV[1]\n            local job_data = ARGV[2]\n            local now = ARGV[3]\n            local delay = ARGV[4]\n            local reentry_position = ARGV[5]  -- \"first\" or \"last\"\n\n            local queue_id = KEYS[1]\n            local delayed_zset_name = KEYS[2]\n            local pending_list_name = KEYS[3]\n\n            local job_data_hash_name = KEYS[4]\n            local job_meta_hash_name = KEYS[5]\n\n            local dedupe_set_name = KEYS[6]\n\n            -- Check if job already exists in any queue\n            if redis.call('SISMEMBER', dedupe_set_name, job_id) == 1 then\n                -- Job with this ID already exists, skip\n                return { 0, job_id }\n            end\n\n            -- Store job data\n            redis.call('HSET', job_data_hash_name, job_id, job_data)\n\n            -- Store job metadata as a hash\n            redis.call('HSET', job_meta_hash_name, 'created_at', now)\n            redis.call('HSET', job_meta_hash_name, 'attempts', 0)\n\n            -- Add to deduplication set\n            redis.call('SADD', dedupe_set_name, job_id)\n\n            -- Add to appropriate queue based on delay\n            if tonumber(delay) \u003e 0 then\n                local process_at = now + tonumber(delay)\n                -- Store position information for this delayed job\n                redis.call('HSET', job_meta_hash_name, 'reentry_position', reentry_position)\n                redis.call('ZADD', delayed_zset_name, process_at, job_id)\n            else\n                -- Non-delayed job always goes to end of pending\n                redis.call('RPUSH', pending_list_name, job_id)\n            end\n\n            return { 1, job_id }\n            \"#,\n        );\n\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        let job = Job {\n            id: job_options.id.clone(),\n            data: job_options.data,\n            attempts: 0,\n            created_at: now,\n            processed_at: None,\n            finished_at: None,\n        };\n\n        let job_data = serde_json::to_string(\u0026job.data)?;\n\n        let delay = job_options.delay.unwrap_or(DelayOptions {\n            delay: Duration::ZERO,\n            position: RequeuePosition::Last,\n        });\n\n        let delay_secs = delay.delay.as_secs();\n        let position_string = delay.position.to_string();\n\n        let _result: (i32, String) = script\n            .key(\u0026self.name)\n            .key(self.delayed_zset_name())\n            .key(self.pending_list_name())\n            .key(self.job_data_hash_name())\n            .key(self.job_meta_hash_name(\u0026job.id))\n            .key(self.dedupe_set_name())\n            .arg(job_options.id)\n            .arg(job_data)\n            .arg(now)\n            .arg(delay_secs)\n            .arg(position_string)\n            .invoke_async(\u0026mut self.redis.clone())\n            .await?;\n\n        // Return job_id whether new or existing\n        Ok(job)\n    }\n\n    pub async fn get_job(\u0026self, job_id: \u0026str) -\u003e Result\u003cOption\u003cJob\u003cT\u003e\u003e, TwmqError\u003e {\n        let mut conn = self.redis.clone();\n        let job_data_t_json: Option\u003cString\u003e = conn.hget(self.job_data_hash_name(), job_id).await?;\n\n        if let Some(data_json) = job_data_t_json {\n            let data_t: T = serde_json::from_str(\u0026data_json)?;\n\n            // Fetch metadata\n            let meta_map: std::collections::HashMap\u003cString, String\u003e =\n                conn.hgetall(self.job_meta_hash_name(job_id)).await?;\n\n            let attempts: u32 = meta_map\n                .get(\"attempts\")\n                .and_then(|s| s.parse().ok())\n                .unwrap_or(0);\n            let created_at: u64 = meta_map\n                .get(\"created_at\")\n                .and_then(|s| s.parse().ok())\n                .unwrap_or(0); // Consider a more robust default or error\n            let processed_at: Option\u003cu64\u003e =\n                meta_map.get(\"processed_at\").and_then(|s| s.parse().ok());\n            let finished_at: Option\u003cu64\u003e = meta_map.get(\"finished_at\").and_then(|s| s.parse().ok());\n            // reentry_position is also in meta if needed for display\n\n            Ok(Some(Job {\n                id: job_id.to_string(),\n                data: data_t,\n                attempts,\n                created_at,\n                processed_at,\n                finished_at,\n            }))\n        } else {\n            Ok(None)\n        }\n    }\n\n    pub async fn count(\u0026self, status: JobStatus) -\u003e Result\u003cusize, TwmqError\u003e {\n        let mut conn = self.redis.clone();\n\n        let count = match status {\n            JobStatus::Pending =\u003e {\n                let count: usize = conn.llen(self.pending_list_name()).await?;\n                count\n            }\n            JobStatus::Active =\u003e {\n                let count: usize = conn.hlen(self.active_hash_name()).await?;\n                count\n            }\n            JobStatus::Delayed =\u003e {\n                let count: usize = conn.zcard(self.delayed_zset_name()).await?;\n                count\n            }\n            JobStatus::Success =\u003e {\n                let count: usize = conn.llen(self.success_list_name()).await?;\n                count\n            }\n            JobStatus::Failed =\u003e {\n                let count: usize = conn.llen(self.failed_list_name()).await?;\n                count\n            }\n        };\n\n        Ok(count)\n    }\n\n    pub async fn work(self: Arc\u003cSelf\u003e) -\u003e Result\u003c(), TwmqError\u003e {\n        // Local semaphore to limit concurrency per instance\n        let semaphore = Arc::new(Semaphore::new(self.options.local_concurrency));\n\n        // Start worker\n        tokio::spawn(async move {\n            let mut interval = tokio::time::interval(self.options.polling_interval);\n\n            loop {\n                interval.tick().await;\n\n                // Check available permits for batch size\n                let available_permits = semaphore.available_permits();\n                if available_permits == 0 \u0026\u0026 !self.options.always_poll {\n                    tracing::debug!(\"No permits available, waiting...\");\n                    continue;\n                }\n\n                tracing::debug!(\"Available permits: {}\", available_permits);\n                // Try to get multiple jobs - as many as we have permits\n                match self.pop_batch_jobs(available_permits).await {\n                    Ok(jobs) =\u003e {\n                        tracing::debug!(\"Got {} jobs\", jobs.len());\n                        for job in jobs {\n                            let permit = semaphore.clone().acquire_owned().await.unwrap();\n                            let queue_clone = self.clone();\n                            let job_id = job.id.clone();\n                            let execution_context = self.execution_context.clone();\n\n                            tokio::spawn(async move {\n                                // Process job - note we don't pass a context here\n                                let result = DurableExecution::process(\u0026job, \u0026execution_context).await;\n\n                                // Mark job as complete (automatically happens in completion handlers)\n\n                                match result {\n                                    JobResult::Success(output) =\u003e {\n                                        // Create transaction pipeline for atomicity\n                                        let mut pipeline = redis::pipe();\n                                        pipeline.atomic(); // Use MULTI/EXEC\n\n                                        // Create transaction context with mutable access to pipeline\n                                        let mut tx_context = TransactionContext::new(\n                                            \u0026mut pipeline,\n                                            queue_clone.name().to_string(),\n                                        );\n\n                                        let success_hook_data = SuccessHookData {\n                                            result: \u0026output,\n                                        };\n\n                                        // Call success hook to populate transaction context\n                                        job.data.on_success(\u0026job, success_hook_data, \u0026mut tx_context, \u0026execution_context).await;\n\n                                        // Complete job with success and execute transaction\n                                        if let Err(e) = queue_clone\n                                            .complete_job_success(\n                                                \u0026job,\n                                                \u0026output,\n                                                tx_context.pipeline(),\n                                            )\n                                            .await\n                                        {\n                                            tracing::error!(\n                                                \"Failed to complete job {} complete handling successfully: {:?}\",\n                                                job.id,\n                                                e\n                                            );\n                                        }\n                                    }\n                                    JobResult::Nack {\n                                        error,\n                                        delay,\n                                        position,\n                                    } =\u003e {\n                                        // Create transaction pipeline for atomicity\n                                        let mut pipeline = redis::pipe();\n                                        pipeline.atomic(); // Use MULTI/EXEC\n\n                                        // Create transaction context with mutable access to pipeline\n                                        let mut tx_context = TransactionContext::new(\n                                            \u0026mut pipeline,\n                                            queue_clone.name().to_string(),\n                                        );\n\n                                        let nack_hook_data = NackHookData {\n                                            error: \u0026error,\n                                            delay,\n                                            position,\n                                        };\n\n                                        // Call nack hook to populate transaction context\n                                        job.data\n                                            .on_nack(\u0026job, nack_hook_data, \u0026mut tx_context,\u0026execution_context)\n                                            .await;\n\n                                        // Complete job with nack and execute transaction\n                                        if let Err(e) = queue_clone\n                                            .complete_job_nack(\n                                                \u0026job,\n                                                \u0026error,\n                                                delay,\n                                                position,\n                                                tx_context.pipeline(),\n                                            )\n                                            .await\n                                        {\n                                            tracing::error!(\n                                                \"Failed to complete job {} complete nack handling successfully: {:?}\",\n                                                job.id,\n                                                e\n                                            );\n                                        }\n                                    }\n                                    JobResult::Fail(error) =\u003e {\n                                        // Create transaction pipeline for atomicity\n                                        let mut pipeline = redis::pipe();\n                                        pipeline.atomic(); // Use MULTI/EXEC\n\n                                        // Create transaction context with mutable access to pipeline\n                                        let mut tx_context = TransactionContext::new(\n                                            \u0026mut pipeline,\n                                            queue_clone.name.clone(),\n                                        );\n\n                                        let fail_hook_data = FailHookData {\n                                            error: \u0026error\n                                        };\n\n                                        // Call fail hook to populate transaction context\n                                        job.data.on_fail(\u0026job, fail_hook_data, \u0026mut tx_context, \u0026execution_context).await;\n\n                                        // Complete job with fail and execute transaction\n                                        if let Err(e) = queue_clone\n                                            .complete_job_fail(\u0026job, \u0026error, tx_context.pipeline())\n                                            .await\n                                        {\n                                            tracing::error!(\n                                                \"Failed to complete job {} complete fail handling successfully: {:?}\",\n                                                job.id,\n                                                e\n                                            );\n                                        }\n                                    }\n                                }\n\n                                // Release permit when done\n                                drop(permit);\n                            }.instrument(tracing::info_span!(\"twmq_worker\", job_id)));\n                        }\n                    }\n                    Err(e) =\u003e {\n                        // No jobs found, wait a bit\n                        tracing::error!(\"Failed to pop batch jobs: {:?}\", e);\n                        sleep(Duration::from_millis(1000)).await;\n                    }\n                };\n            }\n        });\n\n        Ok(())\n    }\n\n    // Improved batch job popping - gets multiple jobs at once\n    async fn pop_batch_jobs(\u0026self, batch_size: usize) -\u003e RedisResult\u003cVec\u003cJob\u003cT\u003e\u003e\u003e {\n        // Lua script that does:\n        // 1. Process expired delayed jobs\n        // 2. Check for timed out active jobs\n        // 3. Pop up to batch_size jobs from pending\n        let script = redis::Script::new(\n            r#\"\n            local now = tonumber(ARGV[1])\n            local batch_size = tonumber(ARGV[2])\n            local lease_seconds = tonumber(ARGV[3])\n\n            local queue_id = KEYS[1]\n            local delayed_zset_name = KEYS[2]\n            local pending_list_name = KEYS[3]\n            local active_hash_name = KEYS[4]\n            local job_data_hash_name = KEYS[5]\n\n\n            local result_jobs = {}\n\n            local timed_out_jobs = {}\n\n            -- Step 1: Clean up all expired leases\n            -- Get all active jobs\n            local active_jobs = redis.call('HGETALL', active_hash_name)\n\n            -- Process in pairs (job_id, lease_expiry)\n            for i = 1, #active_jobs, 2 do\n                local job_id = active_jobs[i]\n                local lease_expiry = tonumber(active_jobs[i + 1])\n                local job_meta_hash_name = 'twmq:' .. queue_id .. ':job:' .. job_id .. ':meta'\n\n                -- Check if lease has expired\n                if lease_expiry \u003c now then\n                    redis.call('HINCRBY', job_meta_hash_name, 'attempts', 1)\n\n                    -- Move job back to pending\n                    redis.call('HDEL', active_hash_name, job_id)\n                    redis.call('LPUSH', pending_list_name, job_id)\n\n                    -- Add to list of timed out jobs\n                    table.insert(timed_out_jobs, job_id)\n                end\n            end\n\n            -- Step 2: Move expired delayed jobs to pending\n            local delayed_jobs = redis.call('ZRANGEBYSCORE', delayed_zset_name, 0, now)\n            for i, job_id in ipairs(delayed_jobs) do\n                -- Check position information\n\n                local job_meta_hash_name = 'twmq:' .. queue_id .. ':job:' .. job_id .. ':meta'\n                local reentry_position = redis.call('HGET', job_meta_hash_name, 'reentry_position') or 'last'\n\n                -- Remove from delayed\n                redis.call('ZREM', delayed_zset_name, job_id)\n                redis.call('HDEL', job_meta_hash_name, 'reentry_position')\n\n                -- Add to pending based on position\n                if reentry_position == 'first' then\n                    redis.call('LPUSH', pending_list_name, job_id)\n                else\n                    redis.call('RPUSH', pending_list_name, job_id)\n                end\n            end\n\n            -- Finally Step 3: Try to pop jobs from pending (up to batch_size)\n            -- Try to pop jobs from pending (up to batch_size)\n            local popped_job_ids = {}\n            for i = 1, batch_size do\n                local job_id = redis.call('LPOP', pending_list_name)\n                if not job_id then\n                    break\n                end\n\n                table.insert(popped_job_ids, job_id)\n            end\n\n            local result_jobs = {}\n\n            -- Process popped jobs\n            for _, job_id in ipairs(popped_job_ids) do\n                -- Get job data\n                local job_data = redis.call('HGET', job_data_hash_name, job_id)\n\n                -- Only process if we have data\n                if job_data then\n                    -- Update metadata\n                    local job_meta_hash_name = 'twmq:' .. queue_id .. ':job:' .. job_id .. ':meta'\n\n\n                    redis.call('HSET', job_meta_hash_name, 'processed_at', now)\n                    local created_at = redis.call('HGET', job_meta_hash_name, 'created_at') or now\n                    local attempts = redis.call('HINCRBY', job_meta_hash_name, 'attempts', 1)\n\n                    -- Set lease expiration\n                    local lease_expiry = now + lease_seconds\n\n                    -- Add to active set with lease expiry as score\n                    redis.call('HSET', active_hash_name, job_id, lease_expiry)\n\n                    -- Add to result with both id and data\n                    table.insert(result_jobs, {job_id, job_data, tostring(attempts), tostring(created_at), tostring(now)})\n                end\n            end\n\n            return result_jobs\n            \"#,\n        );\n\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        let results_from_lua: Vec\u003c(String, String, String, String, String)\u003e = script\n            .key(self.name())\n            .key(self.delayed_zset_name())\n            .key(self.pending_list_name())\n            .key(self.active_hash_name())\n            .key(self.job_data_hash_name())\n            .arg(now)\n            .arg(batch_size)\n            .arg(self.options.lease_duration.as_secs())\n            .invoke_async(\u0026mut self.redis.clone())\n            .await?;\n\n        let mut jobs = Vec::new();\n        for (job_id_str, job_data_t_json, attempts_str, created_at_str, processed_at_str) in\n            results_from_lua\n        {\n            match serde_json::from_str::\u003cT\u003e(\u0026job_data_t_json) {\n                Ok(data_t) =\u003e {\n                    let attempts: u32 = attempts_str.parse().unwrap_or(1); // Default or handle error\n                    let created_at: u64 = created_at_str.parse().unwrap_or(now); // Default or handle error\n                    let processed_at: u64 = processed_at_str.parse().unwrap_or(now); // Default or handle error\n\n                    jobs.push(Job {\n                        id: job_id_str,\n                        data: data_t,\n                        attempts,\n                        created_at,\n                        processed_at: Some(processed_at),\n                        finished_at: None, // Not finished yet\n                    });\n                }\n                Err(e) =\u003e {\n                    // Log error: failed to deserialize job data T for job_id_str\n                    tracing::error!(\n                        \"Failed to deserialize job data for job {}: {}\",\n                        job_id_str,\n                        e\n                    );\n                    // TODO: Potentially move this job_id to a failed state or log for investigation\n                }\n            }\n        }\n\n        Ok(jobs)\n    }\n\n    async fn complete_job_success(\n        \u0026self,\n        job: \u0026Job\u003cT\u003e,\n        result: \u0026R,\n        pipeline: \u0026mut Pipeline,\n    ) -\u003e Result\u003c(), TwmqError\u003e {\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        // Add basic job completion operations to pipeline\n        pipeline\n            .hdel(self.active_hash_name(), \u0026job.id)\n            .lpush(self.success_list_name(), \u0026job.id)\n            // Set finished_at in the job's metadata hash\n            .hset(self.job_meta_hash_name(\u0026job.id), \"finished_at\", now);\n\n        let result_json = serde_json::to_string(result)?;\n        pipeline.hset(self.job_result_hash_name(), \u0026job.id, result_json);\n\n        // Execute main pipeline first\n        pipeline.query_async::\u003c()\u003e(\u0026mut self.redis.clone()).await?;\n\n        // Separate call for pruning with data deletion using Lua\n        let trim_script = redis::Script::new(\n            r#\"\n                local queue_id = KEYS[1]\n                local list_name = KEYS[2]\n                local job_data_hash = KEYS[3]\n                local results_hash = KEYS[4] -- e.g., \"myqueue:results\"\n                local max_len = tonumber(ARGV[1])\n\n                local job_ids_to_delete = redis.call('LRANGE', list_name, max_len, -1)\n\n                if #job_ids_to_delete \u003e 0 then\n                    for _, j_id in ipairs(job_ids_to_delete) do\n                        local job_meta_hash = 'twmq:' .. queue_id .. ':job:' .. j_id .. ':meta'\n                        local errors_list_name = 'twmq:' .. queue_id .. ':job:' .. j_id .. ':errors'\n\n                        redis.call('HDEL', job_data_hash, j_id)\n                        redis.call('DEL', job_meta_hash)\n                        redis.call('HDEL', results_hash, j_id)\n                        redis.call('DEL', errors_list_name)\n                    end\n                    redis.call('LTRIM', list_name, 0, max_len - 1)\n                end\n                return #job_ids_to_delete\n            \"#,\n        );\n\n        let _trimmed_count: usize = trim_script\n            .key(self.name())\n            .key(self.success_list_name())\n            .key(self.job_data_hash_name())\n            .key(self.job_result_hash_name()) // results_hash\n            .arg(self.options.max_success) // max_len (LTRIM is 0 to max_success-1)\n            .invoke_async(\u0026mut self.redis.clone())\n            .await?;\n\n        Ok(())\n    }\n\n    async fn complete_job_nack(\n        \u0026self,\n        job: \u0026Job\u003cT\u003e,\n        error: \u0026E,\n        delay: Option\u003cDuration\u003e,\n        position: RequeuePosition,\n        pipeline: \u0026mut Pipeline,\n    ) -\u003e Result\u003c(), TwmqError\u003e {\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        // Remove from active\n        pipeline.hdel(self.active_hash_name(), \u0026job.id);\n\n        let error_record = JobErrorRecord {\n            attempt: job.attempts,\n            error,\n            details: JobErrorType::nack(delay, position),\n            created_at: now,\n        };\n\n        let error_json = serde_json::to_string(\u0026error_record)?;\n\n        pipeline.lpush(self.job_errors_list_name(\u0026job.id), error_json);\n\n        // Add to proper queue based on delay and position\n        if let Some(delay_duration) = delay {\n            let delay_until = now + delay_duration.as_secs();\n\n            // Store position for when delay expires\n            let pos_str = position.to_string();\n\n            pipeline\n                .hset(\n                    self.job_meta_hash_name(\u0026job.id),\n                    \"reentry_position\",\n                    pos_str,\n                )\n                .zadd(self.delayed_zset_name(), \u0026job.id, delay_until);\n        } else {\n            match position {\n                RequeuePosition::First =\u003e {\n                    pipeline.lpush(self.pending_list_name(), \u0026job.id);\n                }\n                RequeuePosition::Last =\u003e {\n                    pipeline.rpush(self.pending_list_name(), \u0026job.id);\n                }\n            }\n        }\n\n        // Execute pipeline\n        pipeline.query_async::\u003c()\u003e(\u0026mut self.redis.clone()).await?;\n\n        Ok(())\n    }\n\n    async fn complete_job_fail(\n        \u0026self,\n        job: \u0026Job\u003cT\u003e,\n        error: \u0026E,\n        pipeline: \u0026mut Pipeline,\n    ) -\u003e Result\u003c(), TwmqError\u003e {\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        // Remove from active, add to failed\n        pipeline\n            .hdel(self.active_hash_name(), \u0026job.id)\n            .lpush(self.failed_list_name(), \u0026job.id)\n            // Set finished_at in the job's metadata hash\n            .hset(self.job_meta_hash_name(\u0026job.id), \"finished_at\", now);\n\n        // Store error\n        let error_record = JobErrorRecord {\n            attempt: job.attempts,\n            error,\n            details: JobErrorType::fail(),\n            created_at: now,\n        };\n        let error_json = serde_json::to_string(\u0026error_record)?;\n\n        pipeline.lpush(self.job_errors_list_name(\u0026job.id), error_json);\n        pipeline.query_async::\u003c()\u003e(\u0026mut self.redis.clone()).await?;\n\n        // Separate call for pruning with data deletion using Lua\n        let trim_script = redis::Script::new(\n            r#\"\n                local queue_id = KEYS[1]\n                local list_name = KEYS[2]\n                local job_data_hash = KEYS[3]\n                local max_len = tonumber(ARGV[1])\n\n\n                local job_ids_to_delete = redis.call('LRANGE', list_name, max_len, -1)\n\n                if #job_ids_to_delete \u003e 0 then\n                    for _, j_id in ipairs(job_ids_to_delete) do\n                        local errors_list_name = 'twmq:' .. queue_id .. ':job:' .. j_id .. ':errors'\n                        local job_meta_hash = 'twmq:' .. queue_id .. ':job:' .. j_id .. ':meta'\n\n                        redis.call('HDEL', job_data_hash, j_id)\n                        redis.call('DEL', job_meta_hash)\n                        redis.call('DEL', errors_list_name)\n                    end\n                    redis.call('LTRIM', list_name, 0, max_len - 1)\n                end\n                return #job_ids_to_delete\n            \"#,\n        );\n\n        let _trimmed_count: usize = trim_script\n            .key(self.name())\n            .key(self.failed_list_name())\n            .key(self.job_data_hash_name())\n            .arg(self.options.max_failed) // max_len (LTRIM is 0 to max_failed-1)\n            .invoke_async(\u0026mut self.redis.clone())\n            .await?;\n\n        Ok(())\n    }\n}\n","traces":[{"line":52,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":62,"address":[],"length":0,"stats":{"Line":0}},{"line":69,"address":[],"length":0,"stats":{"Line":0}},{"line":72,"address":[],"length":0,"stats":{"Line":0}},{"line":79,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[],"length":0,"stats":{"Line":0}},{"line":86,"address":[],"length":0,"stats":{"Line":0}},{"line":123,"address":[],"length":0,"stats":{"Line":13}},{"line":130,"address":[],"length":0,"stats":{"Line":26}},{"line":131,"address":[],"length":0,"stats":{"Line":13}},{"line":135,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":145,"address":[],"length":0,"stats":{"Line":16}},{"line":147,"address":[],"length":0,"stats":{"Line":16}},{"line":153,"address":[],"length":0,"stats":{"Line":613}},{"line":154,"address":[],"length":0,"stats":{"Line":613}},{"line":157,"address":[],"length":0,"stats":{"Line":308}},{"line":158,"address":[],"length":0,"stats":{"Line":308}},{"line":161,"address":[],"length":0,"stats":{"Line":300}},{"line":162,"address":[],"length":0,"stats":{"Line":300}},{"line":165,"address":[],"length":0,"stats":{"Line":283}},{"line":166,"address":[],"length":0,"stats":{"Line":283}},{"line":169,"address":[],"length":0,"stats":{"Line":47}},{"line":170,"address":[],"length":0,"stats":{"Line":47}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[],"length":0,"stats":{"Line":0}},{"line":177,"address":[],"length":0,"stats":{"Line":300}},{"line":178,"address":[],"length":0,"stats":{"Line":300}},{"line":181,"address":[],"length":0,"stats":{"Line":38}},{"line":182,"address":[],"length":0,"stats":{"Line":38}},{"line":185,"address":[],"length":0,"stats":{"Line":16}},{"line":186,"address":[],"length":0,"stats":{"Line":16}},{"line":189,"address":[],"length":0,"stats":{"Line":29}},{"line":190,"address":[],"length":0,"stats":{"Line":29}},{"line":193,"address":[],"length":0,"stats":{"Line":17}},{"line":194,"address":[],"length":0,"stats":{"Line":17}},{"line":197,"address":[],"length":0,"stats":{"Line":32}},{"line":247,"address":[],"length":0,"stats":{"Line":16}},{"line":248,"address":[],"length":0,"stats":{"Line":16}},{"line":253,"address":[],"length":0,"stats":{"Line":16}},{"line":254,"address":[],"length":0,"stats":{"Line":16}},{"line":261,"address":[],"length":0,"stats":{"Line":32}},{"line":263,"address":[],"length":0,"stats":{"Line":0}},{"line":264,"address":[],"length":0,"stats":{"Line":0}},{"line":265,"address":[],"length":0,"stats":{"Line":0}},{"line":268,"address":[],"length":0,"stats":{"Line":0}},{"line":269,"address":[],"length":0,"stats":{"Line":0}},{"line":271,"address":[],"length":0,"stats":{"Line":16}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":273,"address":[],"length":0,"stats":{"Line":0}},{"line":274,"address":[],"length":0,"stats":{"Line":0}},{"line":275,"address":[],"length":0,"stats":{"Line":0}},{"line":276,"address":[],"length":0,"stats":{"Line":0}},{"line":277,"address":[],"length":0,"stats":{"Line":0}},{"line":278,"address":[],"length":0,"stats":{"Line":0}},{"line":279,"address":[],"length":0,"stats":{"Line":0}},{"line":280,"address":[],"length":0,"stats":{"Line":0}},{"line":281,"address":[],"length":0,"stats":{"Line":0}},{"line":282,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":284,"address":[],"length":0,"stats":{"Line":0}},{"line":287,"address":[],"length":0,"stats":{"Line":0}},{"line":290,"address":[],"length":0,"stats":{"Line":14}},{"line":291,"address":[],"length":0,"stats":{"Line":7}},{"line":292,"address":[],"length":0,"stats":{"Line":14}},{"line":294,"address":[],"length":0,"stats":{"Line":7}},{"line":295,"address":[],"length":0,"stats":{"Line":7}},{"line":298,"address":[],"length":0,"stats":{"Line":7}},{"line":299,"address":[],"length":0,"stats":{"Line":0}},{"line":301,"address":[],"length":0,"stats":{"Line":0}},{"line":303,"address":[],"length":0,"stats":{"Line":7}},{"line":305,"address":[],"length":0,"stats":{"Line":0}},{"line":307,"address":[],"length":0,"stats":{"Line":7}},{"line":309,"address":[],"length":0,"stats":{"Line":0}},{"line":310,"address":[],"length":0,"stats":{"Line":7}},{"line":311,"address":[],"length":0,"stats":{"Line":6}},{"line":314,"address":[],"length":0,"stats":{"Line":0}},{"line":315,"address":[],"length":0,"stats":{"Line":0}},{"line":316,"address":[],"length":0,"stats":{"Line":0}},{"line":317,"address":[],"length":0,"stats":{"Line":0}},{"line":318,"address":[],"length":0,"stats":{"Line":0}},{"line":319,"address":[],"length":0,"stats":{"Line":0}},{"line":320,"address":[],"length":0,"stats":{"Line":0}},{"line":323,"address":[],"length":0,"stats":{"Line":0}},{"line":327,"address":[],"length":0,"stats":{"Line":90}},{"line":328,"address":[],"length":0,"stats":{"Line":45}},{"line":330,"address":[],"length":0,"stats":{"Line":90}},{"line":331,"address":[],"length":0,"stats":{"Line":0}},{"line":332,"address":[],"length":0,"stats":{"Line":24}},{"line":333,"address":[],"length":0,"stats":{"Line":0}},{"line":335,"address":[],"length":0,"stats":{"Line":0}},{"line":336,"address":[],"length":0,"stats":{"Line":16}},{"line":337,"address":[],"length":0,"stats":{"Line":0}},{"line":339,"address":[],"length":0,"stats":{"Line":0}},{"line":340,"address":[],"length":0,"stats":{"Line":8}},{"line":341,"address":[],"length":0,"stats":{"Line":0}},{"line":343,"address":[],"length":0,"stats":{"Line":0}},{"line":344,"address":[],"length":0,"stats":{"Line":42}},{"line":345,"address":[],"length":0,"stats":{"Line":0}},{"line":347,"address":[],"length":0,"stats":{"Line":0}},{"line":348,"address":[],"length":0,"stats":{"Line":0}},{"line":349,"address":[],"length":0,"stats":{"Line":0}},{"line":353,"address":[],"length":0,"stats":{"Line":0}},{"line":356,"address":[],"length":0,"stats":{"Line":26}},{"line":358,"address":[],"length":0,"stats":{"Line":13}},{"line":361,"address":[],"length":0,"stats":{"Line":26}},{"line":362,"address":[],"length":0,"stats":{"Line":13}},{"line":364,"address":[],"length":0,"stats":{"Line":0}},{"line":365,"address":[],"length":0,"stats":{"Line":276}},{"line":368,"address":[],"length":0,"stats":{"Line":263}},{"line":369,"address":[],"length":0,"stats":{"Line":351}},{"line":370,"address":[],"length":0,"stats":{"Line":0}},{"line":371,"address":[],"length":0,"stats":{"Line":0}},{"line":374,"address":[],"length":0,"stats":{"Line":263}},{"line":376,"address":[],"length":0,"stats":{"Line":0}},{"line":377,"address":[],"length":0,"stats":{"Line":263}},{"line":378,"address":[],"length":0,"stats":{"Line":263}},{"line":379,"address":[],"length":0,"stats":{"Line":329}},{"line":380,"address":[],"length":0,"stats":{"Line":33}},{"line":381,"address":[],"length":0,"stats":{"Line":0}},{"line":382,"address":[],"length":0,"stats":{"Line":0}},{"line":383,"address":[],"length":0,"stats":{"Line":0}},{"line":385,"address":[],"length":0,"stats":{"Line":33}},{"line":387,"address":[],"length":0,"stats":{"Line":62}},{"line":391,"address":[],"length":0,"stats":{"Line":29}},{"line":392,"address":[],"length":0,"stats":{"Line":13}},{"line":394,"address":[],"length":0,"stats":{"Line":13}},{"line":395,"address":[],"length":0,"stats":{"Line":13}},{"line":398,"address":[],"length":0,"stats":{"Line":13}},{"line":399,"address":[],"length":0,"stats":{"Line":13}},{"line":400,"address":[],"length":0,"stats":{"Line":13}},{"line":403,"address":[],"length":0,"stats":{"Line":13}},{"line":404,"address":[],"length":0,"stats":{"Line":13}},{"line":408,"address":[],"length":0,"stats":{"Line":13}},{"line":411,"address":[],"length":0,"stats":{"Line":13}},{"line":412,"address":[],"length":0,"stats":{"Line":13}},{"line":413,"address":[],"length":0,"stats":{"Line":13}},{"line":414,"address":[],"length":0,"stats":{"Line":13}},{"line":415,"address":[],"length":0,"stats":{"Line":13}},{"line":417,"address":[],"length":0,"stats":{"Line":13}},{"line":419,"address":[],"length":0,"stats":{"Line":0}},{"line":420,"address":[],"length":0,"stats":{"Line":0}},{"line":421,"address":[],"length":0,"stats":{"Line":0}},{"line":422,"address":[],"length":0,"stats":{"Line":0}},{"line":426,"address":[],"length":0,"stats":{"Line":0}},{"line":427,"address":[],"length":0,"stats":{"Line":16}},{"line":428,"address":[],"length":0,"stats":{"Line":16}},{"line":429,"address":[],"length":0,"stats":{"Line":16}},{"line":430,"address":[],"length":0,"stats":{"Line":16}},{"line":432,"address":[],"length":0,"stats":{"Line":16}},{"line":433,"address":[],"length":0,"stats":{"Line":16}},{"line":436,"address":[],"length":0,"stats":{"Line":16}},{"line":437,"address":[],"length":0,"stats":{"Line":16}},{"line":438,"address":[],"length":0,"stats":{"Line":16}},{"line":441,"address":[],"length":0,"stats":{"Line":16}},{"line":442,"address":[],"length":0,"stats":{"Line":16}},{"line":443,"address":[],"length":0,"stats":{"Line":16}},{"line":444,"address":[],"length":0,"stats":{"Line":16}},{"line":448,"address":[],"length":0,"stats":{"Line":16}},{"line":449,"address":[],"length":0,"stats":{"Line":16}},{"line":450,"address":[],"length":0,"stats":{"Line":16}},{"line":453,"address":[],"length":0,"stats":{"Line":16}},{"line":454,"address":[],"length":0,"stats":{"Line":16}},{"line":455,"address":[],"length":0,"stats":{"Line":16}},{"line":456,"address":[],"length":0,"stats":{"Line":16}},{"line":457,"address":[],"length":0,"stats":{"Line":16}},{"line":458,"address":[],"length":0,"stats":{"Line":16}},{"line":459,"address":[],"length":0,"stats":{"Line":16}},{"line":461,"address":[],"length":0,"stats":{"Line":16}},{"line":463,"address":[],"length":0,"stats":{"Line":0}},{"line":464,"address":[],"length":0,"stats":{"Line":0}},{"line":465,"address":[],"length":0,"stats":{"Line":0}},{"line":466,"address":[],"length":0,"stats":{"Line":0}},{"line":470,"address":[],"length":0,"stats":{"Line":0}},{"line":472,"address":[],"length":0,"stats":{"Line":0}},{"line":473,"address":[],"length":0,"stats":{"Line":0}},{"line":476,"address":[],"length":0,"stats":{"Line":0}},{"line":477,"address":[],"length":0,"stats":{"Line":0}},{"line":478,"address":[],"length":0,"stats":{"Line":0}},{"line":481,"address":[],"length":0,"stats":{"Line":0}},{"line":482,"address":[],"length":0,"stats":{"Line":0}},{"line":486,"address":[],"length":0,"stats":{"Line":0}},{"line":489,"address":[],"length":0,"stats":{"Line":0}},{"line":490,"address":[],"length":0,"stats":{"Line":0}},{"line":491,"address":[],"length":0,"stats":{"Line":0}},{"line":493,"address":[],"length":0,"stats":{"Line":0}},{"line":494,"address":[],"length":0,"stats":{"Line":0}},{"line":495,"address":[],"length":0,"stats":{"Line":0}},{"line":496,"address":[],"length":0,"stats":{"Line":0}},{"line":503,"address":[],"length":0,"stats":{"Line":29}},{"line":504,"address":[],"length":0,"stats":{"Line":29}},{"line":507,"address":[],"length":0,"stats":{"Line":0}},{"line":509,"address":[],"length":0,"stats":{"Line":0}},{"line":510,"address":[],"length":0,"stats":{"Line":0}},{"line":516,"address":[],"length":0,"stats":{"Line":13}},{"line":520,"address":[],"length":0,"stats":{"Line":526}},{"line":629,"address":[],"length":0,"stats":{"Line":263}},{"line":630,"address":[],"length":0,"stats":{"Line":263}},{"line":634,"address":[],"length":0,"stats":{"Line":526}},{"line":635,"address":[],"length":0,"stats":{"Line":263}},{"line":636,"address":[],"length":0,"stats":{"Line":263}},{"line":637,"address":[],"length":0,"stats":{"Line":263}},{"line":638,"address":[],"length":0,"stats":{"Line":263}},{"line":639,"address":[],"length":0,"stats":{"Line":263}},{"line":640,"address":[],"length":0,"stats":{"Line":263}},{"line":641,"address":[],"length":0,"stats":{"Line":263}},{"line":642,"address":[],"length":0,"stats":{"Line":263}},{"line":643,"address":[],"length":0,"stats":{"Line":263}},{"line":644,"address":[],"length":0,"stats":{"Line":263}},{"line":646,"address":[],"length":0,"stats":{"Line":0}},{"line":647,"address":[],"length":0,"stats":{"Line":33}},{"line":648,"address":[],"length":0,"stats":{"Line":296}},{"line":650,"address":[],"length":0,"stats":{"Line":0}},{"line":651,"address":[],"length":0,"stats":{"Line":33}},{"line":652,"address":[],"length":0,"stats":{"Line":33}},{"line":653,"address":[],"length":0,"stats":{"Line":33}},{"line":654,"address":[],"length":0,"stats":{"Line":33}},{"line":656,"address":[],"length":0,"stats":{"Line":33}},{"line":657,"address":[],"length":0,"stats":{"Line":33}},{"line":658,"address":[],"length":0,"stats":{"Line":33}},{"line":659,"address":[],"length":0,"stats":{"Line":33}},{"line":660,"address":[],"length":0,"stats":{"Line":33}},{"line":661,"address":[],"length":0,"stats":{"Line":33}},{"line":662,"address":[],"length":0,"stats":{"Line":33}},{"line":665,"address":[],"length":0,"stats":{"Line":0}},{"line":667,"address":[],"length":0,"stats":{"Line":0}},{"line":668,"address":[],"length":0,"stats":{"Line":0}},{"line":669,"address":[],"length":0,"stats":{"Line":0}},{"line":670,"address":[],"length":0,"stats":{"Line":0}},{"line":677,"address":[],"length":0,"stats":{"Line":0}},{"line":680,"address":[],"length":0,"stats":{"Line":13}},{"line":686,"address":[],"length":0,"stats":{"Line":13}},{"line":687,"address":[],"length":0,"stats":{"Line":13}},{"line":692,"address":[],"length":0,"stats":{"Line":13}},{"line":693,"address":[],"length":0,"stats":{"Line":13}},{"line":694,"address":[],"length":0,"stats":{"Line":13}},{"line":696,"address":[],"length":0,"stats":{"Line":13}},{"line":698,"address":[],"length":0,"stats":{"Line":26}},{"line":699,"address":[],"length":0,"stats":{"Line":0}},{"line":702,"address":[],"length":0,"stats":{"Line":0}},{"line":731,"address":[],"length":0,"stats":{"Line":13}},{"line":732,"address":[],"length":0,"stats":{"Line":0}},{"line":733,"address":[],"length":0,"stats":{"Line":0}},{"line":734,"address":[],"length":0,"stats":{"Line":0}},{"line":735,"address":[],"length":0,"stats":{"Line":0}},{"line":736,"address":[],"length":0,"stats":{"Line":0}},{"line":737,"address":[],"length":0,"stats":{"Line":0}},{"line":738,"address":[],"length":0,"stats":{"Line":0}},{"line":740,"address":[],"length":0,"stats":{"Line":0}},{"line":743,"address":[],"length":0,"stats":{"Line":16}},{"line":751,"address":[],"length":0,"stats":{"Line":16}},{"line":752,"address":[],"length":0,"stats":{"Line":16}},{"line":757,"address":[],"length":0,"stats":{"Line":16}},{"line":760,"address":[],"length":0,"stats":{"Line":16}},{"line":762,"address":[],"length":0,"stats":{"Line":16}},{"line":766,"address":[],"length":0,"stats":{"Line":32}},{"line":768,"address":[],"length":0,"stats":{"Line":0}},{"line":771,"address":[],"length":0,"stats":{"Line":0}},{"line":772,"address":[],"length":0,"stats":{"Line":0}},{"line":775,"address":[],"length":0,"stats":{"Line":0}},{"line":777,"address":[],"length":0,"stats":{"Line":0}},{"line":779,"address":[],"length":0,"stats":{"Line":0}},{"line":781,"address":[],"length":0,"stats":{"Line":0}},{"line":783,"address":[],"length":0,"stats":{"Line":0}},{"line":785,"address":[],"length":0,"stats":{"Line":16}},{"line":786,"address":[],"length":0,"stats":{"Line":0}},{"line":787,"address":[],"length":0,"stats":{"Line":0}},{"line":789,"address":[],"length":0,"stats":{"Line":16}},{"line":790,"address":[],"length":0,"stats":{"Line":16}},{"line":796,"address":[],"length":0,"stats":{"Line":0}},{"line":798,"address":[],"length":0,"stats":{"Line":16}},{"line":801,"address":[],"length":0,"stats":{"Line":0}},{"line":807,"address":[],"length":0,"stats":{"Line":0}},{"line":808,"address":[],"length":0,"stats":{"Line":0}},{"line":813,"address":[],"length":0,"stats":{"Line":0}},{"line":814,"address":[],"length":0,"stats":{"Line":0}},{"line":815,"address":[],"length":0,"stats":{"Line":0}},{"line":817,"address":[],"length":0,"stats":{"Line":0}},{"line":821,"address":[],"length":0,"stats":{"Line":0}},{"line":823,"address":[],"length":0,"stats":{"Line":0}},{"line":826,"address":[],"length":0,"stats":{"Line":0}},{"line":828,"address":[],"length":0,"stats":{"Line":0}},{"line":829,"address":[],"length":0,"stats":{"Line":0}},{"line":857,"address":[],"length":0,"stats":{"Line":0}},{"line":858,"address":[],"length":0,"stats":{"Line":0}},{"line":859,"address":[],"length":0,"stats":{"Line":0}},{"line":860,"address":[],"length":0,"stats":{"Line":0}},{"line":861,"address":[],"length":0,"stats":{"Line":0}},{"line":862,"address":[],"length":0,"stats":{"Line":0}},{"line":863,"address":[],"length":0,"stats":{"Line":0}},{"line":865,"address":[],"length":0,"stats":{"Line":0}}],"covered":153,"coverable":293},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","src","queue.rs"],"content":"use std::time::Duration;\n\n#[derive(Clone, Debug)]\npub struct QueueOptions {\n    pub max_success: usize,\n    pub max_failed: usize,\n    pub lease_duration: Duration,\n    pub local_concurrency: usize,\n\n    pub polling_interval: Duration,\n\n    /// If true, always poll for jobs even if there are no available permits\n    /// This is important, because polling is how delayed and timed out jobs are handled\n    /// If you have a horiztonally scaled deployment, this can be set to the default of false\n    /// But if there's only one node, you can set this to true to avoid the local concurrency from blocking queue housekeeping\n    pub always_poll: bool,\n}\n\nimpl Default for QueueOptions {\n    fn default() -\u003e Self {\n        Self {\n            max_success: 1000,\n            max_failed: 10000,\n            local_concurrency: 100,\n            polling_interval: Duration::from_millis(100),\n            lease_duration: Duration::from_secs(30),\n            always_poll: false,\n        }\n    }\n}\n","traces":[{"line":20,"address":[],"length":0,"stats":{"Line":10}},{"line":25,"address":[],"length":0,"stats":{"Line":10}},{"line":26,"address":[],"length":0,"stats":{"Line":10}}],"covered":3,"coverable":3},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","tests","basic.rs"],"content":"mod fixtures;\nuse fixtures::*;\nuse redis::AsyncCommands;\nuse tracing_subscriber::EnvFilter;\nuse tracing_subscriber::layer::SubscriberExt;\nuse tracing_subscriber::util::SubscriberInitExt;\nuse twmq::Queue;\n\nuse std::sync::atomic::Ordering;\n// Or use specific imports if they are in a different module.\nuse std::sync::Arc;\nuse std::time::Duration;\nuse twmq::job::{JobOptions, JobStatus}; // Assuming JobStatus is in twmq::job\nuse twmq::redis::aio::ConnectionManager; // For cleanup utility\n\nconst REDIS_URL: \u0026str = \"redis://127.0.0.1:6379/\";\n\n// Helper to clean up Redis keys for a given queue name pattern\nasync fn cleanup_redis_keys(conn_manager: \u0026ConnectionManager, queue_name: \u0026str) {\n    let mut conn = conn_manager.clone();\n    let keys_pattern = format!(\"twmq:{}:*\", queue_name);\n\n    let keys: Vec\u003cString\u003e = redis::cmd(\"KEYS\")\n        .arg(\u0026keys_pattern)\n        .query_async(\u0026mut conn)\n        .await\n        .unwrap_or_default();\n    if !keys.is_empty() {\n        redis::cmd(\"DEL\")\n            .arg(keys)\n            .query_async::\u003c()\u003e(\u0026mut conn)\n            .await\n            .unwrap_or_default();\n    }\n    println!(\"Cleaned up keys for pattern: {}\", keys_pattern);\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_queue_push_and_process_job() {\n    tracing_subscriber::registry()\n        .with(EnvFilter::try_from_default_env().unwrap_or_else(|_| {\n            // Default to info level if RUST_LOG environment variable is not set\n            \"thirdweb_engine=debug,tower_http=debug,axum=debug\".into()\n        }))\n        .with(tracing_subscriber::fmt::layer())\n        .init();\n\n    let test_job_id = \"test_job_001\".to_string();\n    let queue_name = format!(\"test_q_simple_{}\", nanoid::nanoid!(6));\n\n    // Reset the flag before each test run\n    TEST_JOB_PROCESSED_SUCCESSFULLY.store(false, Ordering::SeqCst);\n\n    println!(\"Creating queue: {}\", queue_name);\n    let queue = Arc::new(\n        Queue::\u003cTestJobPayload, TestJobOutput, TestJobErrorData, ()\u003e::new(\n            REDIS_URL,\n            \u0026queue_name,\n            None, // Default QueueOptions\n            (),\n        )\n        .await\n        .expect(\"Failed to create queue\"),\n    );\n\n    // Cleanup Redis before starting, in case of previous failed test\n    cleanup_redis_keys(\u0026queue.redis.clone(), \u0026queue_name).await;\n\n    let job_payload = TestJobPayload {\n        message: \"hello from test\".to_string(),\n        id_to_check: test_job_id.clone(),\n    };\n\n    println!(\"Pushing job with ID: {}\", test_job_id);\n    let job_options = JobOptions {\n        data: job_payload,\n        id: test_job_id.clone(),\n        delay: None,\n    };\n\n    let pushed_job_details = queue.push(job_options).await.expect(\"Failed to push job\");\n    assert_eq!(pushed_job_details.id, test_job_id);\n\n    let pending_count = queue\n        .count(JobStatus::Pending)\n        .await\n        .expect(\"Failed to count pending jobs\");\n    assert_eq!(\n        pending_count, 1,\n        \"There should be 1 job in the pending list\"\n    );\n\n    println!(\"Starting worker for queue: {}\", queue_name);\n\n    let queue_name_clone = queue_name.clone();\n    let worker_queue_ref = Arc::clone(\u0026queue);\n    let worker_handle = tokio::spawn(async move {\n        // The worker will loop internally, so we expect it to pick up the job\n        if let Err(e) = worker_queue_ref.work().await {\n            eprintln!(\"Worker for queue {} failed: {:?}\", queue_name_clone, e);\n        }\n    });\n\n    // Wait for the job to be processed\n    // Poll the flag, with a timeout\n    let mut processed = false;\n    for _ in 0..50 {\n        // Max wait 5 seconds (50 * 100ms)\n        if TEST_JOB_PROCESSED_SUCCESSFULLY.load(Ordering::SeqCst) {\n            processed = true;\n            println!(\"Job processed flag is true.\");\n            break;\n        }\n        tokio::time::sleep(Duration::from_millis(100)).await;\n    }\n    assert!(processed, \"Job was not processed by the worker in time\");\n\n    // Verify job moved to success list\n    let success_count = queue\n        .count(JobStatus::Success)\n        .await\n        .expect(\"Failed to count successful jobs\");\n    assert_eq!(\n        success_count, 1,\n        \"There should be 1 job in the success list\"\n    );\n\n    // Verify pending and active lists are empty\n    let pending_after_processing = queue\n        .count(JobStatus::Pending)\n        .await\n        .expect(\"Failed to count pending jobs after processing\");\n    assert_eq!(\n        pending_after_processing, 0,\n        \"Pending list should be empty after processing\"\n    );\n\n    let active_count = queue\n        .count(JobStatus::Active)\n        .await\n        .expect(\"Failed to count active jobs\");\n    assert_eq!(active_count, 0, \"Active list should be empty\");\n\n    // Verify the job result was stored\n    let mut redis_conn = queue.redis.clone();\n    let result_json_opt: Option\u003cString\u003e = redis_conn\n        .hget(queue.job_result_hash_name(), \u0026test_job_id)\n        .await\n        .expect(\"Redis HGET for result failed\");\n\n    assert!(\n        result_json_opt.is_some(),\n        \"Job result should be stored in Redis\"\n    );\n    let result_json = result_json_opt.unwrap();\n    let job_output: TestJobOutput =\n        serde_json::from_str(\u0026result_json).expect(\"Failed to deserialize job output\");\n    assert_eq!(job_output.reply, \"Successfully processed 'hello from test'\");\n\n    println!(\"Test completed for queue: {}\", queue_name.clone());\n\n    // The worker task runs in a loop. For a clean test exit,\n    // you might want to abort it or implement a shutdown signal for the worker.\n    // For this simple test, we'll let it be.\n    worker_handle.abort(); // Or a more graceful shutdown if implemented\n\n    // Cleanup Redis keys after test\n    cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n}\n","traces":[{"line":19,"address":[],"length":0,"stats":{"Line":4}},{"line":20,"address":[],"length":0,"stats":{"Line":2}},{"line":21,"address":[],"length":0,"stats":{"Line":2}},{"line":23,"address":[],"length":0,"stats":{"Line":4}},{"line":24,"address":[],"length":0,"stats":{"Line":2}},{"line":25,"address":[],"length":0,"stats":{"Line":2}},{"line":26,"address":[],"length":0,"stats":{"Line":2}},{"line":28,"address":[],"length":0,"stats":{"Line":2}},{"line":29,"address":[],"length":0,"stats":{"Line":1}},{"line":30,"address":[],"length":0,"stats":{"Line":1}},{"line":31,"address":[],"length":0,"stats":{"Line":1}},{"line":32,"address":[],"length":0,"stats":{"Line":1}},{"line":35,"address":[],"length":0,"stats":{"Line":2}}],"covered":13,"coverable":13},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","tests","basic_hook.rs"],"content":"// Add this to tests/basic.rs\nmod fixtures;\nuse fixtures::{TestJobErrorData, TestJobOutput};\nuse redis::aio::ConnectionManager;\nuse tracing_subscriber::{EnvFilter, layer::SubscriberExt, util::SubscriberInitExt};\n\nuse std::{\n    sync::{\n        Arc,\n        atomic::{AtomicBool, Ordering},\n    },\n    time::Duration,\n};\n\nuse serde::{Deserialize, Serialize};\nuse twmq::{\n    DurableExecution, Queue, SuccessHookData,\n    hooks::TransactionContext,\n    job::{Job, JobResult, JobStatus},\n    queue::QueueOptions,\n};\n\n// Helper to clean up Redis keys for a given queue name pattern\nasync fn cleanup_redis_keys(conn_manager: \u0026ConnectionManager, queue_name: \u0026str) {\n    let mut conn = conn_manager.clone();\n    let keys_pattern = format!(\"twmq:{}:*\", queue_name);\n\n    let keys: Vec\u003cString\u003e = redis::cmd(\"KEYS\")\n        .arg(\u0026keys_pattern)\n        .query_async(\u0026mut conn)\n        .await\n        .unwrap_or_default();\n    if !keys.is_empty() {\n        redis::cmd(\"DEL\")\n            .arg(keys)\n            .query_async::\u003c()\u003e(\u0026mut conn)\n            .await\n            .unwrap_or_default();\n    }\n    println!(\"Cleaned up keys for pattern: {}\", keys_pattern);\n}\n\n// Define webhook job types\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct WebhookJobPayload {\n    pub url: String,\n    pub payload: String,\n    pub parent_job_id: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]\npub struct WebhookJobOutput {\n    pub status_code: u16,\n    pub response: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct WebhookJobErrorData {\n    pub error: String,\n}\n\n// Main job that queues webhook jobs\n#[derive(Serialize, Deserialize, Clone)]\npub struct MainJobPayload {\n    pub message: String,\n    pub id_to_check: String,\n    // Pass webhook queue reference as part of job data\n}\n\npub static MAIN_JOB_PROCESSED: AtomicBool = AtomicBool::new(false);\npub static WEBHOOK_JOB_PROCESSED: AtomicBool = AtomicBool::new(false);\n\nconst REDIS_URL: \u0026str = \"redis://127.0.0.1:6379/\";\n\ntype WebhookQueue = Queue\u003cWebhookJobPayload, WebhookJobOutput, WebhookJobErrorData, ()\u003e;\n\nimpl DurableExecution for MainJobPayload {\n    type Output = TestJobOutput;\n    type ErrorData = TestJobErrorData;\n    type ExecutionContext = Arc\u003cWebhookQueue\u003e;\n\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        _: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        println!(\"MAIN_JOB: Processing job with id: {}\", job.id);\n        tokio::time::sleep(Duration::from_millis(50)).await;\n\n        MAIN_JOB_PROCESSED.store(true, Ordering::SeqCst);\n\n        JobResult::Success(TestJobOutput {\n            reply: format!(\"Main job processed: {}\", job.data.message),\n        })\n    }\n\n    async fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: SuccessHookData\u003c'_, Self::Output\u003e,\n        tx: \u0026mut TransactionContext\u003c'_\u003e,\n        ec: \u0026Self::ExecutionContext,\n    ) {\n        println!(\"MAIN_JOB: on_success hook - queuing webhook job\");\n\n        let webhook_job = WebhookJobPayload {\n            url: \"https://api.example.com/webhook\".to_string(),\n            payload: serde_json::to_string(d.result).unwrap(),\n            parent_job_id: self.id_to_check.clone(),\n        };\n\n        // Use the type-safe API!\n        let mut webhook_builder = ec.clone().job(webhook_job);\n\n        webhook_builder.options.id = format!(\"{}_webhook\", self.id_to_check);\n\n        if let Err(e) = tx.queue_job(webhook_builder) {\n            tracing::error!(\"Failed to queue webhook job: {:?}\", e);\n        } else {\n            tracing::info!(\"Successfully queued webhook job!\");\n        }\n    }\n}\n\nimpl DurableExecution for WebhookJobPayload {\n    type Output = WebhookJobOutput;\n    type ErrorData = WebhookJobErrorData;\n    type ExecutionContext = ();\n\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        _: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        println!(\"WEBHOOK_JOB: Sending webhook to: {}\", job.data.url);\n        println!(\"WEBHOOK_JOB: Payload: {}\", job.data.payload);\n        tokio::time::sleep(Duration::from_millis(25)).await;\n\n        WEBHOOK_JOB_PROCESSED.store(true, Ordering::SeqCst);\n\n        // Simulate successful webhook call\n        JobResult::Success(WebhookJobOutput {\n            status_code: 200,\n            response: \"Webhook delivered successfully\".to_string(),\n        })\n    }\n\n    async fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        _d: SuccessHookData\u003c'_, Self::Output\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\n            \"WEBHOOK_JOB: Webhook delivered successfully for parent: {}\",\n            self.parent_job_id\n        );\n    }\n\n    async fn on_timeout(\u0026self, _tx: \u0026mut TransactionContext\u003c'_\u003e) {}\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_cross_queue_job_scheduling() {\n    tracing_subscriber::registry()\n        .with(\n            EnvFilter::try_from_default_env()\n                .unwrap_or_else(|_| \"thirdweb_engine=debug,tower_http=debug,axum=debug\".into()),\n        )\n        .with(tracing_subscriber::fmt::layer())\n        .init();\n\n    let main_queue_name = format!(\"test_main_{}\", nanoid::nanoid!(6));\n    let webhook_queue_name = format!(\"test_webhook_{}\", nanoid::nanoid!(6));\n\n    // Reset flags\n    MAIN_JOB_PROCESSED.store(false, Ordering::SeqCst);\n    WEBHOOK_JOB_PROCESSED.store(false, Ordering::SeqCst);\n\n    println!(\"Creating main queue: {}\", main_queue_name);\n    println!(\"Creating webhook queue: {}\", webhook_queue_name);\n\n    let mut queue_options = QueueOptions::default();\n    queue_options.local_concurrency = 1;\n\n    // Create webhook queue\n    let webhook_queue = Arc::new(\n        Queue::\u003cWebhookJobPayload, WebhookJobOutput, WebhookJobErrorData, ()\u003e::new(\n            REDIS_URL,\n            \u0026webhook_queue_name,\n            Some(queue_options.clone()),\n            (),\n        )\n        .await\n        .expect(\"Failed to create webhook queue\"),\n    );\n\n    // Create main job queue\n    let main_queue = Arc::new(\n        Queue::\u003cMainJobPayload, TestJobOutput, TestJobErrorData, Arc\u003cWebhookQueue\u003e\u003e::new(\n            REDIS_URL,\n            \u0026main_queue_name,\n            Some(queue_options),\n            webhook_queue.clone(),\n        )\n        .await\n        .expect(\"Failed to create main queue\"),\n    );\n\n    // Clean up before test\n    cleanup_redis_keys(\u0026main_queue.redis, \u0026main_queue_name).await;\n    cleanup_redis_keys(\u0026webhook_queue.redis, \u0026webhook_queue_name).await;\n\n    // Create main job with access to webhook queue\n    let main_job = MainJobPayload {\n        message: \"Process transaction #123\".to_string(),\n        id_to_check: \"main_job_001\".to_string(),\n    };\n\n    println!(\"Pushing main job\");\n\n    main_queue\n        .clone()\n        .job(main_job)\n        .with_id(\"main_job_001\")\n        .push()\n        .await\n        .expect(\"Failed to push main job\");\n\n    // Start workers for both queues\n    println!(\"Starting workers\");\n    let main_worker = {\n        let queue = main_queue.clone();\n        tokio::spawn(async move {\n            if let Err(e) = queue.work().await {\n                eprintln!(\"Main worker failed: {:?}\", e);\n            }\n        })\n    };\n\n    let webhook_worker = {\n        let queue = webhook_queue.clone();\n        tokio::spawn(async move {\n            if let Err(e) = queue.work().await {\n                eprintln!(\"Webhook worker failed: {:?}\", e);\n            }\n        })\n    };\n\n    // Wait for main job to complete\n    let mut main_processed = false;\n    for _ in 0..50 {\n        if MAIN_JOB_PROCESSED.load(Ordering::SeqCst) {\n            main_processed = true;\n            println!(\"Main job processed!\");\n            break;\n        }\n        tokio::time::sleep(Duration::from_millis(100)).await;\n    }\n    assert!(main_processed, \"Main job should be processed\");\n\n    // Give a moment for the webhook job to be queued and processed\n    tokio::time::sleep(Duration::from_millis(200)).await;\n\n    // Check that webhook job was queued and processed\n    let webhook_pending = webhook_queue.count(JobStatus::Pending).await.unwrap();\n    let webhook_success = webhook_queue.count(JobStatus::Success).await.unwrap();\n\n    println!(\n        \"Webhook queue - Pending: {}, Success: {}\",\n        webhook_pending, webhook_success\n    );\n\n    // Either the webhook job is still pending or already succeeded\n    assert!(\n        webhook_pending \u003e 0 || webhook_success \u003e 0,\n        \"Webhook job should be queued\"\n    );\n\n    // Wait for webhook job to complete if it's still pending\n    if webhook_pending \u003e 0 {\n        let mut webhook_processed = false;\n        for _ in 0..50 {\n            if WEBHOOK_JOB_PROCESSED.load(Ordering::SeqCst) {\n                webhook_processed = true;\n                println!(\"Webhook job processed!\");\n                break;\n            }\n            tokio::time::sleep(Duration::from_millis(100)).await;\n        }\n        assert!(webhook_processed, \"Webhook job should be processed\");\n    }\n\n    // Verify final state\n    let main_success = main_queue.count(JobStatus::Success).await.unwrap();\n    let webhook_final_success = webhook_queue.count(JobStatus::Success).await.unwrap();\n\n    assert_eq!(main_success, 1, \"Main job should be in success list\");\n    assert_eq!(\n        webhook_final_success, 1,\n        \"Webhook job should be in success list\"\n    );\n\n    println!(\"✅ Cross-queue job scheduling works!\");\n    println!(\"Main job triggered webhook job atomically via transaction hooks\");\n\n    // Cleanup\n    main_worker.abort();\n    webhook_worker.abort();\n    cleanup_redis_keys(\u0026main_queue.redis, \u0026main_queue_name).await;\n    cleanup_redis_keys(\u0026webhook_queue.redis, \u0026webhook_queue_name).await;\n}\n","traces":[{"line":24,"address":[],"length":0,"stats":{"Line":8}},{"line":25,"address":[],"length":0,"stats":{"Line":4}},{"line":26,"address":[],"length":0,"stats":{"Line":4}},{"line":28,"address":[],"length":0,"stats":{"Line":8}},{"line":29,"address":[],"length":0,"stats":{"Line":4}},{"line":30,"address":[],"length":0,"stats":{"Line":4}},{"line":31,"address":[],"length":0,"stats":{"Line":4}},{"line":33,"address":[],"length":0,"stats":{"Line":4}},{"line":34,"address":[],"length":0,"stats":{"Line":2}},{"line":35,"address":[],"length":0,"stats":{"Line":2}},{"line":36,"address":[],"length":0,"stats":{"Line":2}},{"line":37,"address":[],"length":0,"stats":{"Line":2}},{"line":40,"address":[],"length":0,"stats":{"Line":4}},{"line":82,"address":[],"length":0,"stats":{"Line":1}},{"line":86,"address":[],"length":0,"stats":{"Line":1}},{"line":87,"address":[],"length":0,"stats":{"Line":1}},{"line":89,"address":[],"length":0,"stats":{"Line":1}},{"line":91,"address":[],"length":0,"stats":{"Line":1}},{"line":92,"address":[],"length":0,"stats":{"Line":1}},{"line":96,"address":[],"length":0,"stats":{"Line":1}},{"line":103,"address":[],"length":0,"stats":{"Line":1}},{"line":106,"address":[],"length":0,"stats":{"Line":1}},{"line":107,"address":[],"length":0,"stats":{"Line":1}},{"line":108,"address":[],"length":0,"stats":{"Line":1}},{"line":112,"address":[],"length":0,"stats":{"Line":1}},{"line":114,"address":[],"length":0,"stats":{"Line":1}},{"line":116,"address":[],"length":0,"stats":{"Line":1}},{"line":117,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":1}},{"line":129,"address":[],"length":0,"stats":{"Line":1}},{"line":133,"address":[],"length":0,"stats":{"Line":1}},{"line":134,"address":[],"length":0,"stats":{"Line":1}},{"line":135,"address":[],"length":0,"stats":{"Line":1}},{"line":137,"address":[],"length":0,"stats":{"Line":1}},{"line":140,"address":[],"length":0,"stats":{"Line":1}},{"line":141,"address":[],"length":0,"stats":{"Line":1}},{"line":142,"address":[],"length":0,"stats":{"Line":1}},{"line":146,"address":[],"length":0,"stats":{"Line":1}},{"line":153,"address":[],"length":0,"stats":{"Line":1}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":159,"address":[],"length":0,"stats":{"Line":0}}],"covered":38,"coverable":41},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","tests","delay.rs"],"content":"// tests/delay.rs\n\nuse std::sync::Arc;\nuse std::time::{Duration, SystemTime, UNIX_EPOCH};\n\nuse redis::AsyncCommands;\nuse serde::{Deserialize, Serialize};\nuse tracing_subscriber::{EnvFilter, layer::SubscriberExt, util::SubscriberInitExt};\n\nuse twmq::{\n    DurableExecution, Queue, SuccessHookData,\n    hooks::TransactionContext,\n    job::{DelayOptions, Job, JobResult, JobStatus, RequeuePosition},\n    queue::QueueOptions,\n    redis::aio::ConnectionManager,\n};\n\nconst REDIS_URL: \u0026str = \"redis://127.0.0.1:6379/\";\n\n// Helper to clean up Redis keys\nasync fn cleanup_redis_keys(conn_manager: \u0026ConnectionManager, queue_name: \u0026str) {\n    let mut conn = conn_manager.clone();\n    let keys_pattern = format!(\"twmq:{}:*\", queue_name);\n    let keys: Vec\u003cString\u003e = redis::cmd(\"KEYS\")\n        .arg(\u0026keys_pattern)\n        .query_async(\u0026mut conn)\n        .await\n        .unwrap_or_default();\n    if !keys.is_empty() {\n        redis::cmd(\"DEL\")\n            .arg(keys)\n            .query_async::\u003c()\u003e(\u0026mut conn)\n            .await\n            .unwrap_or_default();\n    }\n    tracing::info!(\"Cleaned up keys for pattern: {}\", keys_pattern);\n}\n\n// Job that tests delay functionality\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct DelayTestJob {\n    pub job_id: String,\n    pub expected_delay_seconds: u64,\n    pub created_at: u64,\n    pub test_id: String, // Unique per test to avoid conflicts\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]\npub struct DelayTestOutput {\n    pub actual_delay_seconds: u64,\n    pub message: String,\n    pub test_id: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct DelayTestErrorData {\n    pub reason: String,\n}\n\nimpl DurableExecution for DelayTestJob {\n    type Output = DelayTestOutput;\n    type ErrorData = DelayTestErrorData;\n    type ExecutionContext = Arc\u003cConnectionManager\u003e;\n\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        _redis_conn: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n        let actual_delay = now - job.data.created_at;\n\n        tracing::info!(\n            \"DELAY_JOB: Processing job {}, expected delay: {}s, actual delay: {}s\",\n            job.data.job_id,\n            job.data.expected_delay_seconds,\n            actual_delay\n        );\n\n        JobResult::Success(DelayTestOutput {\n            actual_delay_seconds: actual_delay,\n            message: format!(\n                \"Job {} processed after {}s delay\",\n                job.data.job_id, actual_delay\n            ),\n            test_id: job.data.test_id.clone(),\n        })\n    }\n\n    async fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: SuccessHookData\u003c'_, Self::Output\u003e,\n        tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _redis_conn: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\"DELAY_JOB: on_success hook - {}\", d.result.message);\n\n        // Store processing order in Redis using test-specific key\n        let order_key = format!(\"test:{}:processing_order\", self.test_id);\n\n        // Use pipeline to add to processing order list\n        tx.pipeline().rpush(\u0026order_key, \u0026self.job_id);\n        tx.pipeline().expire(\u0026order_key, 300); // Expire after 5 minutes\n    }\n\n    async fn on_timeout(\u0026self, _tx: \u0026mut TransactionContext\u003c'_\u003e) {\n        tracing::info!(\"DELAY_JOB: on_timeout hook for job {}\", self.job_id);\n    }\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_job_delay_basic() {\n    tracing_subscriber::registry()\n        .with(EnvFilter::try_from_default_env().unwrap_or_else(|_| \"twmq=debug\".into()))\n        .with(tracing_subscriber::fmt::layer())\n        .init();\n\n    let test_id = nanoid::nanoid!();\n    let queue_name = format!(\"test_delay_{}\", test_id);\n    let job_id = \"delay_job_001\";\n    let delay_duration = Duration::from_secs(2);\n\n    tracing::info!(\n        \"Creating delay test queue: {} (test_id: {})\",\n        queue_name,\n        test_id\n    );\n    tracing::info!(\"Job should be delayed by: {:?}\", delay_duration);\n\n    let queue_options = QueueOptions {\n        local_concurrency: 1,\n        polling_interval: Duration::from_millis(100),\n        always_poll: true,\n        ..Default::default()\n    };\n\n    // Create Redis connection for the execution context\n    let redis_client = redis::Client::open(REDIS_URL).unwrap();\n    let redis_conn = Arc::new(redis_client.get_connection_manager().await.unwrap());\n\n    let queue = Arc::new(\n        Queue::\u003cDelayTestJob, DelayTestOutput, DelayTestErrorData, Arc\u003cConnectionManager\u003e\u003e::new(\n            REDIS_URL,\n            \u0026queue_name,\n            Some(queue_options),\n            redis_conn.clone(),\n        )\n        .await\n        .expect(\"Failed to create delay test queue\"),\n    );\n\n    cleanup_redis_keys(\u0026redis_conn, \u0026queue_name).await;\n\n    let created_at = SystemTime::now()\n        .duration_since(UNIX_EPOCH)\n        .unwrap()\n        .as_secs();\n\n    // Create job with delay\n    let delay_job = DelayTestJob {\n        job_id: job_id.to_string(),\n        expected_delay_seconds: delay_duration.as_secs(),\n        created_at,\n        test_id: test_id.clone(),\n    };\n\n    tracing::info!(\"Pushing delayed job with ID: {}\", job_id);\n\n    let delay_options = DelayOptions {\n        delay: delay_duration,\n        position: RequeuePosition::Last,\n    };\n\n    queue\n        .clone()\n        .job(delay_job)\n        .with_id(job_id)\n        .with_delay(delay_options)\n        .push()\n        .await\n        .expect(\"Failed to push delayed job\");\n\n    // Verify job starts in delayed queue, not pending\n    let initial_pending = queue.count(JobStatus::Pending).await.unwrap();\n    let initial_delayed = queue.count(JobStatus::Delayed).await.unwrap();\n    let initial_active = queue.count(JobStatus::Active).await.unwrap();\n\n    tracing::info!(\n        \"Initial state - Pending: {}, Delayed: {}, Active: {}\",\n        initial_pending,\n        initial_delayed,\n        initial_active\n    );\n\n    assert_eq!(\n        initial_pending, 0,\n        \"Job should not be in pending queue initially\"\n    );\n    assert_eq!(initial_delayed, 1, \"Job should be in delayed queue\");\n    assert_eq!(initial_active, 0, \"No jobs should be active initially\");\n\n    // Start worker\n    tracing::info!(\"Starting worker\");\n    let worker = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            if let Err(e) = queue.work().await {\n                tracing::error!(\"Delay worker failed: {:?}\", e);\n            }\n        })\n    };\n\n    // Job should not be processed immediately (still delayed)\n    tokio::time::sleep(Duration::from_millis(500)).await;\n\n    let early_success_count = queue.count(JobStatus::Success).await.unwrap();\n    assert_eq!(\n        early_success_count, 0,\n        \"Job should not be processed before delay expires\"\n    );\n\n    let mid_pending = queue.count(JobStatus::Pending).await.unwrap();\n    let mid_delayed = queue.count(JobStatus::Delayed).await.unwrap();\n    tracing::info!(\n        \"Mid-delay state - Pending: {}, Delayed: {}\",\n        mid_pending,\n        mid_delayed\n    );\n\n    // Wait for delay to expire and job to be processed\n    let total_wait = delay_duration + Duration::from_secs(2);\n    tracing::info!(\n        \"Waiting {:?} for delay to expire and job to process...\",\n        total_wait\n    );\n\n    let mut job_processed = false;\n    let start_waiting = SystemTime::now();\n\n    while start_waiting.elapsed().unwrap() \u003c total_wait {\n        let success_count = queue.count(JobStatus::Success).await.unwrap();\n        if success_count \u003e 0 {\n            job_processed = true;\n            tracing::info!(\n                \"Job processed after waiting {:?}\",\n                start_waiting.elapsed().unwrap()\n            );\n            break;\n        }\n        tokio::time::sleep(Duration::from_millis(200)).await;\n    }\n\n    assert!(job_processed, \"Job should be processed after delay expires\");\n\n    // Give a moment for final processing\n    tokio::time::sleep(Duration::from_millis(300)).await;\n\n    // Verify final state\n    let final_pending = queue.count(JobStatus::Pending).await.unwrap();\n    let final_delayed = queue.count(JobStatus::Delayed).await.unwrap();\n    let final_active = queue.count(JobStatus::Active).await.unwrap();\n    let final_success = queue.count(JobStatus::Success).await.unwrap();\n\n    tracing::info!(\n        \"Final state - Pending: {}, Delayed: {}, Active: {}, Success: {}\",\n        final_pending,\n        final_delayed,\n        final_active,\n        final_success\n    );\n\n    assert_eq!(final_delayed, 0, \"No jobs should remain in delayed queue\");\n    assert_eq!(final_active, 0, \"No jobs should be active\");\n    assert_eq!(final_success, 1, \"Job should be in success queue\");\n\n    // Verify the job result shows correct delay timing\n    let mut redis_conn_direct = redis_conn.as_ref().clone();\n    let result_json: Option\u003cString\u003e = redis_conn_direct\n        .hget(queue.job_result_hash_name(), job_id)\n        .await\n        .expect(\"Failed to get job result\");\n\n    assert!(result_json.is_some(), \"Job result should be stored\");\n\n    let job_output: DelayTestOutput =\n        serde_json::from_str(\u0026result_json.unwrap()).expect(\"Failed to deserialize job result\");\n\n    tracing::info!(\"Job output: {:?}\", job_output);\n\n    // Allow some tolerance for timing (±1 second)\n    let expected_delay = delay_duration.as_secs();\n    let actual_delay = job_output.actual_delay_seconds;\n\n    assert!(\n        actual_delay \u003e= expected_delay \u0026\u0026 actual_delay \u003c= expected_delay + 2,\n        \"Actual delay ({}) should be close to expected delay ({})\",\n        actual_delay,\n        expected_delay\n    );\n\n    tracing::info!(\"✅ Basic delay mechanism works correctly!\");\n\n    worker.abort();\n    cleanup_redis_keys(\u0026redis_conn, \u0026queue_name).await;\n\n    // Clean up test-specific keys\n    let _: () = redis_conn_direct\n        .del(format!(\"test:{}:processing_order\", test_id))\n        .await\n        .unwrap_or(());\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_delay_position_ordering() {\n    // Test that delayed jobs respect RequeuePosition when they expire\n\n    let test_id = nanoid::nanoid!();\n    let queue_name = format!(\"test_delay_order_{}\", test_id);\n\n    tracing::info!(\n        \"\\n=== Testing delay position ordering (test_id: {}) ===\",\n        test_id\n    );\n\n    let queue_options = QueueOptions {\n        local_concurrency: 1, // Process one job at a time to see clear ordering\n        polling_interval: Duration::from_millis(50),\n        always_poll: true,\n        ..Default::default()\n    };\n\n    // Create Redis connection for the execution context\n    let redis_client = redis::Client::open(REDIS_URL).unwrap();\n    let redis_conn = Arc::new(redis_client.get_connection_manager().await.unwrap());\n\n    let queue = Arc::new(\n        Queue::\u003cDelayTestJob, DelayTestOutput, DelayTestErrorData, Arc\u003cConnectionManager\u003e\u003e::new(\n            REDIS_URL,\n            \u0026queue_name,\n            Some(queue_options),\n            redis_conn.clone(),\n        )\n        .await\n        .expect(\"Failed to create delay order queue\"),\n    );\n\n    cleanup_redis_keys(\u0026redis_conn, \u0026queue_name).await;\n\n    let created_at = SystemTime::now()\n        .duration_since(UNIX_EPOCH)\n        .unwrap()\n        .as_secs();\n    let short_delay = Duration::from_secs(1);\n\n    // Push jobs in specific order:\n    // 1. Normal job (no delay) - should process first\n    // 2. Delayed job with \"First\" position - should process second when delay expires\n    // 3. Delayed job with \"Last\" position - should process third when delay expires\n\n    // Job 1: Normal job (immediate)\n    let immediate_job = DelayTestJob {\n        job_id: \"immediate\".to_string(),\n        expected_delay_seconds: 0,\n        created_at,\n        test_id: test_id.clone(),\n    };\n\n    queue\n        .clone()\n        .job(immediate_job)\n        .with_id(\"immediate\")\n        .push()\n        .await\n        .unwrap();\n\n    // Job 2: Delayed job with First position\n    let delayed_first_job = DelayTestJob {\n        job_id: \"delayed_first\".to_string(),\n        expected_delay_seconds: short_delay.as_secs(),\n        created_at,\n        test_id: test_id.clone(),\n    };\n\n    let first_delay_options = DelayOptions {\n        delay: short_delay,\n        position: RequeuePosition::First,\n    };\n\n    queue\n        .clone()\n        .job(delayed_first_job)\n        .with_id(\"delayed_first\")\n        .with_delay(first_delay_options)\n        .push()\n        .await\n        .unwrap();\n\n    // Job 3: Delayed job with Last position\n    let delayed_last_job = DelayTestJob {\n        job_id: \"delayed_last\".to_string(),\n        expected_delay_seconds: short_delay.as_secs(),\n        created_at,\n        test_id: test_id.clone(),\n    };\n\n    let last_delay_options = DelayOptions {\n        delay: short_delay,\n        position: RequeuePosition::Last,\n    };\n\n    queue\n        .clone()\n        .job(delayed_last_job)\n        .with_id(\"delayed_last\")\n        .with_delay(last_delay_options)\n        .push()\n        .await\n        .unwrap();\n\n    // Verify initial state\n    let pending_count = queue.count(JobStatus::Pending).await.unwrap();\n    let delayed_count = queue.count(JobStatus::Delayed).await.unwrap();\n\n    tracing::info!(\n        \"Initial state - Pending: {}, Delayed: {}\",\n        pending_count,\n        delayed_count\n    );\n    assert_eq!(pending_count, 1, \"Should have 1 immediate job pending\");\n    assert_eq!(delayed_count, 2, \"Should have 2 delayed jobs\");\n\n    // Start worker\n    let worker = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            if let Err(e) = queue.work().await {\n                tracing::error!(\"Delay order worker failed: {:?}\", e);\n            }\n        })\n    };\n\n    // Wait for all jobs to complete\n    let max_wait = short_delay + Duration::from_secs(3);\n    let start_time = SystemTime::now();\n\n    while start_time.elapsed().unwrap() \u003c max_wait {\n        let success_count = queue.count(JobStatus::Success).await.unwrap();\n        if success_count \u003e= 3 {\n            break;\n        }\n        tokio::time::sleep(Duration::from_millis(100)).await;\n    }\n\n    tokio::time::sleep(Duration::from_millis(200)).await;\n\n    // Check processing order from Redis\n    let order_key = format!(\"test:{}:processing_order\", test_id);\n    let mut redis_conn_direct = redis_conn.as_ref().clone();\n    let processing_order: Vec\u003cString\u003e = redis_conn_direct\n        .lrange(\u0026order_key, 0, -1)\n        .await\n        .unwrap_or_default();\n\n    tracing::info!(\"Processing order: {:?}\", processing_order);\n\n    assert_eq!(processing_order.len(), 3, \"All 3 jobs should be processed\");\n    assert_eq!(\n        processing_order[0], \"immediate\",\n        \"Immediate job should process first\"\n    );\n\n    // The delayed jobs should process in position order: First comes before Last\n    assert_eq!(\n        processing_order[1], \"delayed_first\",\n        \"Delayed job with First position should process before Last position\"\n    );\n    assert_eq!(\n        processing_order[2], \"delayed_last\",\n        \"Delayed job with Last position should process last\"\n    );\n\n    tracing::info!(\"✅ Delay position ordering works correctly!\");\n\n    worker.abort();\n    cleanup_redis_keys(\u0026redis_conn, \u0026queue_name).await;\n\n    // Clean up test-specific keys\n    let _: () = redis_conn_direct.del(\u0026order_key).await.unwrap_or(());\n}\n","traces":[{"line":21,"address":[],"length":0,"stats":{"Line":8}},{"line":22,"address":[],"length":0,"stats":{"Line":4}},{"line":23,"address":[],"length":0,"stats":{"Line":4}},{"line":24,"address":[],"length":0,"stats":{"Line":8}},{"line":25,"address":[],"length":0,"stats":{"Line":4}},{"line":26,"address":[],"length":0,"stats":{"Line":4}},{"line":27,"address":[],"length":0,"stats":{"Line":4}},{"line":29,"address":[],"length":0,"stats":{"Line":4}},{"line":30,"address":[],"length":0,"stats":{"Line":2}},{"line":31,"address":[],"length":0,"stats":{"Line":2}},{"line":32,"address":[],"length":0,"stats":{"Line":2}},{"line":33,"address":[],"length":0,"stats":{"Line":2}},{"line":36,"address":[],"length":0,"stats":{"Line":4}},{"line":65,"address":[],"length":0,"stats":{"Line":4}},{"line":69,"address":[],"length":0,"stats":{"Line":4}},{"line":70,"address":[],"length":0,"stats":{"Line":4}},{"line":73,"address":[],"length":0,"stats":{"Line":4}},{"line":75,"address":[],"length":0,"stats":{"Line":4}},{"line":76,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[],"length":0,"stats":{"Line":4}},{"line":83,"address":[],"length":0,"stats":{"Line":4}},{"line":84,"address":[],"length":0,"stats":{"Line":4}},{"line":85,"address":[],"length":0,"stats":{"Line":4}},{"line":86,"address":[],"length":0,"stats":{"Line":4}},{"line":88,"address":[],"length":0,"stats":{"Line":4}},{"line":92,"address":[],"length":0,"stats":{"Line":4}},{"line":99,"address":[],"length":0,"stats":{"Line":4}},{"line":102,"address":[],"length":0,"stats":{"Line":4}},{"line":105,"address":[],"length":0,"stats":{"Line":4}},{"line":106,"address":[],"length":0,"stats":{"Line":4}},{"line":109,"address":[],"length":0,"stats":{"Line":0}},{"line":110,"address":[],"length":0,"stats":{"Line":0}}],"covered":29,"coverable":32},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","tests","fixtures.rs"],"content":"// In your test file or test module\n\nuse std::sync::atomic::{AtomicBool, Ordering};\nuse std::time::Duration;\n\nuse serde::{Deserialize, Serialize};\n\n// Assuming your library's root is `twmq` or you use `crate::` if in lib.rs\nuse twmq::hooks::TransactionContext;\nuse twmq::job::{Job, JobResult};\nuse twmq::{DurableExecution, SuccessHookData};\n\n// --- Test Job Definition ---\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct TestJobPayload {\n    pub message: String,\n    pub id_to_check: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]\npub struct TestJobOutput {\n    pub reply: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct TestJobErrorData {\n    pub reason: String,\n}\n\n// Use a static AtomicBool to signal from the job process to the test\n// In a real scenario, you'd check queue state or results in Redis.\npub static TEST_JOB_PROCESSED_SUCCESSFULLY: AtomicBool = AtomicBool::new(false);\n\n// If DurableExecution is in the root of your crate (lib.rs)\n// use crate::DurableExecution;\n\n// If DurableExecution is in job.rs and job.rs is a module\n// use crate::job::DurableExecution;\n\n// If using async_trait:\n// #[async_trait]\nimpl DurableExecution for TestJobPayload {\n    type Output = TestJobOutput;\n    type ErrorData = TestJobErrorData;\n    type ExecutionContext = ();\n\n    // If not using async_trait, the signature is:\n    // fn process(\u0026self) -\u003e impl std::future::Future\u003cOutput = JobResult\u003cSelf::Output, Self::ErrorData\u003e\u003e + Send + Sync {\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        _: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        println!(\n            \"TEST_JOB: Processing job with id_to_check: {}\",\n            job.data.id_to_check\n        );\n        // Simulate some work\n        tokio::time::sleep(Duration::from_millis(50)).await;\n        TEST_JOB_PROCESSED_SUCCESSFULLY.store(true, Ordering::SeqCst);\n        JobResult::Success(TestJobOutput {\n            reply: format!(\"Successfully processed '{}'\", job.data.message),\n        })\n    }\n\n    async fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        _d: SuccessHookData\u003c'_, Self::Output\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\n            \"TEST_JOB: on_success hook for id_to_check: {}\",\n            self.id_to_check\n        );\n    }\n}\n","traces":[{"line":50,"address":[],"length":0,"stats":{"Line":1}},{"line":54,"address":[],"length":0,"stats":{"Line":1}},{"line":55,"address":[],"length":0,"stats":{"Line":1}},{"line":56,"address":[],"length":0,"stats":{"Line":1}},{"line":59,"address":[],"length":0,"stats":{"Line":1}},{"line":60,"address":[],"length":0,"stats":{"Line":1}},{"line":61,"address":[],"length":0,"stats":{"Line":1}},{"line":62,"address":[],"length":0,"stats":{"Line":1}},{"line":66,"address":[],"length":0,"stats":{"Line":1}},{"line":73,"address":[],"length":0,"stats":{"Line":1}},{"line":74,"address":[],"length":0,"stats":{"Line":0}}],"covered":10,"coverable":11},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","tests","lease_expiry.rs"],"content":"use std::sync::{\n    Arc,\n    atomic::{AtomicBool, Ordering},\n};\nuse std::time::Duration;\n\nuse serde::{Deserialize, Serialize};\nuse tracing_subscriber::{EnvFilter, layer::SubscriberExt, util::SubscriberInitExt};\n\nuse twmq::{\n    DurableExecution, FailHookData, NackHookData, Queue, SuccessHookData,\n    hooks::TransactionContext,\n    job::{Job, JobResult, JobStatus},\n    queue::QueueOptions,\n    redis::aio::ConnectionManager,\n};\n\nconst REDIS_URL: \u0026str = \"redis://127.0.0.1:6379/\";\n\n// Helper to clean up Redis keys\nasync fn cleanup_redis_keys(conn_manager: \u0026ConnectionManager, queue_name: \u0026str) {\n    let mut conn = conn_manager.clone();\n    let keys_pattern = format!(\"twmq:{}:*\", queue_name);\n    let keys: Vec\u003cString\u003e = redis::cmd(\"KEYS\")\n        .arg(\u0026keys_pattern)\n        .query_async(\u0026mut conn)\n        .await\n        .unwrap_or_default();\n    if !keys.is_empty() {\n        redis::cmd(\"DEL\")\n            .arg(keys)\n            .query_async::\u003c()\u003e(\u0026mut conn)\n            .await\n            .unwrap_or_default();\n    }\n    tracing::info!(\"Cleaned up keys for pattern: {}\", keys_pattern);\n}\n\n// Job that sleeps forever to test lease expiry\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct SleepForeverJob {\n    pub id_to_check: String,\n    pub message: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]\npub struct SleepJobOutput {\n    pub message: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct SleepJobErrorData {\n    pub reason: String,\n}\n\npub static MULTI_JOB_STARTED_PROCESSING: AtomicBool = AtomicBool::new(false);\npub static MULTI_JOB_SHOULD_CONTINUE_SLEEPING: AtomicBool = AtomicBool::new(true);\n\n#[derive(Clone)]\npub struct SleepForeverJobExecutionContext {\n    pub started_processing: Arc\u003cAtomicBool\u003e,\n    pub should_continue_sleeping: Arc\u003cAtomicBool\u003e,\n}\n\nimpl DurableExecution for SleepForeverJob {\n    type Output = SleepJobOutput;\n    type ErrorData = SleepJobErrorData;\n    type ExecutionContext = SleepForeverJobExecutionContext;\n\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        ec: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        tracing::info!(\n            \"SLEEP_JOB: Starting to process job {}, attempt {}\",\n            job.id,\n            job.attempts\n        );\n\n        // Signal that we started processing\n        ec.started_processing.store(true, Ordering::SeqCst);\n\n        // Sleep forever (or until test tells us to stop)\n        while ec.should_continue_sleeping.load(Ordering::SeqCst) {\n            tokio::time::sleep(Duration::from_millis(100)).await;\n        }\n\n        tracing::info!(\"SLEEP_JOB: Job {} woke up, finishing\", job.id);\n\n        JobResult::Success(SleepJobOutput {\n            message: format!(\"Job {} completed after sleeping\", job.id),\n        })\n    }\n\n    async fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: SuccessHookData\u003c'_, Self::Output\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\"SLEEP_JOB: on_success hook - {}\", d.result.message);\n    }\n\n    async fn on_nack(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: NackHookData\u003c'_, Self::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\"SLEEP_JOB: on_nack hook - {}\", d.error.reason);\n        if let Some(delay_duration) = d.delay {\n            tracing::info!(\"Will retry after {:?}\", delay_duration);\n        }\n    }\n\n    async fn on_fail(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: FailHookData\u003c'_, Self::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::error!(\"SLEEP_JOB: on_fail hook - {}\", d.error.reason);\n    }\n\n    async fn on_timeout(\u0026self, _tx: \u0026mut TransactionContext\u003c'_\u003e) {\n        tracing::info!(\"SLEEP_JOB: on_timeout hook - job lease expired\");\n    }\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_job_lease_expiry() {\n    tracing_subscriber::registry()\n        .with(EnvFilter::try_from_default_env().unwrap_or_else(|_| \"twmq=debug\".into()))\n        .with(tracing_subscriber::fmt::layer())\n        .init();\n\n    let queue_name = format!(\"test_lease_{}\", nanoid::nanoid!(6));\n    let job_id = \"sleep_job_001\";\n    let lease_duration = Duration::from_secs(3); // Short lease for testing\n\n    let job_started_processing = Arc::new(AtomicBool::new(false));\n    let job_should_continue_sleeping = Arc::new(AtomicBool::new(true));\n\n    // Reset flags\n    job_started_processing.store(false, Ordering::SeqCst);\n    job_should_continue_sleeping.store(true, Ordering::SeqCst);\n\n    tracing::info!(\"Creating lease expiry queue: {}\", queue_name);\n    tracing::info!(\"Lease duration: {:?}\", lease_duration);\n\n    // Create queue with short lease duration\n    let queue_options = QueueOptions {\n        max_success: 1000,\n        max_failed: 1000,\n        lease_duration,\n        polling_interval: Duration::from_millis(100),\n        local_concurrency: 1,\n        always_poll: true,\n    };\n\n    let queue =\n        Arc::new(\n            Queue::\u003c\n                SleepForeverJob,\n                SleepJobOutput,\n                SleepJobErrorData,\n                SleepForeverJobExecutionContext,\n            \u003e::new(\n                REDIS_URL,\n                \u0026queue_name,\n                Some(queue_options),\n                SleepForeverJobExecutionContext {\n                    started_processing: job_started_processing.clone(),\n                    should_continue_sleeping: job_should_continue_sleeping.clone(),\n                },\n            )\n            .await\n            .expect(\"Failed to create lease expiry queue\"),\n        );\n\n    // Clean up before test\n    cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n\n    // Create job that will sleep forever\n    let sleep_job = SleepForeverJob {\n        id_to_check: job_id.to_string(),\n        message: \"I will sleep until my lease expires\".to_string(),\n    };\n\n    tracing::info!(\"Pushing sleep job with ID: {}\", job_id);\n    queue\n        .clone()\n        .job(sleep_job)\n        .with_id(job_id)\n        .push()\n        .await\n        .expect(\"Failed to push sleep job\");\n\n    // Verify job is initially pending\n    let initial_pending = queue.count(JobStatus::Pending).await.unwrap();\n    let initial_active = queue.count(JobStatus::Active).await.unwrap();\n    tracing::info!(\n        \"Initial state - Pending: {}, Active: {}\",\n        initial_pending,\n        initial_active\n    );\n    assert_eq!(initial_pending, 1, \"Job should start in pending\");\n    assert_eq!(initial_active, 0, \"No jobs should be active initially\");\n\n    // Start worker with concurrency 1 (so job won't get picked up again immediately after expiry)\n    tracing::info!(\"Starting worker with concurrency 1\");\n    let worker = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            if let Err(e) = queue.work().await {\n                tracing::error!(\"Lease worker failed: {:?}\", e);\n            }\n        })\n    };\n\n    // Wait for job to start processing\n    let mut job_started = false;\n    for i in 0..50 {\n        if job_started_processing.load(Ordering::SeqCst) {\n            job_started = true;\n            tracing::info!(\"Job started processing after {} polling attempts\", i + 1);\n            break;\n        }\n        tokio::time::sleep(Duration::from_millis(100)).await;\n    }\n    assert!(job_started, \"Job should have started processing\");\n\n    // Job should now be active\n    tokio::time::sleep(Duration::from_millis(200)).await;\n    let active_count = queue.count(JobStatus::Active).await.unwrap();\n    let pending_count = queue.count(JobStatus::Pending).await.unwrap();\n    tracing::info!(\n        \"After job start - Pending: {}, Active: {}\",\n        pending_count,\n        active_count\n    );\n    assert_eq!(active_count, 1, \"Job should be active while processing\");\n    assert_eq!(\n        pending_count, 0,\n        \"No jobs should be pending while one is active\"\n    );\n\n    // Wait for lease to expire (lease duration + some buffer)\n    let wait_time = lease_duration + Duration::from_secs(2);\n    tracing::info!(\"Waiting {:?} for lease to expire...\", wait_time);\n    tokio::time::sleep(wait_time).await;\n\n    // Job should have moved back to pending due to lease expiry\n    let expired_active = queue.count(JobStatus::Active).await.unwrap();\n    let expired_pending = queue.count(JobStatus::Pending).await.unwrap();\n    tracing::info!(\n        \"After lease expiry - Pending: {}, Active: {}\",\n        expired_pending,\n        expired_active\n    );\n\n    assert_eq!(\n        expired_active, 0,\n        \"Job should no longer be active after lease expiry\"\n    );\n    assert_eq!(\n        expired_pending, 1,\n        \"Job should be back in pending after lease expiry\"\n    );\n\n    // Verify the job metadata shows increased attempts\n    let job_after_expiry = queue\n        .get_job(job_id)\n        .await\n        .expect(\"Failed to fetch job\")\n        .expect(\"Job should exist\");\n\n    tracing::info!(\"Job state after lease expiry:\");\n    tracing::info!(\"  Job ID: {}\", job_after_expiry.id);\n    tracing::info!(\"  Attempts: {}\", job_after_expiry.attempts);\n    tracing::info!(\"  Created at: {}\", job_after_expiry.created_at);\n    tracing::info!(\"  Processed at: {:?}\", job_after_expiry.processed_at);\n\n    // Job should have at least 2 attempts (original + after lease expiry)\n    assert!(\n        job_after_expiry.attempts \u003e= 2,\n        \"Job should have at least 2 attempts after lease expiry, but had {}\",\n        job_after_expiry.attempts\n    );\n\n    tracing::info!(\"✅ Lease expiry mechanism works correctly!\");\n    tracing::info!(\"Job moved from active back to pending after lease expired\");\n\n    // Stop the sleeping job and cleanup\n    job_should_continue_sleeping.store(false, Ordering::SeqCst);\n    worker.abort();\n    cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_multiple_job_lease_expiry() {\n    // Test that multiple jobs can have their leases expire independently\n\n    let queue_name = format!(\"test_multi_lease_{}\", nanoid::nanoid!(6));\n    let lease_duration = Duration::from_secs(2);\n\n    let job_started_processing = Arc::new(AtomicBool::new(false));\n    let job_should_continue_sleeping = Arc::new(AtomicBool::new(true));\n\n    // Reset flags\n    job_started_processing.store(false, Ordering::SeqCst);\n    job_should_continue_sleeping.store(true, Ordering::SeqCst);\n\n    tracing::info!(\"\\n=== Testing multiple job lease expiry ===\");\n\n    let queue_options = QueueOptions {\n        max_success: 1000,\n        max_failed: 1000,\n        local_concurrency: 3,\n        lease_duration,\n        polling_interval: Duration::from_millis(100),\n        always_poll: true,\n    };\n\n    let queue =\n        Arc::new(\n            Queue::\u003c\n                SleepForeverJob,\n                SleepJobOutput,\n                SleepJobErrorData,\n                SleepForeverJobExecutionContext,\n            \u003e::new(\n                REDIS_URL,\n                \u0026queue_name,\n                Some(queue_options),\n                SleepForeverJobExecutionContext {\n                    started_processing: job_started_processing.clone(),\n                    should_continue_sleeping: job_should_continue_sleeping.clone(),\n                },\n            )\n            .await\n            .expect(\"Failed to create multi-lease queue\"),\n        );\n\n    cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n\n    // Push multiple jobs\n    let job_ids = vec![\"multi_job_1\", \"multi_job_2\", \"multi_job_3\"];\n    for job_id in job_ids {\n        let sleep_job = SleepForeverJob {\n            id_to_check: job_id.to_string(),\n            message: format!(\"Multi-job test: {}\", job_id),\n        };\n\n        queue\n            .clone()\n            .job(sleep_job)\n            .with_id(job_id)\n            .push()\n            .await\n            .expect(\"Failed to push multi-job\");\n    }\n\n    // Start worker with higher concurrency to process multiple jobs\n    let worker = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            if let Err(e) = queue.work().await {\n                tracing::error!(\"Multi-lease worker failed: {:?}\", e);\n            }\n        })\n    };\n\n    // Wait a bit for jobs to start\n    tokio::time::sleep(Duration::from_millis(500)).await;\n\n    // All jobs should be active\n    let active_count = queue.count(JobStatus::Active).await.unwrap();\n    let pending_count = queue.count(JobStatus::Pending).await.unwrap();\n    tracing::info!(\n        \"During processing - Pending: {}, Active: {}\",\n        pending_count,\n        active_count\n    );\n\n    assert_eq!(pending_count + active_count, 3, \"Total jobs should be 3\");\n\n    // Wait for leases to expire\n    let wait_time = lease_duration + Duration::from_secs(1);\n    tracing::info!(\"Waiting {:?} for leases to expire...\", wait_time);\n    tokio::time::sleep(wait_time).await;\n\n    // All jobs should be back to pending\n    let final_active = queue.count(JobStatus::Active).await.unwrap();\n    let final_pending = queue.count(JobStatus::Pending).await.unwrap();\n    tracing::info!(\n        \"After lease expiry - Pending: {}, Active: {}\",\n        final_pending,\n        final_active\n    );\n\n    assert_eq!(\n        final_active, 0,\n        \"No jobs should be active after lease expiry\"\n    );\n    assert_eq!(final_pending, 3, \"All jobs should be back in pending\");\n\n    tracing::info!(\"✅ Multiple job lease expiry works correctly!\");\n\n    // Cleanup\n    job_should_continue_sleeping.store(false, Ordering::SeqCst);\n    worker.abort();\n    cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n}\n","traces":[{"line":21,"address":[],"length":0,"stats":{"Line":8}},{"line":22,"address":[],"length":0,"stats":{"Line":4}},{"line":23,"address":[],"length":0,"stats":{"Line":4}},{"line":24,"address":[],"length":0,"stats":{"Line":8}},{"line":25,"address":[],"length":0,"stats":{"Line":4}},{"line":26,"address":[],"length":0,"stats":{"Line":4}},{"line":27,"address":[],"length":0,"stats":{"Line":4}},{"line":29,"address":[],"length":0,"stats":{"Line":4}},{"line":30,"address":[],"length":0,"stats":{"Line":2}},{"line":31,"address":[],"length":0,"stats":{"Line":2}},{"line":32,"address":[],"length":0,"stats":{"Line":2}},{"line":33,"address":[],"length":0,"stats":{"Line":2}},{"line":36,"address":[],"length":0,"stats":{"Line":4}},{"line":70,"address":[],"length":0,"stats":{"Line":4}},{"line":74,"address":[],"length":0,"stats":{"Line":4}},{"line":75,"address":[],"length":0,"stats":{"Line":0}},{"line":81,"address":[],"length":0,"stats":{"Line":4}},{"line":84,"address":[],"length":0,"stats":{"Line":158}},{"line":85,"address":[],"length":0,"stats":{"Line":158}},{"line":88,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":0}},{"line":102,"address":[],"length":0,"stats":{"Line":0}},{"line":105,"address":[],"length":0,"stats":{"Line":0}},{"line":112,"address":[],"length":0,"stats":{"Line":0}},{"line":113,"address":[],"length":0,"stats":{"Line":0}},{"line":114,"address":[],"length":0,"stats":{"Line":0}},{"line":118,"address":[],"length":0,"stats":{"Line":0}},{"line":125,"address":[],"length":0,"stats":{"Line":0}},{"line":128,"address":[],"length":0,"stats":{"Line":0}},{"line":129,"address":[],"length":0,"stats":{"Line":0}}],"covered":18,"coverable":30},{"path":["/","Users","d4mr","work","thirdweb","engine-core","twmq","tests","nack.rs"],"content":"// tests/nack.rs\n\nuse std::sync::{\n    Arc,\n    atomic::{AtomicBool, Ordering},\n};\nuse std::time::Duration;\n\nuse redis::AsyncCommands;\nuse serde::{Deserialize, Serialize};\nuse tracing_subscriber::{EnvFilter, layer::SubscriberExt, util::SubscriberInitExt};\n\nuse twmq::{\n    DurableExecution, FailHookData, NackHookData, Queue, SuccessHookData,\n    hooks::TransactionContext,\n    job::{Job, JobResult, JobStatus, RequeuePosition},\n    queue::QueueOptions,\n    redis::aio::ConnectionManager,\n};\n\nconst REDIS_URL: \u0026str = \"redis://127.0.0.1:6379/\";\n\n// Helper to clean up Redis keys\nasync fn cleanup_redis_keys(conn_manager: \u0026ConnectionManager, queue_name: \u0026str) {\n    let mut conn = conn_manager.clone();\n    let keys_pattern = format!(\"twmq:{}:*\", queue_name);\n    let keys: Vec\u003cString\u003e = redis::cmd(\"KEYS\")\n        .arg(\u0026keys_pattern)\n        .query_async(\u0026mut conn)\n        .await\n        .unwrap_or_default();\n    if !keys.is_empty() {\n        redis::cmd(\"DEL\")\n            .arg(keys)\n            .query_async::\u003c()\u003e(\u0026mut conn)\n            .await\n            .unwrap_or_default();\n    }\n    tracing::info!(\"Cleaned up keys for pattern: {}\", keys_pattern);\n}\n\n// Job that retries until reaching desired attempts\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct RetryJobPayload {\n    pub id_to_check: String,\n    pub desired_attempts: u32,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]\npub struct RetryJobOutput {\n    pub final_attempt: u32,\n    pub message: String,\n}\n\n#[derive(Serialize, Deserialize, Debug, Clone)]\npub struct RetryJobErrorData {\n    pub attempt: u32,\n    pub reason: String,\n}\n\npub static RETRY_JOB_FINAL_SUCCESS: AtomicBool = AtomicBool::new(false);\n\nimpl DurableExecution for RetryJobPayload {\n    type Output = RetryJobOutput;\n    type ErrorData = RetryJobErrorData;\n    type ExecutionContext = ();\n\n    async fn process(\n        job: \u0026Job\u003cSelf\u003e,\n        _: \u0026Self::ExecutionContext,\n    ) -\u003e JobResult\u003cSelf::Output, Self::ErrorData\u003e {\n        let current_attempt = job.attempts;\n\n        tracing::info!(\n            \"RETRY_JOB: Processing job {}, attempt {}/{}\",\n            job.id,\n            current_attempt,\n            job.data.desired_attempts\n        );\n\n        if current_attempt \u003c job.data.desired_attempts {\n            // Not enough attempts yet, nack it\n            tracing::info!(\n                \"RETRY_JOB: Nacking job {} (attempt {}/{})\",\n                job.id,\n                current_attempt,\n                job.data.desired_attempts\n            );\n\n            JobResult::Nack {\n                error: RetryJobErrorData {\n                    attempt: current_attempt,\n                    reason: format!(\n                        \"Need {} attempts, only at {}\",\n                        job.data.desired_attempts, current_attempt\n                    ),\n                },\n                delay: None,\n                position: RequeuePosition::Last,\n            }\n        } else {\n            // Reached desired attempts, succeed!\n            tracing::info!(\n                \"RETRY_JOB: Success on attempt {}/{}\",\n                current_attempt,\n                job.data.desired_attempts\n            );\n\n            RETRY_JOB_FINAL_SUCCESS.store(true, Ordering::SeqCst);\n\n            JobResult::Success(RetryJobOutput {\n                final_attempt: current_attempt,\n                message: format!(\"Succeeded after {} attempts\", current_attempt),\n            })\n        }\n    }\n\n    async fn on_success(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: SuccessHookData\u003c'_, Self::Output\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\n            \"RETRY_JOB: on_success hook - final attempt was {}\",\n            d.result.final_attempt\n        );\n    }\n\n    async fn on_nack(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: NackHookData\u003c'_, Self::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::info!(\n            \"RETRY_JOB: on_nack hook - attempt {} failed: {}\",\n            d.error.attempt,\n            d.error.reason\n        );\n        if let Some(delay_duration) = d.delay {\n            tracing::info!(\"Will retry after {:?}\", delay_duration);\n        }\n    }\n\n    async fn on_fail(\n        \u0026self,\n        _job: \u0026Job\u003cSelf\u003e,\n        d: FailHookData\u003c'_, Self::ErrorData\u003e,\n        _tx: \u0026mut TransactionContext\u003c'_\u003e,\n        _ec: \u0026Self::ExecutionContext,\n    ) {\n        tracing::error!(\n            \"RETRY_JOB: on_fail hook - permanently failed at attempt {}\",\n            d.error.attempt\n        );\n    }\n\n    async fn on_timeout(\u0026self, _tx: \u0026mut TransactionContext\u003c'_\u003e) {\n        tracing::info!(\"RETRY_JOB: on_timeout hook\");\n    }\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_job_retry_attempts() {\n    tracing_subscriber::registry()\n        .with(\n            EnvFilter::try_from_default_env()\n                .unwrap_or_else(|_| \"thirdweb_engine=debug,tower_http=debug,axum=debug\".into()),\n        )\n        .with(tracing_subscriber::fmt::layer())\n        .init();\n\n    let queue_name = format!(\"test_retry_{}\", nanoid::nanoid!(6));\n    let job_id = \"retry_job_001\";\n    let desired_attempts = 4u32; // Job should nack 3 times, then succeed on 4th attempt\n\n    // Reset counters\n    RETRY_JOB_FINAL_SUCCESS.store(false, Ordering::SeqCst);\n\n    tracing::info!(\"Creating retry queue: {}\", queue_name);\n    tracing::info!(\"Job should succeed after {} attempts\", desired_attempts);\n\n    let mut queue_options = QueueOptions::default();\n    queue_options.local_concurrency = 1;\n\n    let queue = Arc::new(\n        Queue::\u003cRetryJobPayload, RetryJobOutput, RetryJobErrorData, ()\u003e::new(\n            REDIS_URL,\n            \u0026queue_name,\n            Some(queue_options),\n            (),\n        )\n        .await\n        .expect(\"Failed to create retry queue\"),\n    );\n\n    // Clean up before test\n    cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n\n    // Create job that requires multiple attempts\n    let retry_job = RetryJobPayload {\n        id_to_check: job_id.to_string(),\n        desired_attempts,\n    };\n\n    tracing::info!(\"Pushing retry job with ID: {}\", job_id);\n    queue\n        .clone()\n        .job(retry_job)\n        .with_id(job_id)\n        .push()\n        .await\n        .expect(\"Failed to push retry job\");\n\n    // Start worker\n    tracing::info!(\"Starting worker\");\n    let worker = {\n        let queue = queue.clone();\n        tokio::spawn(async move {\n            if let Err(e) = queue.work().await {\n                tracing::error!(\"Retry worker failed: {:?}\", e);\n            }\n        })\n    };\n\n    // Wait for job to complete (with retries)\n    let mut final_success = false;\n    for i in 0..100 {\n        // Give it plenty of time for retries\n        if RETRY_JOB_FINAL_SUCCESS.load(Ordering::SeqCst) {\n            final_success = true;\n            tracing::info!(\n                \"Retry job finally succeeded after {} polling attempts\",\n                i + 1\n            );\n            break;\n        }\n        tokio::time::sleep(Duration::from_millis(200)).await;\n    }\n\n    assert!(final_success, \"Retry job should eventually succeed\");\n\n    // Give a moment for final processing\n    tokio::time::sleep(Duration::from_millis(300)).await;\n\n    // Verify job is in success list\n    let success_count = queue.count(JobStatus::Success).await.unwrap();\n    assert_eq!(success_count, 1, \"Job should be in success list\");\n\n    // Fetch the completed job and verify attempt count\n    let completed_job = queue\n        .get_job(job_id)\n        .await\n        .expect(\"Failed to fetch job\")\n        .expect(\"Job should exist\");\n\n    tracing::info!(\"Final job state:\");\n    tracing::info!(\"  Job ID: {}\", completed_job.id);\n    tracing::info!(\"  Attempts: {}\", completed_job.attempts);\n    tracing::info!(\"  Created at: {}\", completed_job.created_at);\n    tracing::info!(\"  Processed at: {:?}\", completed_job.processed_at);\n    tracing::info!(\"  Finished at: {:?}\", completed_job.finished_at);\n\n    // Verify the job took exactly the desired number of attempts\n    assert_eq!(\n        completed_job.attempts, desired_attempts,\n        \"Job should have exactly {} attempts, but had {}\",\n        desired_attempts, completed_job.attempts\n    );\n\n    // Verify the job result contains the correct attempt count\n    let mut redis_conn = queue.redis.clone();\n    let result_json: Option\u003cString\u003e = redis_conn\n        .hget(queue.job_result_hash_name(), job_id)\n        .await\n        .expect(\"Failed to get job result\");\n\n    assert!(result_json.is_some(), \"Job result should be stored\");\n\n    let job_output: RetryJobOutput =\n        serde_json::from_str(\u0026result_json.unwrap()).expect(\"Failed to deserialize job result\");\n\n    assert_eq!(\n        job_output.final_attempt, desired_attempts,\n        \"Job result should show final attempt as {}\",\n        desired_attempts\n    );\n\n    tracing::info!(\"✅ Retry mechanism works correctly!\");\n    tracing::info!(\n        \"Job succeeded on attempt {} as expected\",\n        job_output.final_attempt\n    );\n    tracing::info!(\"Result message: {}\", job_output.message);\n\n    // Cleanup\n    worker.abort();\n    cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n}\n\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\nasync fn test_different_retry_counts() {\n    // Test multiple different retry counts to ensure it works consistently\n    let test_cases = vec![1, 2, 3, 5, 7];\n\n    for desired_attempts in test_cases {\n        tracing::info!(\"\\n=== Testing {} attempts ===\", desired_attempts);\n\n        let queue_name = format!(\"test_retry_{}_{}\", desired_attempts, nanoid::nanoid!(4));\n        let job_id = format!(\"retry_job_{}\", desired_attempts);\n\n        // Reset counters\n        RETRY_JOB_FINAL_SUCCESS.store(false, Ordering::SeqCst);\n\n        let mut queue_options = QueueOptions::default();\n        queue_options.local_concurrency = 1;\n\n        let queue = Arc::new(\n            Queue::\u003cRetryJobPayload, RetryJobOutput, RetryJobErrorData, ()\u003e::new(\n                REDIS_URL,\n                \u0026queue_name,\n                Some(queue_options),\n                (),\n            )\n            .await\n            .expect(\"Failed to create retry queue\"),\n        );\n\n        cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n\n        let retry_job = RetryJobPayload {\n            id_to_check: job_id.clone(),\n            desired_attempts,\n        };\n\n        queue\n            .clone()\n            .job(retry_job)\n            .with_id(\u0026job_id)\n            .push()\n            .await\n            .expect(\"Failed to push retry job\");\n\n        let worker = {\n            let queue = queue.clone();\n            tokio::spawn(async move {\n                if let Err(e) = queue.work().await {\n                    tracing::error!(\"Worker failed: {:?}\", e);\n                }\n            })\n        };\n\n        // Wait for completion\n        for _ in 0..50 {\n            if RETRY_JOB_FINAL_SUCCESS.load(Ordering::SeqCst) {\n                break;\n            }\n            tokio::time::sleep(Duration::from_millis(200)).await;\n        }\n\n        tokio::time::sleep(Duration::from_millis(200)).await;\n\n        let completed_job = queue.get_job(\u0026job_id).await.unwrap().unwrap();\n        assert_eq!(completed_job.attempts, desired_attempts);\n\n        worker.abort();\n        cleanup_redis_keys(\u0026queue.redis, \u0026queue_name).await;\n\n        tracing::info!(\"✅ {} attempts test passed\", desired_attempts);\n    }\n\n    tracing::info!(\"\\n✅ All retry count tests passed!\");\n}\n","traces":[{"line":24,"address":[],"length":0,"stats":{"Line":24}},{"line":25,"address":[],"length":0,"stats":{"Line":12}},{"line":26,"address":[],"length":0,"stats":{"Line":12}},{"line":27,"address":[],"length":0,"stats":{"Line":24}},{"line":28,"address":[],"length":0,"stats":{"Line":12}},{"line":29,"address":[],"length":0,"stats":{"Line":12}},{"line":30,"address":[],"length":0,"stats":{"Line":12}},{"line":32,"address":[],"length":0,"stats":{"Line":12}},{"line":33,"address":[],"length":0,"stats":{"Line":6}},{"line":34,"address":[],"length":0,"stats":{"Line":6}},{"line":35,"address":[],"length":0,"stats":{"Line":6}},{"line":36,"address":[],"length":0,"stats":{"Line":6}},{"line":39,"address":[],"length":0,"stats":{"Line":12}},{"line":68,"address":[],"length":0,"stats":{"Line":22}},{"line":72,"address":[],"length":0,"stats":{"Line":22}},{"line":74,"address":[],"length":0,"stats":{"Line":22}},{"line":75,"address":[],"length":0,"stats":{"Line":0}},{"line":81,"address":[],"length":0,"stats":{"Line":22}},{"line":83,"address":[],"length":0,"stats":{"Line":16}},{"line":84,"address":[],"length":0,"stats":{"Line":0}},{"line":91,"address":[],"length":0,"stats":{"Line":16}},{"line":103,"address":[],"length":0,"stats":{"Line":6}},{"line":104,"address":[],"length":0,"stats":{"Line":0}},{"line":118,"address":[],"length":0,"stats":{"Line":6}},{"line":125,"address":[],"length":0,"stats":{"Line":6}},{"line":126,"address":[],"length":0,"stats":{"Line":0}},{"line":131,"address":[],"length":0,"stats":{"Line":16}},{"line":138,"address":[],"length":0,"stats":{"Line":16}},{"line":139,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":16}},{"line":144,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":162,"address":[],"length":0,"stats":{"Line":0}}],"covered":25,"coverable":36}]};
    </script>
    <script crossorigin>/** @license React v16.13.1
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */
'use strict';(function(d,r){"object"===typeof exports&&"undefined"!==typeof module?r(exports):"function"===typeof define&&define.amd?define(["exports"],r):(d=d||self,r(d.React={}))})(this,function(d){function r(a){for(var b="https://reactjs.org/docs/error-decoder.html?invariant="+a,c=1;c<arguments.length;c++)b+="&args[]="+encodeURIComponent(arguments[c]);return"Minified React error #"+a+"; visit "+b+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}
function w(a,b,c){this.props=a;this.context=b;this.refs=ba;this.updater=c||ca}function da(){}function L(a,b,c){this.props=a;this.context=b;this.refs=ba;this.updater=c||ca}function ea(a,b,c){var g,e={},fa=null,d=null;if(null!=b)for(g in void 0!==b.ref&&(d=b.ref),void 0!==b.key&&(fa=""+b.key),b)ha.call(b,g)&&!ia.hasOwnProperty(g)&&(e[g]=b[g]);var h=arguments.length-2;if(1===h)e.children=c;else if(1<h){for(var k=Array(h),f=0;f<h;f++)k[f]=arguments[f+2];e.children=k}if(a&&a.defaultProps)for(g in h=a.defaultProps,
h)void 0===e[g]&&(e[g]=h[g]);return{$$typeof:x,type:a,key:fa,ref:d,props:e,_owner:M.current}}function va(a,b){return{$$typeof:x,type:a.type,key:b,ref:a.ref,props:a.props,_owner:a._owner}}function N(a){return"object"===typeof a&&null!==a&&a.$$typeof===x}function wa(a){var b={"=":"=0",":":"=2"};return"$"+(""+a).replace(/[=:]/g,function(a){return b[a]})}function ja(a,b,c,g){if(C.length){var e=C.pop();e.result=a;e.keyPrefix=b;e.func=c;e.context=g;e.count=0;return e}return{result:a,keyPrefix:b,func:c,
context:g,count:0}}function ka(a){a.result=null;a.keyPrefix=null;a.func=null;a.context=null;a.count=0;10>C.length&&C.push(a)}function O(a,b,c,g){var e=typeof a;if("undefined"===e||"boolean"===e)a=null;var d=!1;if(null===a)d=!0;else switch(e){case "string":case "number":d=!0;break;case "object":switch(a.$$typeof){case x:case xa:d=!0}}if(d)return c(g,a,""===b?"."+P(a,0):b),1;d=0;b=""===b?".":b+":";if(Array.isArray(a))for(var f=0;f<a.length;f++){e=a[f];var h=b+P(e,f);d+=O(e,h,c,g)}else if(null===a||
"object"!==typeof a?h=null:(h=la&&a[la]||a["@@iterator"],h="function"===typeof h?h:null),"function"===typeof h)for(a=h.call(a),f=0;!(e=a.next()).done;)e=e.value,h=b+P(e,f++),d+=O(e,h,c,g);else if("object"===e)throw c=""+a,Error(r(31,"[object Object]"===c?"object with keys {"+Object.keys(a).join(", ")+"}":c,""));return d}function Q(a,b,c){return null==a?0:O(a,"",b,c)}function P(a,b){return"object"===typeof a&&null!==a&&null!=a.key?wa(a.key):b.toString(36)}function ya(a,b,c){a.func.call(a.context,b,
a.count++)}function za(a,b,c){var g=a.result,e=a.keyPrefix;a=a.func.call(a.context,b,a.count++);Array.isArray(a)?R(a,g,c,function(a){return a}):null!=a&&(N(a)&&(a=va(a,e+(!a.key||b&&b.key===a.key?"":(""+a.key).replace(ma,"$&/")+"/")+c)),g.push(a))}function R(a,b,c,g,e){var d="";null!=c&&(d=(""+c).replace(ma,"$&/")+"/");b=ja(b,d,g,e);Q(a,za,b);ka(b)}function t(){var a=na.current;if(null===a)throw Error(r(321));return a}function S(a,b){var c=a.length;a.push(b);a:for(;;){var g=c-1>>>1,e=a[g];if(void 0!==
e&&0<D(e,b))a[g]=b,a[c]=e,c=g;else break a}}function n(a){a=a[0];return void 0===a?null:a}function E(a){var b=a[0];if(void 0!==b){var c=a.pop();if(c!==b){a[0]=c;a:for(var g=0,e=a.length;g<e;){var d=2*(g+1)-1,f=a[d],h=d+1,k=a[h];if(void 0!==f&&0>D(f,c))void 0!==k&&0>D(k,f)?(a[g]=k,a[h]=c,g=h):(a[g]=f,a[d]=c,g=d);else if(void 0!==k&&0>D(k,c))a[g]=k,a[h]=c,g=h;else break a}}return b}return null}function D(a,b){var c=a.sortIndex-b.sortIndex;return 0!==c?c:a.id-b.id}function F(a){for(var b=n(u);null!==
b;){if(null===b.callback)E(u);else if(b.startTime<=a)E(u),b.sortIndex=b.expirationTime,S(p,b);else break;b=n(u)}}function T(a){y=!1;F(a);if(!v)if(null!==n(p))v=!0,z(U);else{var b=n(u);null!==b&&G(T,b.startTime-a)}}function U(a,b){v=!1;y&&(y=!1,V());H=!0;var c=m;try{F(b);for(l=n(p);null!==l&&(!(l.expirationTime>b)||a&&!W());){var g=l.callback;if(null!==g){l.callback=null;m=l.priorityLevel;var e=g(l.expirationTime<=b);b=q();"function"===typeof e?l.callback=e:l===n(p)&&E(p);F(b)}else E(p);l=n(p)}if(null!==
l)var d=!0;else{var f=n(u);null!==f&&G(T,f.startTime-b);d=!1}return d}finally{l=null,m=c,H=!1}}function oa(a){switch(a){case 1:return-1;case 2:return 250;case 5:return 1073741823;case 4:return 1E4;default:return 5E3}}var f="function"===typeof Symbol&&Symbol.for,x=f?Symbol.for("react.element"):60103,xa=f?Symbol.for("react.portal"):60106,Aa=f?Symbol.for("react.fragment"):60107,Ba=f?Symbol.for("react.strict_mode"):60108,Ca=f?Symbol.for("react.profiler"):60114,Da=f?Symbol.for("react.provider"):60109,
Ea=f?Symbol.for("react.context"):60110,Fa=f?Symbol.for("react.forward_ref"):60112,Ga=f?Symbol.for("react.suspense"):60113,Ha=f?Symbol.for("react.memo"):60115,Ia=f?Symbol.for("react.lazy"):60116,la="function"===typeof Symbol&&Symbol.iterator,pa=Object.getOwnPropertySymbols,Ja=Object.prototype.hasOwnProperty,Ka=Object.prototype.propertyIsEnumerable,I=function(){try{if(!Object.assign)return!1;var a=new String("abc");a[5]="de";if("5"===Object.getOwnPropertyNames(a)[0])return!1;var b={};for(a=0;10>a;a++)b["_"+
String.fromCharCode(a)]=a;if("0123456789"!==Object.getOwnPropertyNames(b).map(function(a){return b[a]}).join(""))return!1;var c={};"abcdefghijklmnopqrst".split("").forEach(function(a){c[a]=a});return"abcdefghijklmnopqrst"!==Object.keys(Object.assign({},c)).join("")?!1:!0}catch(g){return!1}}()?Object.assign:function(a,b){if(null===a||void 0===a)throw new TypeError("Object.assign cannot be called with null or undefined");var c=Object(a);for(var g,e=1;e<arguments.length;e++){var d=Object(arguments[e]);
for(var f in d)Ja.call(d,f)&&(c[f]=d[f]);if(pa){g=pa(d);for(var h=0;h<g.length;h++)Ka.call(d,g[h])&&(c[g[h]]=d[g[h]])}}return c},ca={isMounted:function(a){return!1},enqueueForceUpdate:function(a,b,c){},enqueueReplaceState:function(a,b,c,d){},enqueueSetState:function(a,b,c,d){}},ba={};w.prototype.isReactComponent={};w.prototype.setState=function(a,b){if("object"!==typeof a&&"function"!==typeof a&&null!=a)throw Error(r(85));this.updater.enqueueSetState(this,a,b,"setState")};w.prototype.forceUpdate=
function(a){this.updater.enqueueForceUpdate(this,a,"forceUpdate")};da.prototype=w.prototype;f=L.prototype=new da;f.constructor=L;I(f,w.prototype);f.isPureReactComponent=!0;var M={current:null},ha=Object.prototype.hasOwnProperty,ia={key:!0,ref:!0,__self:!0,__source:!0},ma=/\/+/g,C=[],na={current:null},X;if("undefined"===typeof window||"function"!==typeof MessageChannel){var A=null,qa=null,ra=function(){if(null!==A)try{var a=q();A(!0,a);A=null}catch(b){throw setTimeout(ra,0),b;}},La=Date.now();var q=
function(){return Date.now()-La};var z=function(a){null!==A?setTimeout(z,0,a):(A=a,setTimeout(ra,0))};var G=function(a,b){qa=setTimeout(a,b)};var V=function(){clearTimeout(qa)};var W=function(){return!1};f=X=function(){}}else{var Y=window.performance,sa=window.Date,Ma=window.setTimeout,Na=window.clearTimeout;"undefined"!==typeof console&&(f=window.cancelAnimationFrame,"function"!==typeof window.requestAnimationFrame&&console.error("This browser doesn't support requestAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills"),
"function"!==typeof f&&console.error("This browser doesn't support cancelAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills"));if("object"===typeof Y&&"function"===typeof Y.now)q=function(){return Y.now()};else{var Oa=sa.now();q=function(){return sa.now()-Oa}}var J=!1,K=null,Z=-1,ta=5,ua=0;W=function(){return q()>=ua};f=function(){};X=function(a){0>a||125<a?console.error("forceFrameRate takes a positive int between 0 and 125, forcing framerates higher than 125 fps is not unsupported"):
ta=0<a?Math.floor(1E3/a):5};var B=new MessageChannel,aa=B.port2;B.port1.onmessage=function(){if(null!==K){var a=q();ua=a+ta;try{K(!0,a)?aa.postMessage(null):(J=!1,K=null)}catch(b){throw aa.postMessage(null),b;}}else J=!1};z=function(a){K=a;J||(J=!0,aa.postMessage(null))};G=function(a,b){Z=Ma(function(){a(q())},b)};V=function(){Na(Z);Z=-1}}var p=[],u=[],Pa=1,l=null,m=3,H=!1,v=!1,y=!1,Qa=0;B={ReactCurrentDispatcher:na,ReactCurrentOwner:M,IsSomeRendererActing:{current:!1},assign:I};I(B,{Scheduler:{__proto__:null,
unstable_ImmediatePriority:1,unstable_UserBlockingPriority:2,unstable_NormalPriority:3,unstable_IdlePriority:5,unstable_LowPriority:4,unstable_runWithPriority:function(a,b){switch(a){case 1:case 2:case 3:case 4:case 5:break;default:a=3}var c=m;m=a;try{return b()}finally{m=c}},unstable_next:function(a){switch(m){case 1:case 2:case 3:var b=3;break;default:b=m}var c=m;m=b;try{return a()}finally{m=c}},unstable_scheduleCallback:function(a,b,c){var d=q();if("object"===typeof c&&null!==c){var e=c.delay;
e="number"===typeof e&&0<e?d+e:d;c="number"===typeof c.timeout?c.timeout:oa(a)}else c=oa(a),e=d;c=e+c;a={id:Pa++,callback:b,priorityLevel:a,startTime:e,expirationTime:c,sortIndex:-1};e>d?(a.sortIndex=e,S(u,a),null===n(p)&&a===n(u)&&(y?V():y=!0,G(T,e-d))):(a.sortIndex=c,S(p,a),v||H||(v=!0,z(U)));return a},unstable_cancelCallback:function(a){a.callback=null},unstable_wrapCallback:function(a){var b=m;return function(){var c=m;m=b;try{return a.apply(this,arguments)}finally{m=c}}},unstable_getCurrentPriorityLevel:function(){return m},
unstable_shouldYield:function(){var a=q();F(a);var b=n(p);return b!==l&&null!==l&&null!==b&&null!==b.callback&&b.startTime<=a&&b.expirationTime<l.expirationTime||W()},unstable_requestPaint:f,unstable_continueExecution:function(){v||H||(v=!0,z(U))},unstable_pauseExecution:function(){},unstable_getFirstCallbackNode:function(){return n(p)},get unstable_now(){return q},get unstable_forceFrameRate(){return X},unstable_Profiling:null},SchedulerTracing:{__proto__:null,__interactionsRef:null,__subscriberRef:null,
unstable_clear:function(a){return a()},unstable_getCurrent:function(){return null},unstable_getThreadID:function(){return++Qa},unstable_trace:function(a,b,c){return c()},unstable_wrap:function(a){return a},unstable_subscribe:function(a){},unstable_unsubscribe:function(a){}}});d.Children={map:function(a,b,c){if(null==a)return a;var d=[];R(a,d,null,b,c);return d},forEach:function(a,b,c){if(null==a)return a;b=ja(null,null,b,c);Q(a,ya,b);ka(b)},count:function(a){return Q(a,function(){return null},null)},
toArray:function(a){var b=[];R(a,b,null,function(a){return a});return b},only:function(a){if(!N(a))throw Error(r(143));return a}};d.Component=w;d.Fragment=Aa;d.Profiler=Ca;d.PureComponent=L;d.StrictMode=Ba;d.Suspense=Ga;d.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=B;d.cloneElement=function(a,b,c){if(null===a||void 0===a)throw Error(r(267,a));var d=I({},a.props),e=a.key,f=a.ref,m=a._owner;if(null!=b){void 0!==b.ref&&(f=b.ref,m=M.current);void 0!==b.key&&(e=""+b.key);if(a.type&&a.type.defaultProps)var h=
a.type.defaultProps;for(k in b)ha.call(b,k)&&!ia.hasOwnProperty(k)&&(d[k]=void 0===b[k]&&void 0!==h?h[k]:b[k])}var k=arguments.length-2;if(1===k)d.children=c;else if(1<k){h=Array(k);for(var l=0;l<k;l++)h[l]=arguments[l+2];d.children=h}return{$$typeof:x,type:a.type,key:e,ref:f,props:d,_owner:m}};d.createContext=function(a,b){void 0===b&&(b=null);a={$$typeof:Ea,_calculateChangedBits:b,_currentValue:a,_currentValue2:a,_threadCount:0,Provider:null,Consumer:null};a.Provider={$$typeof:Da,_context:a};return a.Consumer=
a};d.createElement=ea;d.createFactory=function(a){var b=ea.bind(null,a);b.type=a;return b};d.createRef=function(){return{current:null}};d.forwardRef=function(a){return{$$typeof:Fa,render:a}};d.isValidElement=N;d.lazy=function(a){return{$$typeof:Ia,_ctor:a,_status:-1,_result:null}};d.memo=function(a,b){return{$$typeof:Ha,type:a,compare:void 0===b?null:b}};d.useCallback=function(a,b){return t().useCallback(a,b)};d.useContext=function(a,b){return t().useContext(a,b)};d.useDebugValue=function(a,b){};
d.useEffect=function(a,b){return t().useEffect(a,b)};d.useImperativeHandle=function(a,b,c){return t().useImperativeHandle(a,b,c)};d.useLayoutEffect=function(a,b){return t().useLayoutEffect(a,b)};d.useMemo=function(a,b){return t().useMemo(a,b)};d.useReducer=function(a,b,c){return t().useReducer(a,b,c)};d.useRef=function(a){return t().useRef(a)};d.useState=function(a){return t().useState(a)};d.version="16.13.1"});
</script>
    <script crossorigin>/** @license React v16.13.1
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */
/*
 Modernizr 3.0.0pre (Custom Build) | MIT
*/
'use strict';(function(I,ea){"object"===typeof exports&&"undefined"!==typeof module?ea(exports,require("react")):"function"===typeof define&&define.amd?define(["exports","react"],ea):(I=I||self,ea(I.ReactDOM={},I.React))})(this,function(I,ea){function k(a){for(var b="https://reactjs.org/docs/error-decoder.html?invariant="+a,c=1;c<arguments.length;c++)b+="&args[]="+encodeURIComponent(arguments[c]);return"Minified React error #"+a+"; visit "+b+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}
function ji(a,b,c,d,e,f,g,h,m){yb=!1;gc=null;ki.apply(li,arguments)}function mi(a,b,c,d,e,f,g,h,m){ji.apply(this,arguments);if(yb){if(yb){var n=gc;yb=!1;gc=null}else throw Error(k(198));hc||(hc=!0,pd=n)}}function lf(a,b,c){var d=a.type||"unknown-event";a.currentTarget=mf(c);mi(d,b,void 0,a);a.currentTarget=null}function nf(){if(ic)for(var a in cb){var b=cb[a],c=ic.indexOf(a);if(!(-1<c))throw Error(k(96,a));if(!jc[c]){if(!b.extractEvents)throw Error(k(97,a));jc[c]=b;c=b.eventTypes;for(var d in c){var e=
void 0;var f=c[d],g=b,h=d;if(qd.hasOwnProperty(h))throw Error(k(99,h));qd[h]=f;var m=f.phasedRegistrationNames;if(m){for(e in m)m.hasOwnProperty(e)&&of(m[e],g,h);e=!0}else f.registrationName?(of(f.registrationName,g,h),e=!0):e=!1;if(!e)throw Error(k(98,d,a));}}}}function of(a,b,c){if(db[a])throw Error(k(100,a));db[a]=b;rd[a]=b.eventTypes[c].dependencies}function pf(a){var b=!1,c;for(c in a)if(a.hasOwnProperty(c)){var d=a[c];if(!cb.hasOwnProperty(c)||cb[c]!==d){if(cb[c])throw Error(k(102,c));cb[c]=
d;b=!0}}b&&nf()}function qf(a){if(a=rf(a)){if("function"!==typeof sd)throw Error(k(280));var b=a.stateNode;b&&(b=td(b),sd(a.stateNode,a.type,b))}}function sf(a){eb?fb?fb.push(a):fb=[a]:eb=a}function tf(){if(eb){var a=eb,b=fb;fb=eb=null;qf(a);if(b)for(a=0;a<b.length;a++)qf(b[a])}}function ud(){if(null!==eb||null!==fb)vd(),tf()}function uf(a,b,c){if(wd)return a(b,c);wd=!0;try{return vf(a,b,c)}finally{wd=!1,ud()}}function ni(a){if(wf.call(xf,a))return!0;if(wf.call(yf,a))return!1;if(oi.test(a))return xf[a]=
!0;yf[a]=!0;return!1}function pi(a,b,c,d){if(null!==c&&0===c.type)return!1;switch(typeof b){case "function":case "symbol":return!0;case "boolean":if(d)return!1;if(null!==c)return!c.acceptsBooleans;a=a.toLowerCase().slice(0,5);return"data-"!==a&&"aria-"!==a;default:return!1}}function qi(a,b,c,d){if(null===b||"undefined"===typeof b||pi(a,b,c,d))return!0;if(d)return!1;if(null!==c)switch(c.type){case 3:return!b;case 4:return!1===b;case 5:return isNaN(b);case 6:return isNaN(b)||1>b}return!1}function L(a,
b,c,d,e,f){this.acceptsBooleans=2===b||3===b||4===b;this.attributeName=d;this.attributeNamespace=e;this.mustUseProperty=c;this.propertyName=a;this.type=b;this.sanitizeURL=f}function xd(a,b,c,d){var e=E.hasOwnProperty(b)?E[b]:null;var f=null!==e?0===e.type:d?!1:!(2<b.length)||"o"!==b[0]&&"O"!==b[0]||"n"!==b[1]&&"N"!==b[1]?!1:!0;f||(qi(b,c,e,d)&&(c=null),d||null===e?ni(b)&&(null===c?a.removeAttribute(b):a.setAttribute(b,""+c)):e.mustUseProperty?a[e.propertyName]=null===c?3===e.type?!1:"":c:(b=e.attributeName,
d=e.attributeNamespace,null===c?a.removeAttribute(b):(e=e.type,c=3===e||4===e&&!0===c?"":""+c,d?a.setAttributeNS(d,b,c):a.setAttribute(b,c))))}function zb(a){if(null===a||"object"!==typeof a)return null;a=zf&&a[zf]||a["@@iterator"];return"function"===typeof a?a:null}function ri(a){if(-1===a._status){a._status=0;var b=a._ctor;b=b();a._result=b;b.then(function(b){0===a._status&&(b=b.default,a._status=1,a._result=b)},function(b){0===a._status&&(a._status=2,a._result=b)})}}function na(a){if(null==a)return null;
if("function"===typeof a)return a.displayName||a.name||null;if("string"===typeof a)return a;switch(a){case Ma:return"Fragment";case gb:return"Portal";case kc:return"Profiler";case Af:return"StrictMode";case lc:return"Suspense";case yd:return"SuspenseList"}if("object"===typeof a)switch(a.$$typeof){case Bf:return"Context.Consumer";case Cf:return"Context.Provider";case zd:var b=a.render;b=b.displayName||b.name||"";return a.displayName||(""!==b?"ForwardRef("+b+")":"ForwardRef");case Ad:return na(a.type);
case Df:return na(a.render);case Ef:if(a=1===a._status?a._result:null)return na(a)}return null}function Bd(a){var b="";do{a:switch(a.tag){case 3:case 4:case 6:case 7:case 10:case 9:var c="";break a;default:var d=a._debugOwner,e=a._debugSource,f=na(a.type);c=null;d&&(c=na(d.type));d=f;f="";e?f=" (at "+e.fileName.replace(si,"")+":"+e.lineNumber+")":c&&(f=" (created by "+c+")");c="\n    in "+(d||"Unknown")+f}b+=c;a=a.return}while(a);return b}function va(a){switch(typeof a){case "boolean":case "number":case "object":case "string":case "undefined":return a;
default:return""}}function Ff(a){var b=a.type;return(a=a.nodeName)&&"input"===a.toLowerCase()&&("checkbox"===b||"radio"===b)}function ti(a){var b=Ff(a)?"checked":"value",c=Object.getOwnPropertyDescriptor(a.constructor.prototype,b),d=""+a[b];if(!a.hasOwnProperty(b)&&"undefined"!==typeof c&&"function"===typeof c.get&&"function"===typeof c.set){var e=c.get,f=c.set;Object.defineProperty(a,b,{configurable:!0,get:function(){return e.call(this)},set:function(a){d=""+a;f.call(this,a)}});Object.defineProperty(a,
b,{enumerable:c.enumerable});return{getValue:function(){return d},setValue:function(a){d=""+a},stopTracking:function(){a._valueTracker=null;delete a[b]}}}}function mc(a){a._valueTracker||(a._valueTracker=ti(a))}function Gf(a){if(!a)return!1;var b=a._valueTracker;if(!b)return!0;var c=b.getValue();var d="";a&&(d=Ff(a)?a.checked?"true":"false":a.value);a=d;return a!==c?(b.setValue(a),!0):!1}function Cd(a,b){var c=b.checked;return M({},b,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:null!=
c?c:a._wrapperState.initialChecked})}function Hf(a,b){var c=null==b.defaultValue?"":b.defaultValue,d=null!=b.checked?b.checked:b.defaultChecked;c=va(null!=b.value?b.value:c);a._wrapperState={initialChecked:d,initialValue:c,controlled:"checkbox"===b.type||"radio"===b.type?null!=b.checked:null!=b.value}}function If(a,b){b=b.checked;null!=b&&xd(a,"checked",b,!1)}function Dd(a,b){If(a,b);var c=va(b.value),d=b.type;if(null!=c)if("number"===d){if(0===c&&""===a.value||a.value!=c)a.value=""+c}else a.value!==
""+c&&(a.value=""+c);else if("submit"===d||"reset"===d){a.removeAttribute("value");return}b.hasOwnProperty("value")?Ed(a,b.type,c):b.hasOwnProperty("defaultValue")&&Ed(a,b.type,va(b.defaultValue));null==b.checked&&null!=b.defaultChecked&&(a.defaultChecked=!!b.defaultChecked)}function Jf(a,b,c){if(b.hasOwnProperty("value")||b.hasOwnProperty("defaultValue")){var d=b.type;if(!("submit"!==d&&"reset"!==d||void 0!==b.value&&null!==b.value))return;b=""+a._wrapperState.initialValue;c||b===a.value||(a.value=
b);a.defaultValue=b}c=a.name;""!==c&&(a.name="");a.defaultChecked=!!a._wrapperState.initialChecked;""!==c&&(a.name=c)}function Ed(a,b,c){if("number"!==b||a.ownerDocument.activeElement!==a)null==c?a.defaultValue=""+a._wrapperState.initialValue:a.defaultValue!==""+c&&(a.defaultValue=""+c)}function ui(a){var b="";ea.Children.forEach(a,function(a){null!=a&&(b+=a)});return b}function Fd(a,b){a=M({children:void 0},b);if(b=ui(b.children))a.children=b;return a}function hb(a,b,c,d){a=a.options;if(b){b={};
for(var e=0;e<c.length;e++)b["$"+c[e]]=!0;for(c=0;c<a.length;c++)e=b.hasOwnProperty("$"+a[c].value),a[c].selected!==e&&(a[c].selected=e),e&&d&&(a[c].defaultSelected=!0)}else{c=""+va(c);b=null;for(e=0;e<a.length;e++){if(a[e].value===c){a[e].selected=!0;d&&(a[e].defaultSelected=!0);return}null!==b||a[e].disabled||(b=a[e])}null!==b&&(b.selected=!0)}}function Gd(a,b){if(null!=b.dangerouslySetInnerHTML)throw Error(k(91));return M({},b,{value:void 0,defaultValue:void 0,children:""+a._wrapperState.initialValue})}
function Kf(a,b){var c=b.value;if(null==c){c=b.children;b=b.defaultValue;if(null!=c){if(null!=b)throw Error(k(92));if(Array.isArray(c)){if(!(1>=c.length))throw Error(k(93));c=c[0]}b=c}null==b&&(b="");c=b}a._wrapperState={initialValue:va(c)}}function Lf(a,b){var c=va(b.value),d=va(b.defaultValue);null!=c&&(c=""+c,c!==a.value&&(a.value=c),null==b.defaultValue&&a.defaultValue!==c&&(a.defaultValue=c));null!=d&&(a.defaultValue=""+d)}function Mf(a,b){b=a.textContent;b===a._wrapperState.initialValue&&""!==
b&&null!==b&&(a.value=b)}function Nf(a){switch(a){case "svg":return"http://www.w3.org/2000/svg";case "math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function Hd(a,b){return null==a||"http://www.w3.org/1999/xhtml"===a?Nf(b):"http://www.w3.org/2000/svg"===a&&"foreignObject"===b?"http://www.w3.org/1999/xhtml":a}function nc(a,b){var c={};c[a.toLowerCase()]=b.toLowerCase();c["Webkit"+a]="webkit"+b;c["Moz"+a]="moz"+b;return c}function oc(a){if(Id[a])return Id[a];
if(!ib[a])return a;var b=ib[a],c;for(c in b)if(b.hasOwnProperty(c)&&c in Of)return Id[a]=b[c];return a}function Jd(a){var b=Pf.get(a);void 0===b&&(b=new Map,Pf.set(a,b));return b}function Na(a){var b=a,c=a;if(a.alternate)for(;b.return;)b=b.return;else{a=b;do b=a,0!==(b.effectTag&1026)&&(c=b.return),a=b.return;while(a)}return 3===b.tag?c:null}function Qf(a){if(13===a.tag){var b=a.memoizedState;null===b&&(a=a.alternate,null!==a&&(b=a.memoizedState));if(null!==b)return b.dehydrated}return null}function Rf(a){if(Na(a)!==
a)throw Error(k(188));}function vi(a){var b=a.alternate;if(!b){b=Na(a);if(null===b)throw Error(k(188));return b!==a?null:a}for(var c=a,d=b;;){var e=c.return;if(null===e)break;var f=e.alternate;if(null===f){d=e.return;if(null!==d){c=d;continue}break}if(e.child===f.child){for(f=e.child;f;){if(f===c)return Rf(e),a;if(f===d)return Rf(e),b;f=f.sibling}throw Error(k(188));}if(c.return!==d.return)c=e,d=f;else{for(var g=!1,h=e.child;h;){if(h===c){g=!0;c=e;d=f;break}if(h===d){g=!0;d=e;c=f;break}h=h.sibling}if(!g){for(h=
f.child;h;){if(h===c){g=!0;c=f;d=e;break}if(h===d){g=!0;d=f;c=e;break}h=h.sibling}if(!g)throw Error(k(189));}}if(c.alternate!==d)throw Error(k(190));}if(3!==c.tag)throw Error(k(188));return c.stateNode.current===c?a:b}function Sf(a){a=vi(a);if(!a)return null;for(var b=a;;){if(5===b.tag||6===b.tag)return b;if(b.child)b.child.return=b,b=b.child;else{if(b===a)break;for(;!b.sibling;){if(!b.return||b.return===a)return null;b=b.return}b.sibling.return=b.return;b=b.sibling}}return null}function jb(a,b){if(null==
b)throw Error(k(30));if(null==a)return b;if(Array.isArray(a)){if(Array.isArray(b))return a.push.apply(a,b),a;a.push(b);return a}return Array.isArray(b)?[a].concat(b):[a,b]}function Kd(a,b,c){Array.isArray(a)?a.forEach(b,c):a&&b.call(c,a)}function pc(a){null!==a&&(Ab=jb(Ab,a));a=Ab;Ab=null;if(a){Kd(a,wi);if(Ab)throw Error(k(95));if(hc)throw a=pd,hc=!1,pd=null,a;}}function Ld(a){a=a.target||a.srcElement||window;a.correspondingUseElement&&(a=a.correspondingUseElement);return 3===a.nodeType?a.parentNode:
a}function Tf(a){if(!wa)return!1;a="on"+a;var b=a in document;b||(b=document.createElement("div"),b.setAttribute(a,"return;"),b="function"===typeof b[a]);return b}function Uf(a){a.topLevelType=null;a.nativeEvent=null;a.targetInst=null;a.ancestors.length=0;10>qc.length&&qc.push(a)}function Vf(a,b,c,d){if(qc.length){var e=qc.pop();e.topLevelType=a;e.eventSystemFlags=d;e.nativeEvent=b;e.targetInst=c;return e}return{topLevelType:a,eventSystemFlags:d,nativeEvent:b,targetInst:c,ancestors:[]}}function Wf(a){var b=
a.targetInst,c=b;do{if(!c){a.ancestors.push(c);break}var d=c;if(3===d.tag)d=d.stateNode.containerInfo;else{for(;d.return;)d=d.return;d=3!==d.tag?null:d.stateNode.containerInfo}if(!d)break;b=c.tag;5!==b&&6!==b||a.ancestors.push(c);c=Bb(d)}while(c);for(c=0;c<a.ancestors.length;c++){b=a.ancestors[c];var e=Ld(a.nativeEvent);d=a.topLevelType;var f=a.nativeEvent,g=a.eventSystemFlags;0===c&&(g|=64);for(var h=null,m=0;m<jc.length;m++){var n=jc[m];n&&(n=n.extractEvents(d,b,f,e,g))&&(h=jb(h,n))}pc(h)}}function Md(a,
b,c){if(!c.has(a)){switch(a){case "scroll":Cb(b,"scroll",!0);break;case "focus":case "blur":Cb(b,"focus",!0);Cb(b,"blur",!0);c.set("blur",null);c.set("focus",null);break;case "cancel":case "close":Tf(a)&&Cb(b,a,!0);break;case "invalid":case "submit":case "reset":break;default:-1===Db.indexOf(a)&&w(a,b)}c.set(a,null)}}function xi(a,b){var c=Jd(b);Nd.forEach(function(a){Md(a,b,c)});yi.forEach(function(a){Md(a,b,c)})}function Od(a,b,c,d,e){return{blockedOn:a,topLevelType:b,eventSystemFlags:c|32,nativeEvent:e,
container:d}}function Xf(a,b){switch(a){case "focus":case "blur":xa=null;break;case "dragenter":case "dragleave":ya=null;break;case "mouseover":case "mouseout":za=null;break;case "pointerover":case "pointerout":Eb.delete(b.pointerId);break;case "gotpointercapture":case "lostpointercapture":Fb.delete(b.pointerId)}}function Gb(a,b,c,d,e,f){if(null===a||a.nativeEvent!==f)return a=Od(b,c,d,e,f),null!==b&&(b=Hb(b),null!==b&&Yf(b)),a;a.eventSystemFlags|=d;return a}function zi(a,b,c,d,e){switch(b){case "focus":return xa=
Gb(xa,a,b,c,d,e),!0;case "dragenter":return ya=Gb(ya,a,b,c,d,e),!0;case "mouseover":return za=Gb(za,a,b,c,d,e),!0;case "pointerover":var f=e.pointerId;Eb.set(f,Gb(Eb.get(f)||null,a,b,c,d,e));return!0;case "gotpointercapture":return f=e.pointerId,Fb.set(f,Gb(Fb.get(f)||null,a,b,c,d,e)),!0}return!1}function Ai(a){var b=Bb(a.target);if(null!==b){var c=Na(b);if(null!==c)if(b=c.tag,13===b){if(b=Qf(c),null!==b){a.blockedOn=b;Pd(a.priority,function(){Bi(c)});return}}else if(3===b&&c.stateNode.hydrate){a.blockedOn=
3===c.tag?c.stateNode.containerInfo:null;return}}a.blockedOn=null}function rc(a){if(null!==a.blockedOn)return!1;var b=Qd(a.topLevelType,a.eventSystemFlags,a.container,a.nativeEvent);if(null!==b){var c=Hb(b);null!==c&&Yf(c);a.blockedOn=b;return!1}return!0}function Zf(a,b,c){rc(a)&&c.delete(b)}function Ci(){for(Rd=!1;0<fa.length;){var a=fa[0];if(null!==a.blockedOn){a=Hb(a.blockedOn);null!==a&&Di(a);break}var b=Qd(a.topLevelType,a.eventSystemFlags,a.container,a.nativeEvent);null!==b?a.blockedOn=b:fa.shift()}null!==
xa&&rc(xa)&&(xa=null);null!==ya&&rc(ya)&&(ya=null);null!==za&&rc(za)&&(za=null);Eb.forEach(Zf);Fb.forEach(Zf)}function Ib(a,b){a.blockedOn===b&&(a.blockedOn=null,Rd||(Rd=!0,$f(ag,Ci)))}function bg(a){if(0<fa.length){Ib(fa[0],a);for(var b=1;b<fa.length;b++){var c=fa[b];c.blockedOn===a&&(c.blockedOn=null)}}null!==xa&&Ib(xa,a);null!==ya&&Ib(ya,a);null!==za&&Ib(za,a);b=function(b){return Ib(b,a)};Eb.forEach(b);Fb.forEach(b);for(b=0;b<Jb.length;b++)c=Jb[b],c.blockedOn===a&&(c.blockedOn=null);for(;0<Jb.length&&
(b=Jb[0],null===b.blockedOn);)Ai(b),null===b.blockedOn&&Jb.shift()}function Sd(a,b){for(var c=0;c<a.length;c+=2){var d=a[c],e=a[c+1],f="on"+(e[0].toUpperCase()+e.slice(1));f={phasedRegistrationNames:{bubbled:f,captured:f+"Capture"},dependencies:[d],eventPriority:b};Td.set(d,b);cg.set(d,f);dg[e]=f}}function w(a,b){Cb(b,a,!1)}function Cb(a,b,c){var d=Td.get(b);switch(void 0===d?2:d){case 0:d=Ei.bind(null,b,1,a);break;case 1:d=Fi.bind(null,b,1,a);break;default:d=sc.bind(null,b,1,a)}c?a.addEventListener(b,
d,!0):a.addEventListener(b,d,!1)}function Ei(a,b,c,d){Oa||vd();var e=sc,f=Oa;Oa=!0;try{eg(e,a,b,c,d)}finally{(Oa=f)||ud()}}function Fi(a,b,c,d){Gi(Hi,sc.bind(null,a,b,c,d))}function sc(a,b,c,d){if(tc)if(0<fa.length&&-1<Nd.indexOf(a))a=Od(null,a,b,c,d),fa.push(a);else{var e=Qd(a,b,c,d);if(null===e)Xf(a,d);else if(-1<Nd.indexOf(a))a=Od(e,a,b,c,d),fa.push(a);else if(!zi(e,a,b,c,d)){Xf(a,d);a=Vf(a,d,null,b);try{uf(Wf,a)}finally{Uf(a)}}}}function Qd(a,b,c,d){c=Ld(d);c=Bb(c);if(null!==c){var e=Na(c);if(null===
e)c=null;else{var f=e.tag;if(13===f){c=Qf(e);if(null!==c)return c;c=null}else if(3===f){if(e.stateNode.hydrate)return 3===e.tag?e.stateNode.containerInfo:null;c=null}else e!==c&&(c=null)}}a=Vf(a,d,c,b);try{uf(Wf,a)}finally{Uf(a)}return null}function fg(a,b,c){return null==b||"boolean"===typeof b||""===b?"":c||"number"!==typeof b||0===b||Kb.hasOwnProperty(a)&&Kb[a]?(""+b).trim():b+"px"}function gg(a,b){a=a.style;for(var c in b)if(b.hasOwnProperty(c)){var d=0===c.indexOf("--"),e=fg(c,b[c],d);"float"===
c&&(c="cssFloat");d?a.setProperty(c,e):a[c]=e}}function Ud(a,b){if(b){if(Ii[a]&&(null!=b.children||null!=b.dangerouslySetInnerHTML))throw Error(k(137,a,""));if(null!=b.dangerouslySetInnerHTML){if(null!=b.children)throw Error(k(60));if(!("object"===typeof b.dangerouslySetInnerHTML&&"__html"in b.dangerouslySetInnerHTML))throw Error(k(61));}if(null!=b.style&&"object"!==typeof b.style)throw Error(k(62,""));}}function Vd(a,b){if(-1===a.indexOf("-"))return"string"===typeof b.is;switch(a){case "annotation-xml":case "color-profile":case "font-face":case "font-face-src":case "font-face-uri":case "font-face-format":case "font-face-name":case "missing-glyph":return!1;
default:return!0}}function oa(a,b){a=9===a.nodeType||11===a.nodeType?a:a.ownerDocument;var c=Jd(a);b=rd[b];for(var d=0;d<b.length;d++)Md(b[d],a,c)}function uc(){}function Wd(a){a=a||("undefined"!==typeof document?document:void 0);if("undefined"===typeof a)return null;try{return a.activeElement||a.body}catch(b){return a.body}}function hg(a){for(;a&&a.firstChild;)a=a.firstChild;return a}function ig(a,b){var c=hg(a);a=0;for(var d;c;){if(3===c.nodeType){d=a+c.textContent.length;if(a<=b&&d>=b)return{node:c,
offset:b-a};a=d}a:{for(;c;){if(c.nextSibling){c=c.nextSibling;break a}c=c.parentNode}c=void 0}c=hg(c)}}function jg(a,b){return a&&b?a===b?!0:a&&3===a.nodeType?!1:b&&3===b.nodeType?jg(a,b.parentNode):"contains"in a?a.contains(b):a.compareDocumentPosition?!!(a.compareDocumentPosition(b)&16):!1:!1}function kg(){for(var a=window,b=Wd();b instanceof a.HTMLIFrameElement;){try{var c="string"===typeof b.contentWindow.location.href}catch(d){c=!1}if(c)a=b.contentWindow;else break;b=Wd(a.document)}return b}
function Xd(a){var b=a&&a.nodeName&&a.nodeName.toLowerCase();return b&&("input"===b&&("text"===a.type||"search"===a.type||"tel"===a.type||"url"===a.type||"password"===a.type)||"textarea"===b||"true"===a.contentEditable)}function lg(a,b){switch(a){case "button":case "input":case "select":case "textarea":return!!b.autoFocus}return!1}function Yd(a,b){return"textarea"===a||"option"===a||"noscript"===a||"string"===typeof b.children||"number"===typeof b.children||"object"===typeof b.dangerouslySetInnerHTML&&
null!==b.dangerouslySetInnerHTML&&null!=b.dangerouslySetInnerHTML.__html}function kb(a){for(;null!=a;a=a.nextSibling){var b=a.nodeType;if(1===b||3===b)break}return a}function mg(a){a=a.previousSibling;for(var b=0;a;){if(8===a.nodeType){var c=a.data;if(c===ng||c===Zd||c===$d){if(0===b)return a;b--}else c===og&&b++}a=a.previousSibling}return null}function Bb(a){var b=a[Aa];if(b)return b;for(var c=a.parentNode;c;){if(b=c[Lb]||c[Aa]){c=b.alternate;if(null!==b.child||null!==c&&null!==c.child)for(a=mg(a);null!==
a;){if(c=a[Aa])return c;a=mg(a)}return b}a=c;c=a.parentNode}return null}function Hb(a){a=a[Aa]||a[Lb];return!a||5!==a.tag&&6!==a.tag&&13!==a.tag&&3!==a.tag?null:a}function Pa(a){if(5===a.tag||6===a.tag)return a.stateNode;throw Error(k(33));}function ae(a){return a[vc]||null}function pa(a){do a=a.return;while(a&&5!==a.tag);return a?a:null}function pg(a,b){var c=a.stateNode;if(!c)return null;var d=td(c);if(!d)return null;c=d[b];a:switch(b){case "onClick":case "onClickCapture":case "onDoubleClick":case "onDoubleClickCapture":case "onMouseDown":case "onMouseDownCapture":case "onMouseMove":case "onMouseMoveCapture":case "onMouseUp":case "onMouseUpCapture":case "onMouseEnter":(d=
!d.disabled)||(a=a.type,d=!("button"===a||"input"===a||"select"===a||"textarea"===a));a=!d;break a;default:a=!1}if(a)return null;if(c&&"function"!==typeof c)throw Error(k(231,b,typeof c));return c}function qg(a,b,c){if(b=pg(a,c.dispatchConfig.phasedRegistrationNames[b]))c._dispatchListeners=jb(c._dispatchListeners,b),c._dispatchInstances=jb(c._dispatchInstances,a)}function Ji(a){if(a&&a.dispatchConfig.phasedRegistrationNames){for(var b=a._targetInst,c=[];b;)c.push(b),b=pa(b);for(b=c.length;0<b--;)qg(c[b],
"captured",a);for(b=0;b<c.length;b++)qg(c[b],"bubbled",a)}}function be(a,b,c){a&&c&&c.dispatchConfig.registrationName&&(b=pg(a,c.dispatchConfig.registrationName))&&(c._dispatchListeners=jb(c._dispatchListeners,b),c._dispatchInstances=jb(c._dispatchInstances,a))}function Ki(a){a&&a.dispatchConfig.registrationName&&be(a._targetInst,null,a)}function lb(a){Kd(a,Ji)}function rg(){if(wc)return wc;var a,b=ce,c=b.length,d,e="value"in Ba?Ba.value:Ba.textContent,f=e.length;for(a=0;a<c&&b[a]===e[a];a++);var g=
c-a;for(d=1;d<=g&&b[c-d]===e[f-d];d++);return wc=e.slice(a,1<d?1-d:void 0)}function xc(){return!0}function yc(){return!1}function R(a,b,c,d){this.dispatchConfig=a;this._targetInst=b;this.nativeEvent=c;a=this.constructor.Interface;for(var e in a)a.hasOwnProperty(e)&&((b=a[e])?this[e]=b(c):"target"===e?this.target=d:this[e]=c[e]);this.isDefaultPrevented=(null!=c.defaultPrevented?c.defaultPrevented:!1===c.returnValue)?xc:yc;this.isPropagationStopped=yc;return this}function Li(a,b,c,d){if(this.eventPool.length){var e=
this.eventPool.pop();this.call(e,a,b,c,d);return e}return new this(a,b,c,d)}function Mi(a){if(!(a instanceof this))throw Error(k(279));a.destructor();10>this.eventPool.length&&this.eventPool.push(a)}function sg(a){a.eventPool=[];a.getPooled=Li;a.release=Mi}function tg(a,b){switch(a){case "keyup":return-1!==Ni.indexOf(b.keyCode);case "keydown":return 229!==b.keyCode;case "keypress":case "mousedown":case "blur":return!0;default:return!1}}function ug(a){a=a.detail;return"object"===typeof a&&"data"in
a?a.data:null}function Oi(a,b){switch(a){case "compositionend":return ug(b);case "keypress":if(32!==b.which)return null;vg=!0;return wg;case "textInput":return a=b.data,a===wg&&vg?null:a;default:return null}}function Pi(a,b){if(mb)return"compositionend"===a||!de&&tg(a,b)?(a=rg(),wc=ce=Ba=null,mb=!1,a):null;switch(a){case "paste":return null;case "keypress":if(!(b.ctrlKey||b.altKey||b.metaKey)||b.ctrlKey&&b.altKey){if(b.char&&1<b.char.length)return b.char;if(b.which)return String.fromCharCode(b.which)}return null;
case "compositionend":return xg&&"ko"!==b.locale?null:b.data;default:return null}}function yg(a){var b=a&&a.nodeName&&a.nodeName.toLowerCase();return"input"===b?!!Qi[a.type]:"textarea"===b?!0:!1}function zg(a,b,c){a=R.getPooled(Ag.change,a,b,c);a.type="change";sf(c);lb(a);return a}function Ri(a){pc(a)}function zc(a){var b=Pa(a);if(Gf(b))return a}function Si(a,b){if("change"===a)return b}function Bg(){Mb&&(Mb.detachEvent("onpropertychange",Cg),Nb=Mb=null)}function Cg(a){if("value"===a.propertyName&&
zc(Nb))if(a=zg(Nb,a,Ld(a)),Oa)pc(a);else{Oa=!0;try{ee(Ri,a)}finally{Oa=!1,ud()}}}function Ti(a,b,c){"focus"===a?(Bg(),Mb=b,Nb=c,Mb.attachEvent("onpropertychange",Cg)):"blur"===a&&Bg()}function Ui(a,b){if("selectionchange"===a||"keyup"===a||"keydown"===a)return zc(Nb)}function Vi(a,b){if("click"===a)return zc(b)}function Wi(a,b){if("input"===a||"change"===a)return zc(b)}function Xi(a){var b=this.nativeEvent;return b.getModifierState?b.getModifierState(a):(a=Yi[a])?!!b[a]:!1}function fe(a){return Xi}
function Zi(a,b){return a===b&&(0!==a||1/a===1/b)||a!==a&&b!==b}function Ob(a,b){if(Qa(a,b))return!0;if("object"!==typeof a||null===a||"object"!==typeof b||null===b)return!1;var c=Object.keys(a),d=Object.keys(b);if(c.length!==d.length)return!1;for(d=0;d<c.length;d++)if(!$i.call(b,c[d])||!Qa(a[c[d]],b[c[d]]))return!1;return!0}function Dg(a,b){var c=b.window===b?b.document:9===b.nodeType?b:b.ownerDocument;if(ge||null==nb||nb!==Wd(c))return null;c=nb;"selectionStart"in c&&Xd(c)?c={start:c.selectionStart,
end:c.selectionEnd}:(c=(c.ownerDocument&&c.ownerDocument.defaultView||window).getSelection(),c={anchorNode:c.anchorNode,anchorOffset:c.anchorOffset,focusNode:c.focusNode,focusOffset:c.focusOffset});return Pb&&Ob(Pb,c)?null:(Pb=c,a=R.getPooled(Eg.select,he,a,b),a.type="select",a.target=nb,lb(a),a)}function Ac(a){var b=a.keyCode;"charCode"in a?(a=a.charCode,0===a&&13===b&&(a=13)):a=b;10===a&&(a=13);return 32<=a||13===a?a:0}function q(a,b){0>ob||(a.current=ie[ob],ie[ob]=null,ob--)}function y(a,b,c){ob++;
ie[ob]=a.current;a.current=b}function pb(a,b){var c=a.type.contextTypes;if(!c)return Ca;var d=a.stateNode;if(d&&d.__reactInternalMemoizedUnmaskedChildContext===b)return d.__reactInternalMemoizedMaskedChildContext;var e={},f;for(f in c)e[f]=b[f];d&&(a=a.stateNode,a.__reactInternalMemoizedUnmaskedChildContext=b,a.__reactInternalMemoizedMaskedChildContext=e);return e}function N(a){a=a.childContextTypes;return null!==a&&void 0!==a}function Fg(a,b,c){if(B.current!==Ca)throw Error(k(168));y(B,b);y(G,c)}
function Gg(a,b,c){var d=a.stateNode;a=b.childContextTypes;if("function"!==typeof d.getChildContext)return c;d=d.getChildContext();for(var e in d)if(!(e in a))throw Error(k(108,na(b)||"Unknown",e));return M({},c,{},d)}function Bc(a){a=(a=a.stateNode)&&a.__reactInternalMemoizedMergedChildContext||Ca;Ra=B.current;y(B,a);y(G,G.current);return!0}function Hg(a,b,c){var d=a.stateNode;if(!d)throw Error(k(169));c?(a=Gg(a,b,Ra),d.__reactInternalMemoizedMergedChildContext=a,q(G),q(B),y(B,a)):q(G);y(G,c)}function Cc(){switch(aj()){case Dc:return 99;
case Ig:return 98;case Jg:return 97;case Kg:return 96;case Lg:return 95;default:throw Error(k(332));}}function Mg(a){switch(a){case 99:return Dc;case 98:return Ig;case 97:return Jg;case 96:return Kg;case 95:return Lg;default:throw Error(k(332));}}function Da(a,b){a=Mg(a);return bj(a,b)}function Ng(a,b,c){a=Mg(a);return je(a,b,c)}function Og(a){null===qa?(qa=[a],Ec=je(Dc,Pg)):qa.push(a);return Qg}function ha(){if(null!==Ec){var a=Ec;Ec=null;Rg(a)}Pg()}function Pg(){if(!ke&&null!==qa){ke=!0;var a=0;
try{var b=qa;Da(99,function(){for(;a<b.length;a++){var c=b[a];do c=c(!0);while(null!==c)}});qa=null}catch(c){throw null!==qa&&(qa=qa.slice(a+1)),je(Dc,ha),c;}finally{ke=!1}}}function Fc(a,b,c){c/=10;return 1073741821-(((1073741821-a+b/10)/c|0)+1)*c}function aa(a,b){if(a&&a.defaultProps){b=M({},b);a=a.defaultProps;for(var c in a)void 0===b[c]&&(b[c]=a[c])}return b}function le(){Gc=qb=Hc=null}function me(a){var b=Ic.current;q(Ic);a.type._context._currentValue=b}function Sg(a,b){for(;null!==a;){var c=
a.alternate;if(a.childExpirationTime<b)a.childExpirationTime=b,null!==c&&c.childExpirationTime<b&&(c.childExpirationTime=b);else if(null!==c&&c.childExpirationTime<b)c.childExpirationTime=b;else break;a=a.return}}function rb(a,b){Hc=a;Gc=qb=null;a=a.dependencies;null!==a&&null!==a.firstContext&&(a.expirationTime>=b&&(ia=!0),a.firstContext=null)}function W(a,b){if(Gc!==a&&!1!==b&&0!==b){if("number"!==typeof b||1073741823===b)Gc=a,b=1073741823;b={context:a,observedBits:b,next:null};if(null===qb){if(null===
Hc)throw Error(k(308));qb=b;Hc.dependencies={expirationTime:0,firstContext:b,responders:null}}else qb=qb.next=b}return a._currentValue}function ne(a){a.updateQueue={baseState:a.memoizedState,baseQueue:null,shared:{pending:null},effects:null}}function oe(a,b){a=a.updateQueue;b.updateQueue===a&&(b.updateQueue={baseState:a.baseState,baseQueue:a.baseQueue,shared:a.shared,effects:a.effects})}function Ea(a,b){a={expirationTime:a,suspenseConfig:b,tag:Tg,payload:null,callback:null,next:null};return a.next=
a}function Fa(a,b){a=a.updateQueue;if(null!==a){a=a.shared;var c=a.pending;null===c?b.next=b:(b.next=c.next,c.next=b);a.pending=b}}function Ug(a,b){var c=a.alternate;null!==c&&oe(c,a);a=a.updateQueue;c=a.baseQueue;null===c?(a.baseQueue=b.next=b,b.next=b):(b.next=c.next,c.next=b)}function Qb(a,b,c,d){var e=a.updateQueue;Ga=!1;var f=e.baseQueue,g=e.shared.pending;if(null!==g){if(null!==f){var h=f.next;f.next=g.next;g.next=h}f=g;e.shared.pending=null;h=a.alternate;null!==h&&(h=h.updateQueue,null!==h&&
(h.baseQueue=g))}if(null!==f){h=f.next;var m=e.baseState,n=0,k=null,ba=null,l=null;if(null!==h){var p=h;do{g=p.expirationTime;if(g<d){var t={expirationTime:p.expirationTime,suspenseConfig:p.suspenseConfig,tag:p.tag,payload:p.payload,callback:p.callback,next:null};null===l?(ba=l=t,k=m):l=l.next=t;g>n&&(n=g)}else{null!==l&&(l=l.next={expirationTime:1073741823,suspenseConfig:p.suspenseConfig,tag:p.tag,payload:p.payload,callback:p.callback,next:null});Vg(g,p.suspenseConfig);a:{var q=a,r=p;g=b;t=c;switch(r.tag){case 1:q=
r.payload;if("function"===typeof q){m=q.call(t,m,g);break a}m=q;break a;case 3:q.effectTag=q.effectTag&-4097|64;case Tg:q=r.payload;g="function"===typeof q?q.call(t,m,g):q;if(null===g||void 0===g)break a;m=M({},m,g);break a;case Jc:Ga=!0}}null!==p.callback&&(a.effectTag|=32,g=e.effects,null===g?e.effects=[p]:g.push(p))}p=p.next;if(null===p||p===h)if(g=e.shared.pending,null===g)break;else p=f.next=g.next,g.next=h,e.baseQueue=f=g,e.shared.pending=null}while(1)}null===l?k=m:l.next=ba;e.baseState=k;e.baseQueue=
l;Kc(n);a.expirationTime=n;a.memoizedState=m}}function Wg(a,b,c){a=b.effects;b.effects=null;if(null!==a)for(b=0;b<a.length;b++){var d=a[b],e=d.callback;if(null!==e){d.callback=null;d=e;e=c;if("function"!==typeof d)throw Error(k(191,d));d.call(e)}}}function Lc(a,b,c,d){b=a.memoizedState;c=c(d,b);c=null===c||void 0===c?b:M({},b,c);a.memoizedState=c;0===a.expirationTime&&(a.updateQueue.baseState=c)}function Xg(a,b,c,d,e,f,g){a=a.stateNode;return"function"===typeof a.shouldComponentUpdate?a.shouldComponentUpdate(d,
f,g):b.prototype&&b.prototype.isPureReactComponent?!Ob(c,d)||!Ob(e,f):!0}function Yg(a,b,c){var d=!1,e=Ca;var f=b.contextType;"object"===typeof f&&null!==f?f=W(f):(e=N(b)?Ra:B.current,d=b.contextTypes,f=(d=null!==d&&void 0!==d)?pb(a,e):Ca);b=new b(c,f);a.memoizedState=null!==b.state&&void 0!==b.state?b.state:null;b.updater=Mc;a.stateNode=b;b._reactInternalFiber=a;d&&(a=a.stateNode,a.__reactInternalMemoizedUnmaskedChildContext=e,a.__reactInternalMemoizedMaskedChildContext=f);return b}function Zg(a,
b,c,d){a=b.state;"function"===typeof b.componentWillReceiveProps&&b.componentWillReceiveProps(c,d);"function"===typeof b.UNSAFE_componentWillReceiveProps&&b.UNSAFE_componentWillReceiveProps(c,d);b.state!==a&&Mc.enqueueReplaceState(b,b.state,null)}function pe(a,b,c,d){var e=a.stateNode;e.props=c;e.state=a.memoizedState;e.refs=$g;ne(a);var f=b.contextType;"object"===typeof f&&null!==f?e.context=W(f):(f=N(b)?Ra:B.current,e.context=pb(a,f));Qb(a,c,e,d);e.state=a.memoizedState;f=b.getDerivedStateFromProps;
"function"===typeof f&&(Lc(a,b,f,c),e.state=a.memoizedState);"function"===typeof b.getDerivedStateFromProps||"function"===typeof e.getSnapshotBeforeUpdate||"function"!==typeof e.UNSAFE_componentWillMount&&"function"!==typeof e.componentWillMount||(b=e.state,"function"===typeof e.componentWillMount&&e.componentWillMount(),"function"===typeof e.UNSAFE_componentWillMount&&e.UNSAFE_componentWillMount(),b!==e.state&&Mc.enqueueReplaceState(e,e.state,null),Qb(a,c,e,d),e.state=a.memoizedState);"function"===
typeof e.componentDidMount&&(a.effectTag|=4)}function Rb(a,b,c){a=c.ref;if(null!==a&&"function"!==typeof a&&"object"!==typeof a){if(c._owner){c=c._owner;if(c){if(1!==c.tag)throw Error(k(309));var d=c.stateNode}if(!d)throw Error(k(147,a));var e=""+a;if(null!==b&&null!==b.ref&&"function"===typeof b.ref&&b.ref._stringRef===e)return b.ref;b=function(a){var b=d.refs;b===$g&&(b=d.refs={});null===a?delete b[e]:b[e]=a};b._stringRef=e;return b}if("string"!==typeof a)throw Error(k(284));if(!c._owner)throw Error(k(290,
a));}return a}function Nc(a,b){if("textarea"!==a.type)throw Error(k(31,"[object Object]"===Object.prototype.toString.call(b)?"object with keys {"+Object.keys(b).join(", ")+"}":b,""));}function ah(a){function b(b,c){if(a){var d=b.lastEffect;null!==d?(d.nextEffect=c,b.lastEffect=c):b.firstEffect=b.lastEffect=c;c.nextEffect=null;c.effectTag=8}}function c(c,d){if(!a)return null;for(;null!==d;)b(c,d),d=d.sibling;return null}function d(a,b){for(a=new Map;null!==b;)null!==b.key?a.set(b.key,b):a.set(b.index,
b),b=b.sibling;return a}function e(a,b){a=Sa(a,b);a.index=0;a.sibling=null;return a}function f(b,c,d){b.index=d;if(!a)return c;d=b.alternate;if(null!==d)return d=d.index,d<c?(b.effectTag=2,c):d;b.effectTag=2;return c}function g(b){a&&null===b.alternate&&(b.effectTag=2);return b}function h(a,b,c,d){if(null===b||6!==b.tag)return b=qe(c,a.mode,d),b.return=a,b;b=e(b,c);b.return=a;return b}function m(a,b,c,d){if(null!==b&&b.elementType===c.type)return d=e(b,c.props),d.ref=Rb(a,b,c),d.return=a,d;d=Oc(c.type,
c.key,c.props,null,a.mode,d);d.ref=Rb(a,b,c);d.return=a;return d}function n(a,b,c,d){if(null===b||4!==b.tag||b.stateNode.containerInfo!==c.containerInfo||b.stateNode.implementation!==c.implementation)return b=re(c,a.mode,d),b.return=a,b;b=e(b,c.children||[]);b.return=a;return b}function l(a,b,c,d,f){if(null===b||7!==b.tag)return b=Ha(c,a.mode,d,f),b.return=a,b;b=e(b,c);b.return=a;return b}function ba(a,b,c){if("string"===typeof b||"number"===typeof b)return b=qe(""+b,a.mode,c),b.return=a,b;if("object"===
typeof b&&null!==b){switch(b.$$typeof){case Pc:return c=Oc(b.type,b.key,b.props,null,a.mode,c),c.ref=Rb(a,null,b),c.return=a,c;case gb:return b=re(b,a.mode,c),b.return=a,b}if(Qc(b)||zb(b))return b=Ha(b,a.mode,c,null),b.return=a,b;Nc(a,b)}return null}function p(a,b,c,d){var e=null!==b?b.key:null;if("string"===typeof c||"number"===typeof c)return null!==e?null:h(a,b,""+c,d);if("object"===typeof c&&null!==c){switch(c.$$typeof){case Pc:return c.key===e?c.type===Ma?l(a,b,c.props.children,d,e):m(a,b,c,
d):null;case gb:return c.key===e?n(a,b,c,d):null}if(Qc(c)||zb(c))return null!==e?null:l(a,b,c,d,null);Nc(a,c)}return null}function t(a,b,c,d,e){if("string"===typeof d||"number"===typeof d)return a=a.get(c)||null,h(b,a,""+d,e);if("object"===typeof d&&null!==d){switch(d.$$typeof){case Pc:return a=a.get(null===d.key?c:d.key)||null,d.type===Ma?l(b,a,d.props.children,e,d.key):m(b,a,d,e);case gb:return a=a.get(null===d.key?c:d.key)||null,n(b,a,d,e)}if(Qc(d)||zb(d))return a=a.get(c)||null,l(b,a,d,e,null);
Nc(b,d)}return null}function q(e,g,h,m){for(var n=null,k=null,l=g,r=g=0,C=null;null!==l&&r<h.length;r++){l.index>r?(C=l,l=null):C=l.sibling;var O=p(e,l,h[r],m);if(null===O){null===l&&(l=C);break}a&&l&&null===O.alternate&&b(e,l);g=f(O,g,r);null===k?n=O:k.sibling=O;k=O;l=C}if(r===h.length)return c(e,l),n;if(null===l){for(;r<h.length;r++)l=ba(e,h[r],m),null!==l&&(g=f(l,g,r),null===k?n=l:k.sibling=l,k=l);return n}for(l=d(e,l);r<h.length;r++)C=t(l,e,r,h[r],m),null!==C&&(a&&null!==C.alternate&&l.delete(null===
C.key?r:C.key),g=f(C,g,r),null===k?n=C:k.sibling=C,k=C);a&&l.forEach(function(a){return b(e,a)});return n}function w(e,g,h,n){var m=zb(h);if("function"!==typeof m)throw Error(k(150));h=m.call(h);if(null==h)throw Error(k(151));for(var l=m=null,r=g,C=g=0,O=null,v=h.next();null!==r&&!v.done;C++,v=h.next()){r.index>C?(O=r,r=null):O=r.sibling;var q=p(e,r,v.value,n);if(null===q){null===r&&(r=O);break}a&&r&&null===q.alternate&&b(e,r);g=f(q,g,C);null===l?m=q:l.sibling=q;l=q;r=O}if(v.done)return c(e,r),m;
if(null===r){for(;!v.done;C++,v=h.next())v=ba(e,v.value,n),null!==v&&(g=f(v,g,C),null===l?m=v:l.sibling=v,l=v);return m}for(r=d(e,r);!v.done;C++,v=h.next())v=t(r,e,C,v.value,n),null!==v&&(a&&null!==v.alternate&&r.delete(null===v.key?C:v.key),g=f(v,g,C),null===l?m=v:l.sibling=v,l=v);a&&r.forEach(function(a){return b(e,a)});return m}return function(a,d,f,h){var m="object"===typeof f&&null!==f&&f.type===Ma&&null===f.key;m&&(f=f.props.children);var n="object"===typeof f&&null!==f;if(n)switch(f.$$typeof){case Pc:a:{n=
f.key;for(m=d;null!==m;){if(m.key===n){switch(m.tag){case 7:if(f.type===Ma){c(a,m.sibling);d=e(m,f.props.children);d.return=a;a=d;break a}break;default:if(m.elementType===f.type){c(a,m.sibling);d=e(m,f.props);d.ref=Rb(a,m,f);d.return=a;a=d;break a}}c(a,m);break}else b(a,m);m=m.sibling}f.type===Ma?(d=Ha(f.props.children,a.mode,h,f.key),d.return=a,a=d):(h=Oc(f.type,f.key,f.props,null,a.mode,h),h.ref=Rb(a,d,f),h.return=a,a=h)}return g(a);case gb:a:{for(m=f.key;null!==d;){if(d.key===m)if(4===d.tag&&d.stateNode.containerInfo===
f.containerInfo&&d.stateNode.implementation===f.implementation){c(a,d.sibling);d=e(d,f.children||[]);d.return=a;a=d;break a}else{c(a,d);break}else b(a,d);d=d.sibling}d=re(f,a.mode,h);d.return=a;a=d}return g(a)}if("string"===typeof f||"number"===typeof f)return f=""+f,null!==d&&6===d.tag?(c(a,d.sibling),d=e(d,f),d.return=a,a=d):(c(a,d),d=qe(f,a.mode,h),d.return=a,a=d),g(a);if(Qc(f))return q(a,d,f,h);if(zb(f))return w(a,d,f,h);n&&Nc(a,f);if("undefined"===typeof f&&!m)switch(a.tag){case 1:case 0:throw a=
a.type,Error(k(152,a.displayName||a.name||"Component"));}return c(a,d)}}function Ta(a){if(a===Sb)throw Error(k(174));return a}function se(a,b){y(Tb,b);y(Ub,a);y(ja,Sb);a=b.nodeType;switch(a){case 9:case 11:b=(b=b.documentElement)?b.namespaceURI:Hd(null,"");break;default:a=8===a?b.parentNode:b,b=a.namespaceURI||null,a=a.tagName,b=Hd(b,a)}q(ja);y(ja,b)}function tb(a){q(ja);q(Ub);q(Tb)}function bh(a){Ta(Tb.current);var b=Ta(ja.current);var c=Hd(b,a.type);b!==c&&(y(Ub,a),y(ja,c))}function te(a){Ub.current===
a&&(q(ja),q(Ub))}function Rc(a){for(var b=a;null!==b;){if(13===b.tag){var c=b.memoizedState;if(null!==c&&(c=c.dehydrated,null===c||c.data===$d||c.data===Zd))return b}else if(19===b.tag&&void 0!==b.memoizedProps.revealOrder){if(0!==(b.effectTag&64))return b}else if(null!==b.child){b.child.return=b;b=b.child;continue}if(b===a)break;for(;null===b.sibling;){if(null===b.return||b.return===a)return null;b=b.return}b.sibling.return=b.return;b=b.sibling}return null}function ue(a,b){return{responder:a,props:b}}
function S(){throw Error(k(321));}function ve(a,b){if(null===b)return!1;for(var c=0;c<b.length&&c<a.length;c++)if(!Qa(a[c],b[c]))return!1;return!0}function we(a,b,c,d,e,f){Ia=f;z=b;b.memoizedState=null;b.updateQueue=null;b.expirationTime=0;Sc.current=null===a||null===a.memoizedState?dj:ej;a=c(d,e);if(b.expirationTime===Ia){f=0;do{b.expirationTime=0;if(!(25>f))throw Error(k(301));f+=1;J=K=null;b.updateQueue=null;Sc.current=fj;a=c(d,e)}while(b.expirationTime===Ia)}Sc.current=Tc;b=null!==K&&null!==K.next;
Ia=0;J=K=z=null;Uc=!1;if(b)throw Error(k(300));return a}function ub(){var a={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};null===J?z.memoizedState=J=a:J=J.next=a;return J}function vb(){if(null===K){var a=z.alternate;a=null!==a?a.memoizedState:null}else a=K.next;var b=null===J?z.memoizedState:J.next;if(null!==b)J=b,K=a;else{if(null===a)throw Error(k(310));K=a;a={memoizedState:K.memoizedState,baseState:K.baseState,baseQueue:K.baseQueue,queue:K.queue,next:null};null===J?z.memoizedState=
J=a:J=J.next=a}return J}function Ua(a,b){return"function"===typeof b?b(a):b}function Vc(a,b,c){b=vb();c=b.queue;if(null===c)throw Error(k(311));c.lastRenderedReducer=a;var d=K,e=d.baseQueue,f=c.pending;if(null!==f){if(null!==e){var g=e.next;e.next=f.next;f.next=g}d.baseQueue=e=f;c.pending=null}if(null!==e){e=e.next;d=d.baseState;var h=g=f=null,m=e;do{var n=m.expirationTime;if(n<Ia){var l={expirationTime:m.expirationTime,suspenseConfig:m.suspenseConfig,action:m.action,eagerReducer:m.eagerReducer,eagerState:m.eagerState,
next:null};null===h?(g=h=l,f=d):h=h.next=l;n>z.expirationTime&&(z.expirationTime=n,Kc(n))}else null!==h&&(h=h.next={expirationTime:1073741823,suspenseConfig:m.suspenseConfig,action:m.action,eagerReducer:m.eagerReducer,eagerState:m.eagerState,next:null}),Vg(n,m.suspenseConfig),d=m.eagerReducer===a?m.eagerState:a(d,m.action);m=m.next}while(null!==m&&m!==e);null===h?f=d:h.next=g;Qa(d,b.memoizedState)||(ia=!0);b.memoizedState=d;b.baseState=f;b.baseQueue=h;c.lastRenderedState=d}return[b.memoizedState,
c.dispatch]}function Wc(a,b,c){b=vb();c=b.queue;if(null===c)throw Error(k(311));c.lastRenderedReducer=a;var d=c.dispatch,e=c.pending,f=b.memoizedState;if(null!==e){c.pending=null;var g=e=e.next;do f=a(f,g.action),g=g.next;while(g!==e);Qa(f,b.memoizedState)||(ia=!0);b.memoizedState=f;null===b.baseQueue&&(b.baseState=f);c.lastRenderedState=f}return[f,d]}function xe(a){var b=ub();"function"===typeof a&&(a=a());b.memoizedState=b.baseState=a;a=b.queue={pending:null,dispatch:null,lastRenderedReducer:Ua,
lastRenderedState:a};a=a.dispatch=ch.bind(null,z,a);return[b.memoizedState,a]}function ye(a,b,c,d){a={tag:a,create:b,destroy:c,deps:d,next:null};b=z.updateQueue;null===b?(b={lastEffect:null},z.updateQueue=b,b.lastEffect=a.next=a):(c=b.lastEffect,null===c?b.lastEffect=a.next=a:(d=c.next,c.next=a,a.next=d,b.lastEffect=a));return a}function dh(a){return vb().memoizedState}function ze(a,b,c,d){var e=ub();z.effectTag|=a;e.memoizedState=ye(1|b,c,void 0,void 0===d?null:d)}function Ae(a,b,c,d){var e=vb();
d=void 0===d?null:d;var f=void 0;if(null!==K){var g=K.memoizedState;f=g.destroy;if(null!==d&&ve(d,g.deps)){ye(b,c,f,d);return}}z.effectTag|=a;e.memoizedState=ye(1|b,c,f,d)}function eh(a,b){return ze(516,4,a,b)}function Xc(a,b){return Ae(516,4,a,b)}function fh(a,b){return Ae(4,2,a,b)}function gh(a,b){if("function"===typeof b)return a=a(),b(a),function(){b(null)};if(null!==b&&void 0!==b)return a=a(),b.current=a,function(){b.current=null}}function hh(a,b,c){c=null!==c&&void 0!==c?c.concat([a]):null;
return Ae(4,2,gh.bind(null,b,a),c)}function Be(a,b){}function ih(a,b){ub().memoizedState=[a,void 0===b?null:b];return a}function Yc(a,b){var c=vb();b=void 0===b?null:b;var d=c.memoizedState;if(null!==d&&null!==b&&ve(b,d[1]))return d[0];c.memoizedState=[a,b];return a}function jh(a,b){var c=vb();b=void 0===b?null:b;var d=c.memoizedState;if(null!==d&&null!==b&&ve(b,d[1]))return d[0];a=a();c.memoizedState=[a,b];return a}function Ce(a,b,c){var d=Cc();Da(98>d?98:d,function(){a(!0)});Da(97<d?97:d,function(){var d=
X.suspense;X.suspense=void 0===b?null:b;try{a(!1),c()}finally{X.suspense=d}})}function ch(a,b,c){var d=ka(),e=Vb.suspense;d=Va(d,a,e);e={expirationTime:d,suspenseConfig:e,action:c,eagerReducer:null,eagerState:null,next:null};var f=b.pending;null===f?e.next=e:(e.next=f.next,f.next=e);b.pending=e;f=a.alternate;if(a===z||null!==f&&f===z)Uc=!0,e.expirationTime=Ia,z.expirationTime=Ia;else{if(0===a.expirationTime&&(null===f||0===f.expirationTime)&&(f=b.lastRenderedReducer,null!==f))try{var g=b.lastRenderedState,
h=f(g,c);e.eagerReducer=f;e.eagerState=h;if(Qa(h,g))return}catch(m){}finally{}Ja(a,d)}}function kh(a,b){var c=la(5,null,null,0);c.elementType="DELETED";c.type="DELETED";c.stateNode=b;c.return=a;c.effectTag=8;null!==a.lastEffect?(a.lastEffect.nextEffect=c,a.lastEffect=c):a.firstEffect=a.lastEffect=c}function lh(a,b){switch(a.tag){case 5:var c=a.type;b=1!==b.nodeType||c.toLowerCase()!==b.nodeName.toLowerCase()?null:b;return null!==b?(a.stateNode=b,!0):!1;case 6:return b=""===a.pendingProps||3!==b.nodeType?
null:b,null!==b?(a.stateNode=b,!0):!1;case 13:return!1;default:return!1}}function De(a){if(Wa){var b=Ka;if(b){var c=b;if(!lh(a,b)){b=kb(c.nextSibling);if(!b||!lh(a,b)){a.effectTag=a.effectTag&-1025|2;Wa=!1;ra=a;return}kh(ra,c)}ra=a;Ka=kb(b.firstChild)}else a.effectTag=a.effectTag&-1025|2,Wa=!1,ra=a}}function mh(a){for(a=a.return;null!==a&&5!==a.tag&&3!==a.tag&&13!==a.tag;)a=a.return;ra=a}function Zc(a){if(a!==ra)return!1;if(!Wa)return mh(a),Wa=!0,!1;var b=a.type;if(5!==a.tag||"head"!==b&&"body"!==
b&&!Yd(b,a.memoizedProps))for(b=Ka;b;)kh(a,b),b=kb(b.nextSibling);mh(a);if(13===a.tag){a=a.memoizedState;a=null!==a?a.dehydrated:null;if(!a)throw Error(k(317));a:{a=a.nextSibling;for(b=0;a;){if(8===a.nodeType){var c=a.data;if(c===og){if(0===b){Ka=kb(a.nextSibling);break a}b--}else c!==ng&&c!==Zd&&c!==$d||b++}a=a.nextSibling}Ka=null}}else Ka=ra?kb(a.stateNode.nextSibling):null;return!0}function Ee(){Ka=ra=null;Wa=!1}function T(a,b,c,d){b.child=null===a?Fe(b,null,c,d):wb(b,a.child,c,d)}function nh(a,
b,c,d,e){c=c.render;var f=b.ref;rb(b,e);d=we(a,b,c,d,f,e);if(null!==a&&!ia)return b.updateQueue=a.updateQueue,b.effectTag&=-517,a.expirationTime<=e&&(a.expirationTime=0),sa(a,b,e);b.effectTag|=1;T(a,b,d,e);return b.child}function oh(a,b,c,d,e,f){if(null===a){var g=c.type;if("function"===typeof g&&!Ge(g)&&void 0===g.defaultProps&&null===c.compare&&void 0===c.defaultProps)return b.tag=15,b.type=g,ph(a,b,g,d,e,f);a=Oc(c.type,null,d,null,b.mode,f);a.ref=b.ref;a.return=b;return b.child=a}g=a.child;if(e<
f&&(e=g.memoizedProps,c=c.compare,c=null!==c?c:Ob,c(e,d)&&a.ref===b.ref))return sa(a,b,f);b.effectTag|=1;a=Sa(g,d);a.ref=b.ref;a.return=b;return b.child=a}function ph(a,b,c,d,e,f){return null!==a&&Ob(a.memoizedProps,d)&&a.ref===b.ref&&(ia=!1,e<f)?(b.expirationTime=a.expirationTime,sa(a,b,f)):He(a,b,c,d,f)}function qh(a,b){var c=b.ref;if(null===a&&null!==c||null!==a&&a.ref!==c)b.effectTag|=128}function He(a,b,c,d,e){var f=N(c)?Ra:B.current;f=pb(b,f);rb(b,e);c=we(a,b,c,d,f,e);if(null!==a&&!ia)return b.updateQueue=
a.updateQueue,b.effectTag&=-517,a.expirationTime<=e&&(a.expirationTime=0),sa(a,b,e);b.effectTag|=1;T(a,b,c,e);return b.child}function rh(a,b,c,d,e){if(N(c)){var f=!0;Bc(b)}else f=!1;rb(b,e);if(null===b.stateNode)null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2),Yg(b,c,d),pe(b,c,d,e),d=!0;else if(null===a){var g=b.stateNode,h=b.memoizedProps;g.props=h;var m=g.context,n=c.contextType;"object"===typeof n&&null!==n?n=W(n):(n=N(c)?Ra:B.current,n=pb(b,n));var l=c.getDerivedStateFromProps,k="function"===
typeof l||"function"===typeof g.getSnapshotBeforeUpdate;k||"function"!==typeof g.UNSAFE_componentWillReceiveProps&&"function"!==typeof g.componentWillReceiveProps||(h!==d||m!==n)&&Zg(b,g,d,n);Ga=!1;var p=b.memoizedState;g.state=p;Qb(b,d,g,e);m=b.memoizedState;h!==d||p!==m||G.current||Ga?("function"===typeof l&&(Lc(b,c,l,d),m=b.memoizedState),(h=Ga||Xg(b,c,h,d,p,m,n))?(k||"function"!==typeof g.UNSAFE_componentWillMount&&"function"!==typeof g.componentWillMount||("function"===typeof g.componentWillMount&&
g.componentWillMount(),"function"===typeof g.UNSAFE_componentWillMount&&g.UNSAFE_componentWillMount()),"function"===typeof g.componentDidMount&&(b.effectTag|=4)):("function"===typeof g.componentDidMount&&(b.effectTag|=4),b.memoizedProps=d,b.memoizedState=m),g.props=d,g.state=m,g.context=n,d=h):("function"===typeof g.componentDidMount&&(b.effectTag|=4),d=!1)}else g=b.stateNode,oe(a,b),h=b.memoizedProps,g.props=b.type===b.elementType?h:aa(b.type,h),m=g.context,n=c.contextType,"object"===typeof n&&null!==
n?n=W(n):(n=N(c)?Ra:B.current,n=pb(b,n)),l=c.getDerivedStateFromProps,(k="function"===typeof l||"function"===typeof g.getSnapshotBeforeUpdate)||"function"!==typeof g.UNSAFE_componentWillReceiveProps&&"function"!==typeof g.componentWillReceiveProps||(h!==d||m!==n)&&Zg(b,g,d,n),Ga=!1,m=b.memoizedState,g.state=m,Qb(b,d,g,e),p=b.memoizedState,h!==d||m!==p||G.current||Ga?("function"===typeof l&&(Lc(b,c,l,d),p=b.memoizedState),(l=Ga||Xg(b,c,h,d,m,p,n))?(k||"function"!==typeof g.UNSAFE_componentWillUpdate&&
"function"!==typeof g.componentWillUpdate||("function"===typeof g.componentWillUpdate&&g.componentWillUpdate(d,p,n),"function"===typeof g.UNSAFE_componentWillUpdate&&g.UNSAFE_componentWillUpdate(d,p,n)),"function"===typeof g.componentDidUpdate&&(b.effectTag|=4),"function"===typeof g.getSnapshotBeforeUpdate&&(b.effectTag|=256)):("function"!==typeof g.componentDidUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=4),"function"!==typeof g.getSnapshotBeforeUpdate||h===a.memoizedProps&&m===
a.memoizedState||(b.effectTag|=256),b.memoizedProps=d,b.memoizedState=p),g.props=d,g.state=p,g.context=n,d=l):("function"!==typeof g.componentDidUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=4),"function"!==typeof g.getSnapshotBeforeUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=256),d=!1);return Ie(a,b,c,d,f,e)}function Ie(a,b,c,d,e,f){qh(a,b);var g=0!==(b.effectTag&64);if(!d&&!g)return e&&Hg(b,c,!1),sa(a,b,f);d=b.stateNode;gj.current=b;var h=g&&"function"!==typeof c.getDerivedStateFromError?
null:d.render();b.effectTag|=1;null!==a&&g?(b.child=wb(b,a.child,null,f),b.child=wb(b,null,h,f)):T(a,b,h,f);b.memoizedState=d.state;e&&Hg(b,c,!0);return b.child}function sh(a){var b=a.stateNode;b.pendingContext?Fg(a,b.pendingContext,b.pendingContext!==b.context):b.context&&Fg(a,b.context,!1);se(a,b.containerInfo)}function th(a,b,c){var d=b.mode,e=b.pendingProps,f=D.current,g=!1,h;(h=0!==(b.effectTag&64))||(h=0!==(f&2)&&(null===a||null!==a.memoizedState));h?(g=!0,b.effectTag&=-65):null!==a&&null===
a.memoizedState||void 0===e.fallback||!0===e.unstable_avoidThisFallback||(f|=1);y(D,f&1);if(null===a){void 0!==e.fallback&&De(b);if(g){g=e.fallback;e=Ha(null,d,0,null);e.return=b;if(0===(b.mode&2))for(a=null!==b.memoizedState?b.child.child:b.child,e.child=a;null!==a;)a.return=e,a=a.sibling;c=Ha(g,d,c,null);c.return=b;e.sibling=c;b.memoizedState=Je;b.child=e;return c}d=e.children;b.memoizedState=null;return b.child=Fe(b,null,d,c)}if(null!==a.memoizedState){a=a.child;d=a.sibling;if(g){e=e.fallback;
c=Sa(a,a.pendingProps);c.return=b;if(0===(b.mode&2)&&(g=null!==b.memoizedState?b.child.child:b.child,g!==a.child))for(c.child=g;null!==g;)g.return=c,g=g.sibling;d=Sa(d,e);d.return=b;c.sibling=d;c.childExpirationTime=0;b.memoizedState=Je;b.child=c;return d}c=wb(b,a.child,e.children,c);b.memoizedState=null;return b.child=c}a=a.child;if(g){g=e.fallback;e=Ha(null,d,0,null);e.return=b;e.child=a;null!==a&&(a.return=e);if(0===(b.mode&2))for(a=null!==b.memoizedState?b.child.child:b.child,e.child=a;null!==
a;)a.return=e,a=a.sibling;c=Ha(g,d,c,null);c.return=b;e.sibling=c;c.effectTag|=2;e.childExpirationTime=0;b.memoizedState=Je;b.child=e;return c}b.memoizedState=null;return b.child=wb(b,a,e.children,c)}function uh(a,b){a.expirationTime<b&&(a.expirationTime=b);var c=a.alternate;null!==c&&c.expirationTime<b&&(c.expirationTime=b);Sg(a.return,b)}function Ke(a,b,c,d,e,f){var g=a.memoizedState;null===g?a.memoizedState={isBackwards:b,rendering:null,renderingStartTime:0,last:d,tail:c,tailExpiration:0,tailMode:e,
lastEffect:f}:(g.isBackwards=b,g.rendering=null,g.renderingStartTime=0,g.last=d,g.tail=c,g.tailExpiration=0,g.tailMode=e,g.lastEffect=f)}function vh(a,b,c){var d=b.pendingProps,e=d.revealOrder,f=d.tail;T(a,b,d.children,c);d=D.current;if(0!==(d&2))d=d&1|2,b.effectTag|=64;else{if(null!==a&&0!==(a.effectTag&64))a:for(a=b.child;null!==a;){if(13===a.tag)null!==a.memoizedState&&uh(a,c);else if(19===a.tag)uh(a,c);else if(null!==a.child){a.child.return=a;a=a.child;continue}if(a===b)break a;for(;null===a.sibling;){if(null===
a.return||a.return===b)break a;a=a.return}a.sibling.return=a.return;a=a.sibling}d&=1}y(D,d);if(0===(b.mode&2))b.memoizedState=null;else switch(e){case "forwards":c=b.child;for(e=null;null!==c;)a=c.alternate,null!==a&&null===Rc(a)&&(e=c),c=c.sibling;c=e;null===c?(e=b.child,b.child=null):(e=c.sibling,c.sibling=null);Ke(b,!1,e,c,f,b.lastEffect);break;case "backwards":c=null;e=b.child;for(b.child=null;null!==e;){a=e.alternate;if(null!==a&&null===Rc(a)){b.child=e;break}a=e.sibling;e.sibling=c;c=e;e=a}Ke(b,
!0,c,null,f,b.lastEffect);break;case "together":Ke(b,!1,null,null,void 0,b.lastEffect);break;default:b.memoizedState=null}return b.child}function sa(a,b,c){null!==a&&(b.dependencies=a.dependencies);var d=b.expirationTime;0!==d&&Kc(d);if(b.childExpirationTime<c)return null;if(null!==a&&b.child!==a.child)throw Error(k(153));if(null!==b.child){a=b.child;c=Sa(a,a.pendingProps);b.child=c;for(c.return=b;null!==a.sibling;)a=a.sibling,c=c.sibling=Sa(a,a.pendingProps),c.return=b;c.sibling=null}return b.child}
function $c(a,b){switch(a.tailMode){case "hidden":b=a.tail;for(var c=null;null!==b;)null!==b.alternate&&(c=b),b=b.sibling;null===c?a.tail=null:c.sibling=null;break;case "collapsed":c=a.tail;for(var d=null;null!==c;)null!==c.alternate&&(d=c),c=c.sibling;null===d?b||null===a.tail?a.tail=null:a.tail.sibling=null:d.sibling=null}}function hj(a,b,c){var d=b.pendingProps;switch(b.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return null;case 1:return N(b.type)&&(q(G),q(B)),
null;case 3:return tb(),q(G),q(B),c=b.stateNode,c.pendingContext&&(c.context=c.pendingContext,c.pendingContext=null),null!==a&&null!==a.child||!Zc(b)||(b.effectTag|=4),wh(b),null;case 5:te(b);c=Ta(Tb.current);var e=b.type;if(null!==a&&null!=b.stateNode)ij(a,b,e,d,c),a.ref!==b.ref&&(b.effectTag|=128);else{if(!d){if(null===b.stateNode)throw Error(k(166));return null}a=Ta(ja.current);if(Zc(b)){d=b.stateNode;e=b.type;var f=b.memoizedProps;d[Aa]=b;d[vc]=f;switch(e){case "iframe":case "object":case "embed":w("load",
d);break;case "video":case "audio":for(a=0;a<Db.length;a++)w(Db[a],d);break;case "source":w("error",d);break;case "img":case "image":case "link":w("error",d);w("load",d);break;case "form":w("reset",d);w("submit",d);break;case "details":w("toggle",d);break;case "input":Hf(d,f);w("invalid",d);oa(c,"onChange");break;case "select":d._wrapperState={wasMultiple:!!f.multiple};w("invalid",d);oa(c,"onChange");break;case "textarea":Kf(d,f),w("invalid",d),oa(c,"onChange")}Ud(e,f);a=null;for(var g in f)if(f.hasOwnProperty(g)){var h=
f[g];"children"===g?"string"===typeof h?d.textContent!==h&&(a=["children",h]):"number"===typeof h&&d.textContent!==""+h&&(a=["children",""+h]):db.hasOwnProperty(g)&&null!=h&&oa(c,g)}switch(e){case "input":mc(d);Jf(d,f,!0);break;case "textarea":mc(d);Mf(d);break;case "select":case "option":break;default:"function"===typeof f.onClick&&(d.onclick=uc)}c=a;b.updateQueue=c;null!==c&&(b.effectTag|=4)}else{g=9===c.nodeType?c:c.ownerDocument;"http://www.w3.org/1999/xhtml"===a&&(a=Nf(e));"http://www.w3.org/1999/xhtml"===
a?"script"===e?(a=g.createElement("div"),a.innerHTML="<script>\x3c/script>",a=a.removeChild(a.firstChild)):"string"===typeof d.is?a=g.createElement(e,{is:d.is}):(a=g.createElement(e),"select"===e&&(g=a,d.multiple?g.multiple=!0:d.size&&(g.size=d.size))):a=g.createElementNS(a,e);a[Aa]=b;a[vc]=d;jj(a,b,!1,!1);b.stateNode=a;g=Vd(e,d);switch(e){case "iframe":case "object":case "embed":w("load",a);h=d;break;case "video":case "audio":for(h=0;h<Db.length;h++)w(Db[h],a);h=d;break;case "source":w("error",a);
h=d;break;case "img":case "image":case "link":w("error",a);w("load",a);h=d;break;case "form":w("reset",a);w("submit",a);h=d;break;case "details":w("toggle",a);h=d;break;case "input":Hf(a,d);h=Cd(a,d);w("invalid",a);oa(c,"onChange");break;case "option":h=Fd(a,d);break;case "select":a._wrapperState={wasMultiple:!!d.multiple};h=M({},d,{value:void 0});w("invalid",a);oa(c,"onChange");break;case "textarea":Kf(a,d);h=Gd(a,d);w("invalid",a);oa(c,"onChange");break;default:h=d}Ud(e,h);var m=h;for(f in m)if(m.hasOwnProperty(f)){var n=
m[f];"style"===f?gg(a,n):"dangerouslySetInnerHTML"===f?(n=n?n.__html:void 0,null!=n&&xh(a,n)):"children"===f?"string"===typeof n?("textarea"!==e||""!==n)&&Wb(a,n):"number"===typeof n&&Wb(a,""+n):"suppressContentEditableWarning"!==f&&"suppressHydrationWarning"!==f&&"autoFocus"!==f&&(db.hasOwnProperty(f)?null!=n&&oa(c,f):null!=n&&xd(a,f,n,g))}switch(e){case "input":mc(a);Jf(a,d,!1);break;case "textarea":mc(a);Mf(a);break;case "option":null!=d.value&&a.setAttribute("value",""+va(d.value));break;case "select":a.multiple=
!!d.multiple;c=d.value;null!=c?hb(a,!!d.multiple,c,!1):null!=d.defaultValue&&hb(a,!!d.multiple,d.defaultValue,!0);break;default:"function"===typeof h.onClick&&(a.onclick=uc)}lg(e,d)&&(b.effectTag|=4)}null!==b.ref&&(b.effectTag|=128)}return null;case 6:if(a&&null!=b.stateNode)kj(a,b,a.memoizedProps,d);else{if("string"!==typeof d&&null===b.stateNode)throw Error(k(166));c=Ta(Tb.current);Ta(ja.current);Zc(b)?(c=b.stateNode,d=b.memoizedProps,c[Aa]=b,c.nodeValue!==d&&(b.effectTag|=4)):(c=(9===c.nodeType?
c:c.ownerDocument).createTextNode(d),c[Aa]=b,b.stateNode=c)}return null;case 13:q(D);d=b.memoizedState;if(0!==(b.effectTag&64))return b.expirationTime=c,b;c=null!==d;d=!1;null===a?void 0!==b.memoizedProps.fallback&&Zc(b):(e=a.memoizedState,d=null!==e,c||null===e||(e=a.child.sibling,null!==e&&(f=b.firstEffect,null!==f?(b.firstEffect=e,e.nextEffect=f):(b.firstEffect=b.lastEffect=e,e.nextEffect=null),e.effectTag=8)));if(c&&!d&&0!==(b.mode&2))if(null===a&&!0!==b.memoizedProps.unstable_avoidThisFallback||
0!==(D.current&1))F===Xa&&(F=ad);else{if(F===Xa||F===ad)F=bd;0!==Xb&&null!==U&&(Ya(U,P),yh(U,Xb))}if(c||d)b.effectTag|=4;return null;case 4:return tb(),wh(b),null;case 10:return me(b),null;case 17:return N(b.type)&&(q(G),q(B)),null;case 19:q(D);d=b.memoizedState;if(null===d)return null;e=0!==(b.effectTag&64);f=d.rendering;if(null===f)if(e)$c(d,!1);else{if(F!==Xa||null!==a&&0!==(a.effectTag&64))for(f=b.child;null!==f;){a=Rc(f);if(null!==a){b.effectTag|=64;$c(d,!1);e=a.updateQueue;null!==e&&(b.updateQueue=
e,b.effectTag|=4);null===d.lastEffect&&(b.firstEffect=null);b.lastEffect=d.lastEffect;for(d=b.child;null!==d;)e=d,f=c,e.effectTag&=2,e.nextEffect=null,e.firstEffect=null,e.lastEffect=null,a=e.alternate,null===a?(e.childExpirationTime=0,e.expirationTime=f,e.child=null,e.memoizedProps=null,e.memoizedState=null,e.updateQueue=null,e.dependencies=null):(e.childExpirationTime=a.childExpirationTime,e.expirationTime=a.expirationTime,e.child=a.child,e.memoizedProps=a.memoizedProps,e.memoizedState=a.memoizedState,
e.updateQueue=a.updateQueue,f=a.dependencies,e.dependencies=null===f?null:{expirationTime:f.expirationTime,firstContext:f.firstContext,responders:f.responders}),d=d.sibling;y(D,D.current&1|2);return b.child}f=f.sibling}}else{if(!e)if(a=Rc(f),null!==a){if(b.effectTag|=64,e=!0,c=a.updateQueue,null!==c&&(b.updateQueue=c,b.effectTag|=4),$c(d,!0),null===d.tail&&"hidden"===d.tailMode&&!f.alternate)return b=b.lastEffect=d.lastEffect,null!==b&&(b.nextEffect=null),null}else 2*Y()-d.renderingStartTime>d.tailExpiration&&
1<c&&(b.effectTag|=64,e=!0,$c(d,!1),b.expirationTime=b.childExpirationTime=c-1);d.isBackwards?(f.sibling=b.child,b.child=f):(c=d.last,null!==c?c.sibling=f:b.child=f,d.last=f)}return null!==d.tail?(0===d.tailExpiration&&(d.tailExpiration=Y()+500),c=d.tail,d.rendering=c,d.tail=c.sibling,d.lastEffect=b.lastEffect,d.renderingStartTime=Y(),c.sibling=null,b=D.current,y(D,e?b&1|2:b&1),c):null}throw Error(k(156,b.tag));}function lj(a,b){switch(a.tag){case 1:return N(a.type)&&(q(G),q(B)),b=a.effectTag,b&4096?
(a.effectTag=b&-4097|64,a):null;case 3:tb();q(G);q(B);b=a.effectTag;if(0!==(b&64))throw Error(k(285));a.effectTag=b&-4097|64;return a;case 5:return te(a),null;case 13:return q(D),b=a.effectTag,b&4096?(a.effectTag=b&-4097|64,a):null;case 19:return q(D),null;case 4:return tb(),null;case 10:return me(a),null;default:return null}}function Le(a,b){return{value:a,source:b,stack:Bd(b)}}function Me(a,b){var c=b.source,d=b.stack;null===d&&null!==c&&(d=Bd(c));null!==c&&na(c.type);b=b.value;null!==a&&1===a.tag&&
na(a.type);try{console.error(b)}catch(e){setTimeout(function(){throw e;})}}function mj(a,b){try{b.props=a.memoizedProps,b.state=a.memoizedState,b.componentWillUnmount()}catch(c){Za(a,c)}}function zh(a){var b=a.ref;if(null!==b)if("function"===typeof b)try{b(null)}catch(c){Za(a,c)}else b.current=null}function nj(a,b){switch(b.tag){case 0:case 11:case 15:case 22:return;case 1:if(b.effectTag&256&&null!==a){var c=a.memoizedProps,d=a.memoizedState;a=b.stateNode;b=a.getSnapshotBeforeUpdate(b.elementType===
b.type?c:aa(b.type,c),d);a.__reactInternalSnapshotBeforeUpdate=b}return;case 3:case 5:case 6:case 4:case 17:return}throw Error(k(163));}function Ah(a,b){b=b.updateQueue;b=null!==b?b.lastEffect:null;if(null!==b){var c=b=b.next;do{if((c.tag&a)===a){var d=c.destroy;c.destroy=void 0;void 0!==d&&d()}c=c.next}while(c!==b)}}function Bh(a,b){b=b.updateQueue;b=null!==b?b.lastEffect:null;if(null!==b){var c=b=b.next;do{if((c.tag&a)===a){var d=c.create;c.destroy=d()}c=c.next}while(c!==b)}}function oj(a,b,c,d){switch(c.tag){case 0:case 11:case 15:case 22:Bh(3,
c);return;case 1:a=c.stateNode;c.effectTag&4&&(null===b?a.componentDidMount():(d=c.elementType===c.type?b.memoizedProps:aa(c.type,b.memoizedProps),a.componentDidUpdate(d,b.memoizedState,a.__reactInternalSnapshotBeforeUpdate)));b=c.updateQueue;null!==b&&Wg(c,b,a);return;case 3:b=c.updateQueue;if(null!==b){a=null;if(null!==c.child)switch(c.child.tag){case 5:a=c.child.stateNode;break;case 1:a=c.child.stateNode}Wg(c,b,a)}return;case 5:a=c.stateNode;null===b&&c.effectTag&4&&lg(c.type,c.memoizedProps)&&
a.focus();return;case 6:return;case 4:return;case 12:return;case 13:null===c.memoizedState&&(c=c.alternate,null!==c&&(c=c.memoizedState,null!==c&&(c=c.dehydrated,null!==c&&bg(c))));return;case 19:case 17:case 20:case 21:return}throw Error(k(163));}function Ch(a,b,c){"function"===typeof Ne&&Ne(b);switch(b.tag){case 0:case 11:case 14:case 15:case 22:a=b.updateQueue;if(null!==a&&(a=a.lastEffect,null!==a)){var d=a.next;Da(97<c?97:c,function(){var a=d;do{var c=a.destroy;if(void 0!==c){var g=b;try{c()}catch(h){Za(g,
h)}}a=a.next}while(a!==d)})}break;case 1:zh(b);c=b.stateNode;"function"===typeof c.componentWillUnmount&&mj(b,c);break;case 5:zh(b);break;case 4:Dh(a,b,c)}}function Eh(a){var b=a.alternate;a.return=null;a.child=null;a.memoizedState=null;a.updateQueue=null;a.dependencies=null;a.alternate=null;a.firstEffect=null;a.lastEffect=null;a.pendingProps=null;a.memoizedProps=null;a.stateNode=null;null!==b&&Eh(b)}function Fh(a){return 5===a.tag||3===a.tag||4===a.tag}function Gh(a){a:{for(var b=a.return;null!==
b;){if(Fh(b)){var c=b;break a}b=b.return}throw Error(k(160));}b=c.stateNode;switch(c.tag){case 5:var d=!1;break;case 3:b=b.containerInfo;d=!0;break;case 4:b=b.containerInfo;d=!0;break;default:throw Error(k(161));}c.effectTag&16&&(Wb(b,""),c.effectTag&=-17);a:b:for(c=a;;){for(;null===c.sibling;){if(null===c.return||Fh(c.return)){c=null;break a}c=c.return}c.sibling.return=c.return;for(c=c.sibling;5!==c.tag&&6!==c.tag&&18!==c.tag;){if(c.effectTag&2)continue b;if(null===c.child||4===c.tag)continue b;
else c.child.return=c,c=c.child}if(!(c.effectTag&2)){c=c.stateNode;break a}}d?Oe(a,c,b):Pe(a,c,b)}function Oe(a,b,c){var d=a.tag,e=5===d||6===d;if(e)a=e?a.stateNode:a.stateNode.instance,b?8===c.nodeType?c.parentNode.insertBefore(a,b):c.insertBefore(a,b):(8===c.nodeType?(b=c.parentNode,b.insertBefore(a,c)):(b=c,b.appendChild(a)),c=c._reactRootContainer,null!==c&&void 0!==c||null!==b.onclick||(b.onclick=uc));else if(4!==d&&(a=a.child,null!==a))for(Oe(a,b,c),a=a.sibling;null!==a;)Oe(a,b,c),a=a.sibling}
function Pe(a,b,c){var d=a.tag,e=5===d||6===d;if(e)a=e?a.stateNode:a.stateNode.instance,b?c.insertBefore(a,b):c.appendChild(a);else if(4!==d&&(a=a.child,null!==a))for(Pe(a,b,c),a=a.sibling;null!==a;)Pe(a,b,c),a=a.sibling}function Dh(a,b,c){for(var d=b,e=!1,f,g;;){if(!e){e=d.return;a:for(;;){if(null===e)throw Error(k(160));f=e.stateNode;switch(e.tag){case 5:g=!1;break a;case 3:f=f.containerInfo;g=!0;break a;case 4:f=f.containerInfo;g=!0;break a}e=e.return}e=!0}if(5===d.tag||6===d.tag){a:for(var h=
a,m=d,n=c,l=m;;)if(Ch(h,l,n),null!==l.child&&4!==l.tag)l.child.return=l,l=l.child;else{if(l===m)break a;for(;null===l.sibling;){if(null===l.return||l.return===m)break a;l=l.return}l.sibling.return=l.return;l=l.sibling}g?(h=f,m=d.stateNode,8===h.nodeType?h.parentNode.removeChild(m):h.removeChild(m)):f.removeChild(d.stateNode)}else if(4===d.tag){if(null!==d.child){f=d.stateNode.containerInfo;g=!0;d.child.return=d;d=d.child;continue}}else if(Ch(a,d,c),null!==d.child){d.child.return=d;d=d.child;continue}if(d===
b)break;for(;null===d.sibling;){if(null===d.return||d.return===b)return;d=d.return;4===d.tag&&(e=!1)}d.sibling.return=d.return;d=d.sibling}}function Qe(a,b){switch(b.tag){case 0:case 11:case 14:case 15:case 22:Ah(3,b);return;case 1:return;case 5:var c=b.stateNode;if(null!=c){var d=b.memoizedProps,e=null!==a?a.memoizedProps:d;a=b.type;var f=b.updateQueue;b.updateQueue=null;if(null!==f){c[vc]=d;"input"===a&&"radio"===d.type&&null!=d.name&&If(c,d);Vd(a,e);b=Vd(a,d);for(e=0;e<f.length;e+=2){var g=f[e],
h=f[e+1];"style"===g?gg(c,h):"dangerouslySetInnerHTML"===g?xh(c,h):"children"===g?Wb(c,h):xd(c,g,h,b)}switch(a){case "input":Dd(c,d);break;case "textarea":Lf(c,d);break;case "select":b=c._wrapperState.wasMultiple,c._wrapperState.wasMultiple=!!d.multiple,a=d.value,null!=a?hb(c,!!d.multiple,a,!1):b!==!!d.multiple&&(null!=d.defaultValue?hb(c,!!d.multiple,d.defaultValue,!0):hb(c,!!d.multiple,d.multiple?[]:"",!1))}}}return;case 6:if(null===b.stateNode)throw Error(k(162));b.stateNode.nodeValue=b.memoizedProps;
return;case 3:b=b.stateNode;b.hydrate&&(b.hydrate=!1,bg(b.containerInfo));return;case 12:return;case 13:c=b;null===b.memoizedState?d=!1:(d=!0,c=b.child,Re=Y());if(null!==c)a:for(a=c;;){if(5===a.tag)f=a.stateNode,d?(f=f.style,"function"===typeof f.setProperty?f.setProperty("display","none","important"):f.display="none"):(f=a.stateNode,e=a.memoizedProps.style,e=void 0!==e&&null!==e&&e.hasOwnProperty("display")?e.display:null,f.style.display=fg("display",e));else if(6===a.tag)a.stateNode.nodeValue=d?
"":a.memoizedProps;else if(13===a.tag&&null!==a.memoizedState&&null===a.memoizedState.dehydrated){f=a.child.sibling;f.return=a;a=f;continue}else if(null!==a.child){a.child.return=a;a=a.child;continue}if(a===c)break;for(;null===a.sibling;){if(null===a.return||a.return===c)break a;a=a.return}a.sibling.return=a.return;a=a.sibling}Hh(b);return;case 19:Hh(b);return;case 17:return}throw Error(k(163));}function Hh(a){var b=a.updateQueue;if(null!==b){a.updateQueue=null;var c=a.stateNode;null===c&&(c=a.stateNode=
new pj);b.forEach(function(b){var d=qj.bind(null,a,b);c.has(b)||(c.add(b),b.then(d,d))})}}function Ih(a,b,c){c=Ea(c,null);c.tag=3;c.payload={element:null};var d=b.value;c.callback=function(){cd||(cd=!0,Se=d);Me(a,b)};return c}function Jh(a,b,c){c=Ea(c,null);c.tag=3;var d=a.type.getDerivedStateFromError;if("function"===typeof d){var e=b.value;c.payload=function(){Me(a,b);return d(e)}}var f=a.stateNode;null!==f&&"function"===typeof f.componentDidCatch&&(c.callback=function(){"function"!==typeof d&&
(null===La?La=new Set([this]):La.add(this),Me(a,b));var c=b.stack;this.componentDidCatch(b.value,{componentStack:null!==c?c:""})});return c}function ka(){return(p&(ca|ma))!==H?1073741821-(Y()/10|0):0!==dd?dd:dd=1073741821-(Y()/10|0)}function Va(a,b,c){b=b.mode;if(0===(b&2))return 1073741823;var d=Cc();if(0===(b&4))return 99===d?1073741823:1073741822;if((p&ca)!==H)return P;if(null!==c)a=Fc(a,c.timeoutMs|0||5E3,250);else switch(d){case 99:a=1073741823;break;case 98:a=Fc(a,150,100);break;case 97:case 96:a=
Fc(a,5E3,250);break;case 95:a=2;break;default:throw Error(k(326));}null!==U&&a===P&&--a;return a}function ed(a,b){a.expirationTime<b&&(a.expirationTime=b);var c=a.alternate;null!==c&&c.expirationTime<b&&(c.expirationTime=b);var d=a.return,e=null;if(null===d&&3===a.tag)e=a.stateNode;else for(;null!==d;){c=d.alternate;d.childExpirationTime<b&&(d.childExpirationTime=b);null!==c&&c.childExpirationTime<b&&(c.childExpirationTime=b);if(null===d.return&&3===d.tag){e=d.stateNode;break}d=d.return}null!==e&&
(U===e&&(Kc(b),F===bd&&Ya(e,P)),yh(e,b));return e}function fd(a){var b=a.lastExpiredTime;if(0!==b)return b;b=a.firstPendingTime;if(!Kh(a,b))return b;var c=a.lastPingedTime;a=a.nextKnownPendingLevel;a=c>a?c:a;return 2>=a&&b!==a?0:a}function V(a){if(0!==a.lastExpiredTime)a.callbackExpirationTime=1073741823,a.callbackPriority=99,a.callbackNode=Og(Te.bind(null,a));else{var b=fd(a),c=a.callbackNode;if(0===b)null!==c&&(a.callbackNode=null,a.callbackExpirationTime=0,a.callbackPriority=90);else{var d=ka();
1073741823===b?d=99:1===b||2===b?d=95:(d=10*(1073741821-b)-10*(1073741821-d),d=0>=d?99:250>=d?98:5250>=d?97:95);if(null!==c){var e=a.callbackPriority;if(a.callbackExpirationTime===b&&e>=d)return;c!==Qg&&Rg(c)}a.callbackExpirationTime=b;a.callbackPriority=d;b=1073741823===b?Og(Te.bind(null,a)):Ng(d,Lh.bind(null,a),{timeout:10*(1073741821-b)-Y()});a.callbackNode=b}}}function Lh(a,b){dd=0;if(b)return b=ka(),Ue(a,b),V(a),null;var c=fd(a);if(0!==c){b=a.callbackNode;if((p&(ca|ma))!==H)throw Error(k(327));
xb();a===U&&c===P||$a(a,c);if(null!==t){var d=p;p|=ca;var e=Mh();do try{rj();break}catch(h){Nh(a,h)}while(1);le();p=d;gd.current=e;if(F===hd)throw b=id,$a(a,c),Ya(a,c),V(a),b;if(null===t)switch(e=a.finishedWork=a.current.alternate,a.finishedExpirationTime=c,d=F,U=null,d){case Xa:case hd:throw Error(k(345));case Oh:Ue(a,2<c?2:c);break;case ad:Ya(a,c);d=a.lastSuspendedTime;c===d&&(a.nextKnownPendingLevel=Ve(e));if(1073741823===ta&&(e=Re+Ph-Y(),10<e)){if(jd){var f=a.lastPingedTime;if(0===f||f>=c){a.lastPingedTime=
c;$a(a,c);break}}f=fd(a);if(0!==f&&f!==c)break;if(0!==d&&d!==c){a.lastPingedTime=d;break}a.timeoutHandle=We(ab.bind(null,a),e);break}ab(a);break;case bd:Ya(a,c);d=a.lastSuspendedTime;c===d&&(a.nextKnownPendingLevel=Ve(e));if(jd&&(e=a.lastPingedTime,0===e||e>=c)){a.lastPingedTime=c;$a(a,c);break}e=fd(a);if(0!==e&&e!==c)break;if(0!==d&&d!==c){a.lastPingedTime=d;break}1073741823!==Yb?d=10*(1073741821-Yb)-Y():1073741823===ta?d=0:(d=10*(1073741821-ta)-5E3,e=Y(),c=10*(1073741821-c)-e,d=e-d,0>d&&(d=0),d=
(120>d?120:480>d?480:1080>d?1080:1920>d?1920:3E3>d?3E3:4320>d?4320:1960*sj(d/1960))-d,c<d&&(d=c));if(10<d){a.timeoutHandle=We(ab.bind(null,a),d);break}ab(a);break;case Xe:if(1073741823!==ta&&null!==kd){f=ta;var g=kd;d=g.busyMinDurationMs|0;0>=d?d=0:(e=g.busyDelayMs|0,f=Y()-(10*(1073741821-f)-(g.timeoutMs|0||5E3)),d=f<=e?0:e+d-f);if(10<d){Ya(a,c);a.timeoutHandle=We(ab.bind(null,a),d);break}}ab(a);break;default:throw Error(k(329));}V(a);if(a.callbackNode===b)return Lh.bind(null,a)}}return null}function Te(a){var b=
a.lastExpiredTime;b=0!==b?b:1073741823;if((p&(ca|ma))!==H)throw Error(k(327));xb();a===U&&b===P||$a(a,b);if(null!==t){var c=p;p|=ca;var d=Mh();do try{tj();break}catch(e){Nh(a,e)}while(1);le();p=c;gd.current=d;if(F===hd)throw c=id,$a(a,b),Ya(a,b),V(a),c;if(null!==t)throw Error(k(261));a.finishedWork=a.current.alternate;a.finishedExpirationTime=b;U=null;ab(a);V(a)}return null}function uj(){if(null!==bb){var a=bb;bb=null;a.forEach(function(a,c){Ue(c,a);V(c)});ha()}}function Qh(a,b){var c=p;p|=1;try{return a(b)}finally{p=
c,p===H&&ha()}}function Rh(a,b){var c=p;p&=-2;p|=Ye;try{return a(b)}finally{p=c,p===H&&ha()}}function $a(a,b){a.finishedWork=null;a.finishedExpirationTime=0;var c=a.timeoutHandle;-1!==c&&(a.timeoutHandle=-1,vj(c));if(null!==t)for(c=t.return;null!==c;){var d=c;switch(d.tag){case 1:d=d.type.childContextTypes;null!==d&&void 0!==d&&(q(G),q(B));break;case 3:tb();q(G);q(B);break;case 5:te(d);break;case 4:tb();break;case 13:q(D);break;case 19:q(D);break;case 10:me(d)}c=c.return}U=a;t=Sa(a.current,null);
P=b;F=Xa;id=null;Yb=ta=1073741823;kd=null;Xb=0;jd=!1}function Nh(a,b){do{try{le();Sc.current=Tc;if(Uc)for(var c=z.memoizedState;null!==c;){var d=c.queue;null!==d&&(d.pending=null);c=c.next}Ia=0;J=K=z=null;Uc=!1;if(null===t||null===t.return)return F=hd,id=b,t=null;a:{var e=a,f=t.return,g=t,h=b;b=P;g.effectTag|=2048;g.firstEffect=g.lastEffect=null;if(null!==h&&"object"===typeof h&&"function"===typeof h.then){var m=h;if(0===(g.mode&2)){var n=g.alternate;n?(g.updateQueue=n.updateQueue,g.memoizedState=
n.memoizedState,g.expirationTime=n.expirationTime):(g.updateQueue=null,g.memoizedState=null)}var l=0!==(D.current&1),k=f;do{var p;if(p=13===k.tag){var q=k.memoizedState;if(null!==q)p=null!==q.dehydrated?!0:!1;else{var w=k.memoizedProps;p=void 0===w.fallback?!1:!0!==w.unstable_avoidThisFallback?!0:l?!1:!0}}if(p){var y=k.updateQueue;if(null===y){var r=new Set;r.add(m);k.updateQueue=r}else y.add(m);if(0===(k.mode&2)){k.effectTag|=64;g.effectTag&=-2981;if(1===g.tag)if(null===g.alternate)g.tag=17;else{var O=
Ea(1073741823,null);O.tag=Jc;Fa(g,O)}g.expirationTime=1073741823;break a}h=void 0;g=b;var v=e.pingCache;null===v?(v=e.pingCache=new wj,h=new Set,v.set(m,h)):(h=v.get(m),void 0===h&&(h=new Set,v.set(m,h)));if(!h.has(g)){h.add(g);var x=xj.bind(null,e,m,g);m.then(x,x)}k.effectTag|=4096;k.expirationTime=b;break a}k=k.return}while(null!==k);h=Error((na(g.type)||"A React component")+" suspended while rendering, but no fallback UI was specified.\n\nAdd a <Suspense fallback=...> component higher in the tree to provide a loading indicator or placeholder to display."+
Bd(g))}F!==Xe&&(F=Oh);h=Le(h,g);k=f;do{switch(k.tag){case 3:m=h;k.effectTag|=4096;k.expirationTime=b;var A=Ih(k,m,b);Ug(k,A);break a;case 1:m=h;var u=k.type,B=k.stateNode;if(0===(k.effectTag&64)&&("function"===typeof u.getDerivedStateFromError||null!==B&&"function"===typeof B.componentDidCatch&&(null===La||!La.has(B)))){k.effectTag|=4096;k.expirationTime=b;var H=Jh(k,m,b);Ug(k,H);break a}}k=k.return}while(null!==k)}t=Sh(t)}catch(cj){b=cj;continue}break}while(1)}function Mh(a){a=gd.current;gd.current=
Tc;return null===a?Tc:a}function Vg(a,b){a<ta&&2<a&&(ta=a);null!==b&&a<Yb&&2<a&&(Yb=a,kd=b)}function Kc(a){a>Xb&&(Xb=a)}function tj(){for(;null!==t;)t=Th(t)}function rj(){for(;null!==t&&!yj();)t=Th(t)}function Th(a){var b=zj(a.alternate,a,P);a.memoizedProps=a.pendingProps;null===b&&(b=Sh(a));Uh.current=null;return b}function Sh(a){t=a;do{var b=t.alternate;a=t.return;if(0===(t.effectTag&2048)){b=hj(b,t,P);if(1===P||1!==t.childExpirationTime){for(var c=0,d=t.child;null!==d;){var e=d.expirationTime,
f=d.childExpirationTime;e>c&&(c=e);f>c&&(c=f);d=d.sibling}t.childExpirationTime=c}if(null!==b)return b;null!==a&&0===(a.effectTag&2048)&&(null===a.firstEffect&&(a.firstEffect=t.firstEffect),null!==t.lastEffect&&(null!==a.lastEffect&&(a.lastEffect.nextEffect=t.firstEffect),a.lastEffect=t.lastEffect),1<t.effectTag&&(null!==a.lastEffect?a.lastEffect.nextEffect=t:a.firstEffect=t,a.lastEffect=t))}else{b=lj(t);if(null!==b)return b.effectTag&=2047,b;null!==a&&(a.firstEffect=a.lastEffect=null,a.effectTag|=
2048)}b=t.sibling;if(null!==b)return b;t=a}while(null!==t);F===Xa&&(F=Xe);return null}function Ve(a){var b=a.expirationTime;a=a.childExpirationTime;return b>a?b:a}function ab(a){var b=Cc();Da(99,Aj.bind(null,a,b));return null}function Aj(a,b){do xb();while(null!==Zb);if((p&(ca|ma))!==H)throw Error(k(327));var c=a.finishedWork,d=a.finishedExpirationTime;if(null===c)return null;a.finishedWork=null;a.finishedExpirationTime=0;if(c===a.current)throw Error(k(177));a.callbackNode=null;a.callbackExpirationTime=
0;a.callbackPriority=90;a.nextKnownPendingLevel=0;var e=Ve(c);a.firstPendingTime=e;d<=a.lastSuspendedTime?a.firstSuspendedTime=a.lastSuspendedTime=a.nextKnownPendingLevel=0:d<=a.firstSuspendedTime&&(a.firstSuspendedTime=d-1);d<=a.lastPingedTime&&(a.lastPingedTime=0);d<=a.lastExpiredTime&&(a.lastExpiredTime=0);a===U&&(t=U=null,P=0);1<c.effectTag?null!==c.lastEffect?(c.lastEffect.nextEffect=c,e=c.firstEffect):e=c:e=c.firstEffect;if(null!==e){var f=p;p|=ma;Uh.current=null;Ze=tc;var g=kg();if(Xd(g)){if("selectionStart"in
g)var h={start:g.selectionStart,end:g.selectionEnd};else a:{h=(h=g.ownerDocument)&&h.defaultView||window;var m=h.getSelection&&h.getSelection();if(m&&0!==m.rangeCount){h=m.anchorNode;var n=m.anchorOffset,q=m.focusNode;m=m.focusOffset;try{h.nodeType,q.nodeType}catch(sb){h=null;break a}var ba=0,w=-1,y=-1,B=0,D=0,r=g,z=null;b:for(;;){for(var v;;){r!==h||0!==n&&3!==r.nodeType||(w=ba+n);r!==q||0!==m&&3!==r.nodeType||(y=ba+m);3===r.nodeType&&(ba+=r.nodeValue.length);if(null===(v=r.firstChild))break;z=r;
r=v}for(;;){if(r===g)break b;z===h&&++B===n&&(w=ba);z===q&&++D===m&&(y=ba);if(null!==(v=r.nextSibling))break;r=z;z=r.parentNode}r=v}h=-1===w||-1===y?null:{start:w,end:y}}else h=null}h=h||{start:0,end:0}}else h=null;$e={activeElementDetached:null,focusedElem:g,selectionRange:h};tc=!1;l=e;do try{Bj()}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);l=e;do try{for(g=a,h=b;null!==l;){var x=l.effectTag;x&16&&Wb(l.stateNode,"");if(x&128){var A=l.alternate;if(null!==A){var u=
A.ref;null!==u&&("function"===typeof u?u(null):u.current=null)}}switch(x&1038){case 2:Gh(l);l.effectTag&=-3;break;case 6:Gh(l);l.effectTag&=-3;Qe(l.alternate,l);break;case 1024:l.effectTag&=-1025;break;case 1028:l.effectTag&=-1025;Qe(l.alternate,l);break;case 4:Qe(l.alternate,l);break;case 8:n=l,Dh(g,n,h),Eh(n)}l=l.nextEffect}}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);u=$e;A=kg();x=u.focusedElem;h=u.selectionRange;if(A!==x&&x&&x.ownerDocument&&jg(x.ownerDocument.documentElement,
x)){null!==h&&Xd(x)&&(A=h.start,u=h.end,void 0===u&&(u=A),"selectionStart"in x?(x.selectionStart=A,x.selectionEnd=Math.min(u,x.value.length)):(u=(A=x.ownerDocument||document)&&A.defaultView||window,u.getSelection&&(u=u.getSelection(),n=x.textContent.length,g=Math.min(h.start,n),h=void 0===h.end?g:Math.min(h.end,n),!u.extend&&g>h&&(n=h,h=g,g=n),n=ig(x,g),q=ig(x,h),n&&q&&(1!==u.rangeCount||u.anchorNode!==n.node||u.anchorOffset!==n.offset||u.focusNode!==q.node||u.focusOffset!==q.offset)&&(A=A.createRange(),
A.setStart(n.node,n.offset),u.removeAllRanges(),g>h?(u.addRange(A),u.extend(q.node,q.offset)):(A.setEnd(q.node,q.offset),u.addRange(A))))));A=[];for(u=x;u=u.parentNode;)1===u.nodeType&&A.push({element:u,left:u.scrollLeft,top:u.scrollTop});"function"===typeof x.focus&&x.focus();for(x=0;x<A.length;x++)u=A[x],u.element.scrollLeft=u.left,u.element.scrollTop=u.top}tc=!!Ze;$e=Ze=null;a.current=c;l=e;do try{for(x=a;null!==l;){var F=l.effectTag;F&36&&oj(x,l.alternate,l);if(F&128){A=void 0;var E=l.ref;if(null!==
E){var G=l.stateNode;switch(l.tag){case 5:A=G;break;default:A=G}"function"===typeof E?E(A):E.current=A}}l=l.nextEffect}}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);l=null;Cj();p=f}else a.current=c;if(ld)ld=!1,Zb=a,$b=b;else for(l=e;null!==l;)b=l.nextEffect,l.nextEffect=null,l=b;b=a.firstPendingTime;0===b&&(La=null);1073741823===b?a===af?ac++:(ac=0,af=a):ac=0;"function"===typeof bf&&bf(c.stateNode,d);V(a);if(cd)throw cd=!1,a=Se,Se=null,a;if((p&Ye)!==H)return null;
ha();return null}function Bj(){for(;null!==l;){var a=l.effectTag;0!==(a&256)&&nj(l.alternate,l);0===(a&512)||ld||(ld=!0,Ng(97,function(){xb();return null}));l=l.nextEffect}}function xb(){if(90!==$b){var a=97<$b?97:$b;$b=90;return Da(a,Dj)}}function Dj(){if(null===Zb)return!1;var a=Zb;Zb=null;if((p&(ca|ma))!==H)throw Error(k(331));var b=p;p|=ma;for(a=a.current.firstEffect;null!==a;){try{var c=a;if(0!==(c.effectTag&512))switch(c.tag){case 0:case 11:case 15:case 22:Ah(5,c),Bh(5,c)}}catch(d){if(null===
a)throw Error(k(330));Za(a,d)}c=a.nextEffect;a.nextEffect=null;a=c}p=b;ha();return!0}function Vh(a,b,c){b=Le(c,b);b=Ih(a,b,1073741823);Fa(a,b);a=ed(a,1073741823);null!==a&&V(a)}function Za(a,b){if(3===a.tag)Vh(a,a,b);else for(var c=a.return;null!==c;){if(3===c.tag){Vh(c,a,b);break}else if(1===c.tag){var d=c.stateNode;if("function"===typeof c.type.getDerivedStateFromError||"function"===typeof d.componentDidCatch&&(null===La||!La.has(d))){a=Le(b,a);a=Jh(c,a,1073741823);Fa(c,a);c=ed(c,1073741823);null!==
c&&V(c);break}}c=c.return}}function xj(a,b,c){var d=a.pingCache;null!==d&&d.delete(b);U===a&&P===c?F===bd||F===ad&&1073741823===ta&&Y()-Re<Ph?$a(a,P):jd=!0:Kh(a,c)&&(b=a.lastPingedTime,0!==b&&b<c||(a.lastPingedTime=c,V(a)))}function qj(a,b){var c=a.stateNode;null!==c&&c.delete(b);b=0;0===b&&(b=ka(),b=Va(b,a,null));a=ed(a,b);null!==a&&V(a)}function Ej(a){if("undefined"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__)return!1;var b=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(b.isDisabled||!b.supportsFiber)return!0;try{var c=
b.inject(a);bf=function(a,e){try{b.onCommitFiberRoot(c,a,void 0,64===(a.current.effectTag&64))}catch(f){}};Ne=function(a){try{b.onCommitFiberUnmount(c,a)}catch(e){}}}catch(d){}return!0}function Fj(a,b,c,d){this.tag=a;this.key=c;this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null;this.index=0;this.ref=null;this.pendingProps=b;this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null;this.mode=d;this.effectTag=0;this.lastEffect=this.firstEffect=this.nextEffect=
null;this.childExpirationTime=this.expirationTime=0;this.alternate=null}function Ge(a){a=a.prototype;return!(!a||!a.isReactComponent)}function Gj(a){if("function"===typeof a)return Ge(a)?1:0;if(void 0!==a&&null!==a){a=a.$$typeof;if(a===zd)return 11;if(a===Ad)return 14}return 2}function Sa(a,b){var c=a.alternate;null===c?(c=la(a.tag,b,a.key,a.mode),c.elementType=a.elementType,c.type=a.type,c.stateNode=a.stateNode,c.alternate=a,a.alternate=c):(c.pendingProps=b,c.effectTag=0,c.nextEffect=null,c.firstEffect=
null,c.lastEffect=null);c.childExpirationTime=a.childExpirationTime;c.expirationTime=a.expirationTime;c.child=a.child;c.memoizedProps=a.memoizedProps;c.memoizedState=a.memoizedState;c.updateQueue=a.updateQueue;b=a.dependencies;c.dependencies=null===b?null:{expirationTime:b.expirationTime,firstContext:b.firstContext,responders:b.responders};c.sibling=a.sibling;c.index=a.index;c.ref=a.ref;return c}function Oc(a,b,c,d,e,f){var g=2;d=a;if("function"===typeof a)Ge(a)&&(g=1);else if("string"===typeof a)g=
5;else a:switch(a){case Ma:return Ha(c.children,e,f,b);case Hj:g=8;e|=7;break;case Af:g=8;e|=1;break;case kc:return a=la(12,c,b,e|8),a.elementType=kc,a.type=kc,a.expirationTime=f,a;case lc:return a=la(13,c,b,e),a.type=lc,a.elementType=lc,a.expirationTime=f,a;case yd:return a=la(19,c,b,e),a.elementType=yd,a.expirationTime=f,a;default:if("object"===typeof a&&null!==a)switch(a.$$typeof){case Cf:g=10;break a;case Bf:g=9;break a;case zd:g=11;break a;case Ad:g=14;break a;case Ef:g=16;d=null;break a;case Df:g=
22;break a}throw Error(k(130,null==a?a:typeof a,""));}b=la(g,c,b,e);b.elementType=a;b.type=d;b.expirationTime=f;return b}function Ha(a,b,c,d){a=la(7,a,d,b);a.expirationTime=c;return a}function qe(a,b,c){a=la(6,a,null,b);a.expirationTime=c;return a}function re(a,b,c){b=la(4,null!==a.children?a.children:[],a.key,b);b.expirationTime=c;b.stateNode={containerInfo:a.containerInfo,pendingChildren:null,implementation:a.implementation};return b}function Ij(a,b,c){this.tag=b;this.current=null;this.containerInfo=
a;this.pingCache=this.pendingChildren=null;this.finishedExpirationTime=0;this.finishedWork=null;this.timeoutHandle=-1;this.pendingContext=this.context=null;this.hydrate=c;this.callbackNode=null;this.callbackPriority=90;this.lastExpiredTime=this.lastPingedTime=this.nextKnownPendingLevel=this.lastSuspendedTime=this.firstSuspendedTime=this.firstPendingTime=0}function Kh(a,b){var c=a.firstSuspendedTime;a=a.lastSuspendedTime;return 0!==c&&c>=b&&a<=b}function Ya(a,b){var c=a.firstSuspendedTime,d=a.lastSuspendedTime;
c<b&&(a.firstSuspendedTime=b);if(d>b||0===c)a.lastSuspendedTime=b;b<=a.lastPingedTime&&(a.lastPingedTime=0);b<=a.lastExpiredTime&&(a.lastExpiredTime=0)}function yh(a,b){b>a.firstPendingTime&&(a.firstPendingTime=b);var c=a.firstSuspendedTime;0!==c&&(b>=c?a.firstSuspendedTime=a.lastSuspendedTime=a.nextKnownPendingLevel=0:b>=a.lastSuspendedTime&&(a.lastSuspendedTime=b+1),b>a.nextKnownPendingLevel&&(a.nextKnownPendingLevel=b))}function Ue(a,b){var c=a.lastExpiredTime;if(0===c||c>b)a.lastExpiredTime=b}
function md(a,b,c,d){var e=b.current,f=ka(),g=Vb.suspense;f=Va(f,e,g);a:if(c){c=c._reactInternalFiber;b:{if(Na(c)!==c||1!==c.tag)throw Error(k(170));var h=c;do{switch(h.tag){case 3:h=h.stateNode.context;break b;case 1:if(N(h.type)){h=h.stateNode.__reactInternalMemoizedMergedChildContext;break b}}h=h.return}while(null!==h);throw Error(k(171));}if(1===c.tag){var m=c.type;if(N(m)){c=Gg(c,m,h);break a}}c=h}else c=Ca;null===b.context?b.context=c:b.pendingContext=c;b=Ea(f,g);b.payload={element:a};d=void 0===
d?null:d;null!==d&&(b.callback=d);Fa(e,b);Ja(e,f);return f}function cf(a){a=a.current;if(!a.child)return null;switch(a.child.tag){case 5:return a.child.stateNode;default:return a.child.stateNode}}function Wh(a,b){a=a.memoizedState;null!==a&&null!==a.dehydrated&&a.retryTime<b&&(a.retryTime=b)}function df(a,b){Wh(a,b);(a=a.alternate)&&Wh(a,b)}function ef(a,b,c){c=null!=c&&!0===c.hydrate;var d=new Ij(a,b,c),e=la(3,null,null,2===b?7:1===b?3:0);d.current=e;e.stateNode=d;ne(e);a[Lb]=d.current;c&&0!==b&&
xi(a,9===a.nodeType?a:a.ownerDocument);this._internalRoot=d}function bc(a){return!(!a||1!==a.nodeType&&9!==a.nodeType&&11!==a.nodeType&&(8!==a.nodeType||" react-mount-point-unstable "!==a.nodeValue))}function Jj(a,b){b||(b=a?9===a.nodeType?a.documentElement:a.firstChild:null,b=!(!b||1!==b.nodeType||!b.hasAttribute("data-reactroot")));if(!b)for(var c;c=a.lastChild;)a.removeChild(c);return new ef(a,0,b?{hydrate:!0}:void 0)}function nd(a,b,c,d,e){var f=c._reactRootContainer;if(f){var g=f._internalRoot;
if("function"===typeof e){var h=e;e=function(){var a=cf(g);h.call(a)}}md(b,g,a,e)}else{f=c._reactRootContainer=Jj(c,d);g=f._internalRoot;if("function"===typeof e){var m=e;e=function(){var a=cf(g);m.call(a)}}Rh(function(){md(b,g,a,e)})}return cf(g)}function Kj(a,b,c){var d=3<arguments.length&&void 0!==arguments[3]?arguments[3]:null;return{$$typeof:gb,key:null==d?null:""+d,children:a,containerInfo:b,implementation:c}}function Xh(a,b){var c=2<arguments.length&&void 0!==arguments[2]?arguments[2]:null;
if(!bc(b))throw Error(k(200));return Kj(a,b,null,c)}if(!ea)throw Error(k(227));var ki=function(a,b,c,d,e,f,g,h,m){var n=Array.prototype.slice.call(arguments,3);try{b.apply(c,n)}catch(C){this.onError(C)}},yb=!1,gc=null,hc=!1,pd=null,li={onError:function(a){yb=!0;gc=a}},td=null,rf=null,mf=null,ic=null,cb={},jc=[],qd={},db={},rd={},wa=!("undefined"===typeof window||"undefined"===typeof window.document||"undefined"===typeof window.document.createElement),M=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.assign,
sd=null,eb=null,fb=null,ee=function(a,b){return a(b)},eg=function(a,b,c,d,e){return a(b,c,d,e)},vd=function(){},vf=ee,Oa=!1,wd=!1,Z=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.Scheduler,Lj=Z.unstable_cancelCallback,ff=Z.unstable_now,$f=Z.unstable_scheduleCallback,Mj=Z.unstable_shouldYield,Yh=Z.unstable_requestPaint,Pd=Z.unstable_runWithPriority,Nj=Z.unstable_getCurrentPriorityLevel,Oj=Z.unstable_ImmediatePriority,Zh=Z.unstable_UserBlockingPriority,ag=Z.unstable_NormalPriority,Pj=Z.unstable_LowPriority,
Qj=Z.unstable_IdlePriority,oi=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,wf=Object.prototype.hasOwnProperty,yf={},xf={},E={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach(function(a){E[a]=
new L(a,0,!1,a,null,!1)});[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach(function(a){var b=a[0];E[b]=new L(b,1,!1,a[1],null,!1)});["contentEditable","draggable","spellCheck","value"].forEach(function(a){E[a]=new L(a,2,!1,a.toLowerCase(),null,!1)});["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach(function(a){E[a]=new L(a,2,!1,a,null,!1)});"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach(function(a){E[a]=
new L(a,3,!1,a.toLowerCase(),null,!1)});["checked","multiple","muted","selected"].forEach(function(a){E[a]=new L(a,3,!0,a,null,!1)});["capture","download"].forEach(function(a){E[a]=new L(a,4,!1,a,null,!1)});["cols","rows","size","span"].forEach(function(a){E[a]=new L(a,6,!1,a,null,!1)});["rowSpan","start"].forEach(function(a){E[a]=new L(a,5,!1,a.toLowerCase(),null,!1)});var gf=/[\-:]([a-z])/g,hf=function(a){return a[1].toUpperCase()};"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach(function(a){var b=
a.replace(gf,hf);E[b]=new L(b,1,!1,a,null,!1)});"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach(function(a){var b=a.replace(gf,hf);E[b]=new L(b,1,!1,a,"http://www.w3.org/1999/xlink",!1)});["xml:base","xml:lang","xml:space"].forEach(function(a){var b=a.replace(gf,hf);E[b]=new L(b,1,!1,a,"http://www.w3.org/XML/1998/namespace",!1)});["tabIndex","crossOrigin"].forEach(function(a){E[a]=new L(a,1,!1,a.toLowerCase(),null,!1)});E.xlinkHref=new L("xlinkHref",1,
!1,"xlink:href","http://www.w3.org/1999/xlink",!0);["src","href","action","formAction"].forEach(function(a){E[a]=new L(a,1,!1,a.toLowerCase(),null,!0)});var da=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;da.hasOwnProperty("ReactCurrentDispatcher")||(da.ReactCurrentDispatcher={current:null});da.hasOwnProperty("ReactCurrentBatchConfig")||(da.ReactCurrentBatchConfig={suspense:null});var si=/^(.*)[\\\/]/,Q="function"===typeof Symbol&&Symbol.for,Pc=Q?Symbol.for("react.element"):60103,gb=Q?Symbol.for("react.portal"):
60106,Ma=Q?Symbol.for("react.fragment"):60107,Af=Q?Symbol.for("react.strict_mode"):60108,kc=Q?Symbol.for("react.profiler"):60114,Cf=Q?Symbol.for("react.provider"):60109,Bf=Q?Symbol.for("react.context"):60110,Hj=Q?Symbol.for("react.concurrent_mode"):60111,zd=Q?Symbol.for("react.forward_ref"):60112,lc=Q?Symbol.for("react.suspense"):60113,yd=Q?Symbol.for("react.suspense_list"):60120,Ad=Q?Symbol.for("react.memo"):60115,Ef=Q?Symbol.for("react.lazy"):60116,Df=Q?Symbol.for("react.block"):60121,zf="function"===
typeof Symbol&&Symbol.iterator,od,xh=function(a){return"undefined"!==typeof MSApp&&MSApp.execUnsafeLocalFunction?function(b,c,d,e){MSApp.execUnsafeLocalFunction(function(){return a(b,c,d,e)})}:a}(function(a,b){if("http://www.w3.org/2000/svg"!==a.namespaceURI||"innerHTML"in a)a.innerHTML=b;else{od=od||document.createElement("div");od.innerHTML="<svg>"+b.valueOf().toString()+"</svg>";for(b=od.firstChild;a.firstChild;)a.removeChild(a.firstChild);for(;b.firstChild;)a.appendChild(b.firstChild)}}),Wb=function(a,
b){if(b){var c=a.firstChild;if(c&&c===a.lastChild&&3===c.nodeType){c.nodeValue=b;return}}a.textContent=b},ib={animationend:nc("Animation","AnimationEnd"),animationiteration:nc("Animation","AnimationIteration"),animationstart:nc("Animation","AnimationStart"),transitionend:nc("Transition","TransitionEnd")},Id={},Of={};wa&&(Of=document.createElement("div").style,"AnimationEvent"in window||(delete ib.animationend.animation,delete ib.animationiteration.animation,delete ib.animationstart.animation),"TransitionEvent"in
window||delete ib.transitionend.transition);var $h=oc("animationend"),ai=oc("animationiteration"),bi=oc("animationstart"),ci=oc("transitionend"),Db="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Pf=new ("function"===typeof WeakMap?WeakMap:Map),Ab=null,wi=function(a){if(a){var b=a._dispatchListeners,c=a._dispatchInstances;
if(Array.isArray(b))for(var d=0;d<b.length&&!a.isPropagationStopped();d++)lf(a,b[d],c[d]);else b&&lf(a,b,c);a._dispatchListeners=null;a._dispatchInstances=null;a.isPersistent()||a.constructor.release(a)}},qc=[],Rd=!1,fa=[],xa=null,ya=null,za=null,Eb=new Map,Fb=new Map,Jb=[],Nd="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput close cancel copy cut paste click change contextmenu reset submit".split(" "),
yi="focus blur dragenter dragleave mouseover mouseout pointerover pointerout gotpointercapture lostpointercapture".split(" "),dg={},cg=new Map,Td=new Map,Rj=["abort","abort",$h,"animationEnd",ai,"animationIteration",bi,"animationStart","canplay","canPlay","canplaythrough","canPlayThrough","durationchange","durationChange","emptied","emptied","encrypted","encrypted","ended","ended","error","error","gotpointercapture","gotPointerCapture","load","load","loadeddata","loadedData","loadedmetadata","loadedMetadata",
"loadstart","loadStart","lostpointercapture","lostPointerCapture","playing","playing","progress","progress","seeking","seeking","stalled","stalled","suspend","suspend","timeupdate","timeUpdate",ci,"transitionEnd","waiting","waiting"];Sd("blur blur cancel cancel click click close close contextmenu contextMenu copy copy cut cut auxclick auxClick dblclick doubleClick dragend dragEnd dragstart dragStart drop drop focus focus input input invalid invalid keydown keyDown keypress keyPress keyup keyUp mousedown mouseDown mouseup mouseUp paste paste pause pause play play pointercancel pointerCancel pointerdown pointerDown pointerup pointerUp ratechange rateChange reset reset seeked seeked submit submit touchcancel touchCancel touchend touchEnd touchstart touchStart volumechange volumeChange".split(" "),
0);Sd("drag drag dragenter dragEnter dragexit dragExit dragleave dragLeave dragover dragOver mousemove mouseMove mouseout mouseOut mouseover mouseOver pointermove pointerMove pointerout pointerOut pointerover pointerOver scroll scroll toggle toggle touchmove touchMove wheel wheel".split(" "),1);Sd(Rj,2);(function(a,b){for(var c=0;c<a.length;c++)Td.set(a[c],b)})("change selectionchange textInput compositionstart compositionend compositionupdate".split(" "),0);var Hi=Zh,Gi=Pd,tc=!0,Kb={animationIterationCount:!0,
borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,
strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},Sj=["Webkit","ms","Moz","O"];Object.keys(Kb).forEach(function(a){Sj.forEach(function(b){b=b+a.charAt(0).toUpperCase()+a.substring(1);Kb[b]=Kb[a]})});var Ii=M({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0}),ng="$",og="/$",$d="$?",Zd="$!",Ze=null,$e=null,We="function"===typeof setTimeout?setTimeout:void 0,vj="function"===
typeof clearTimeout?clearTimeout:void 0,jf=Math.random().toString(36).slice(2),Aa="__reactInternalInstance$"+jf,vc="__reactEventHandlers$"+jf,Lb="__reactContainere$"+jf,Ba=null,ce=null,wc=null;M(R.prototype,{preventDefault:function(){this.defaultPrevented=!0;var a=this.nativeEvent;a&&(a.preventDefault?a.preventDefault():"unknown"!==typeof a.returnValue&&(a.returnValue=!1),this.isDefaultPrevented=xc)},stopPropagation:function(){var a=this.nativeEvent;a&&(a.stopPropagation?a.stopPropagation():"unknown"!==
typeof a.cancelBubble&&(a.cancelBubble=!0),this.isPropagationStopped=xc)},persist:function(){this.isPersistent=xc},isPersistent:yc,destructor:function(){var a=this.constructor.Interface,b;for(b in a)this[b]=null;this.nativeEvent=this._targetInst=this.dispatchConfig=null;this.isPropagationStopped=this.isDefaultPrevented=yc;this._dispatchInstances=this._dispatchListeners=null}});R.Interface={type:null,target:null,currentTarget:function(){return null},eventPhase:null,bubbles:null,cancelable:null,timeStamp:function(a){return a.timeStamp||
Date.now()},defaultPrevented:null,isTrusted:null};R.extend=function(a){function b(){return c.apply(this,arguments)}var c=this,d=function(){};d.prototype=c.prototype;d=new d;M(d,b.prototype);b.prototype=d;b.prototype.constructor=b;b.Interface=M({},c.Interface,a);b.extend=c.extend;sg(b);return b};sg(R);var Tj=R.extend({data:null}),Uj=R.extend({data:null}),Ni=[9,13,27,32],de=wa&&"CompositionEvent"in window,cc=null;wa&&"documentMode"in document&&(cc=document.documentMode);var Vj=wa&&"TextEvent"in window&&
!cc,xg=wa&&(!de||cc&&8<cc&&11>=cc),wg=String.fromCharCode(32),ua={beforeInput:{phasedRegistrationNames:{bubbled:"onBeforeInput",captured:"onBeforeInputCapture"},dependencies:["compositionend","keypress","textInput","paste"]},compositionEnd:{phasedRegistrationNames:{bubbled:"onCompositionEnd",captured:"onCompositionEndCapture"},dependencies:"blur compositionend keydown keypress keyup mousedown".split(" ")},compositionStart:{phasedRegistrationNames:{bubbled:"onCompositionStart",captured:"onCompositionStartCapture"},
dependencies:"blur compositionstart keydown keypress keyup mousedown".split(" ")},compositionUpdate:{phasedRegistrationNames:{bubbled:"onCompositionUpdate",captured:"onCompositionUpdateCapture"},dependencies:"blur compositionupdate keydown keypress keyup mousedown".split(" ")}},vg=!1,mb=!1,Wj={eventTypes:ua,extractEvents:function(a,b,c,d,e){var f;if(de)b:{switch(a){case "compositionstart":var g=ua.compositionStart;break b;case "compositionend":g=ua.compositionEnd;break b;case "compositionupdate":g=
ua.compositionUpdate;break b}g=void 0}else mb?tg(a,c)&&(g=ua.compositionEnd):"keydown"===a&&229===c.keyCode&&(g=ua.compositionStart);g?(xg&&"ko"!==c.locale&&(mb||g!==ua.compositionStart?g===ua.compositionEnd&&mb&&(f=rg()):(Ba=d,ce="value"in Ba?Ba.value:Ba.textContent,mb=!0)),e=Tj.getPooled(g,b,c,d),f?e.data=f:(f=ug(c),null!==f&&(e.data=f)),lb(e),f=e):f=null;(a=Vj?Oi(a,c):Pi(a,c))?(b=Uj.getPooled(ua.beforeInput,b,c,d),b.data=a,lb(b)):b=null;return null===f?b:null===b?f:[f,b]}},Qi={color:!0,date:!0,
datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0},Ag={change:{phasedRegistrationNames:{bubbled:"onChange",captured:"onChangeCapture"},dependencies:"blur change click focus input keydown keyup selectionchange".split(" ")}},Mb=null,Nb=null,kf=!1;wa&&(kf=Tf("input")&&(!document.documentMode||9<document.documentMode));var Xj={eventTypes:Ag,_isInputEventSupported:kf,extractEvents:function(a,b,c,d,e){e=b?Pa(b):window;var f=
e.nodeName&&e.nodeName.toLowerCase();if("select"===f||"input"===f&&"file"===e.type)var g=Si;else if(yg(e))if(kf)g=Wi;else{g=Ui;var h=Ti}else(f=e.nodeName)&&"input"===f.toLowerCase()&&("checkbox"===e.type||"radio"===e.type)&&(g=Vi);if(g&&(g=g(a,b)))return zg(g,c,d);h&&h(a,e,b);"blur"===a&&(a=e._wrapperState)&&a.controlled&&"number"===e.type&&Ed(e,"number",e.value)}},dc=R.extend({view:null,detail:null}),Yi={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"},di=0,ei=0,fi=!1,gi=!1,ec=dc.extend({screenX:null,
screenY:null,clientX:null,clientY:null,pageX:null,pageY:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,getModifierState:fe,button:null,buttons:null,relatedTarget:function(a){return a.relatedTarget||(a.fromElement===a.srcElement?a.toElement:a.fromElement)},movementX:function(a){if("movementX"in a)return a.movementX;var b=di;di=a.screenX;return fi?"mousemove"===a.type?a.screenX-b:0:(fi=!0,0)},movementY:function(a){if("movementY"in a)return a.movementY;var b=ei;ei=a.screenY;return gi?"mousemove"===
a.type?a.screenY-b:0:(gi=!0,0)}}),hi=ec.extend({pointerId:null,width:null,height:null,pressure:null,tangentialPressure:null,tiltX:null,tiltY:null,twist:null,pointerType:null,isPrimary:null}),fc={mouseEnter:{registrationName:"onMouseEnter",dependencies:["mouseout","mouseover"]},mouseLeave:{registrationName:"onMouseLeave",dependencies:["mouseout","mouseover"]},pointerEnter:{registrationName:"onPointerEnter",dependencies:["pointerout","pointerover"]},pointerLeave:{registrationName:"onPointerLeave",dependencies:["pointerout",
"pointerover"]}},Yj={eventTypes:fc,extractEvents:function(a,b,c,d,e){var f="mouseover"===a||"pointerover"===a,g="mouseout"===a||"pointerout"===a;if(f&&0===(e&32)&&(c.relatedTarget||c.fromElement)||!g&&!f)return null;f=d.window===d?d:(f=d.ownerDocument)?f.defaultView||f.parentWindow:window;if(g){if(g=b,b=(b=c.relatedTarget||c.toElement)?Bb(b):null,null!==b){var h=Na(b);if(b!==h||5!==b.tag&&6!==b.tag)b=null}}else g=null;if(g===b)return null;if("mouseout"===a||"mouseover"===a){var m=ec;var n=fc.mouseLeave;
var l=fc.mouseEnter;var k="mouse"}else if("pointerout"===a||"pointerover"===a)m=hi,n=fc.pointerLeave,l=fc.pointerEnter,k="pointer";a=null==g?f:Pa(g);f=null==b?f:Pa(b);n=m.getPooled(n,g,c,d);n.type=k+"leave";n.target=a;n.relatedTarget=f;c=m.getPooled(l,b,c,d);c.type=k+"enter";c.target=f;c.relatedTarget=a;d=g;k=b;if(d&&k)a:{m=d;l=k;g=0;for(a=m;a;a=pa(a))g++;a=0;for(b=l;b;b=pa(b))a++;for(;0<g-a;)m=pa(m),g--;for(;0<a-g;)l=pa(l),a--;for(;g--;){if(m===l||m===l.alternate)break a;m=pa(m);l=pa(l)}m=null}else m=
null;l=m;for(m=[];d&&d!==l;){g=d.alternate;if(null!==g&&g===l)break;m.push(d);d=pa(d)}for(d=[];k&&k!==l;){g=k.alternate;if(null!==g&&g===l)break;d.push(k);k=pa(k)}for(k=0;k<m.length;k++)be(m[k],"bubbled",n);for(k=d.length;0<k--;)be(d[k],"captured",c);return 0===(e&64)?[n]:[n,c]}},Qa="function"===typeof Object.is?Object.is:Zi,$i=Object.prototype.hasOwnProperty,Zj=wa&&"documentMode"in document&&11>=document.documentMode,Eg={select:{phasedRegistrationNames:{bubbled:"onSelect",captured:"onSelectCapture"},
dependencies:"blur contextmenu dragend focus keydown keyup mousedown mouseup selectionchange".split(" ")}},nb=null,he=null,Pb=null,ge=!1,ak={eventTypes:Eg,extractEvents:function(a,b,c,d,e,f){e=f||(d.window===d?d.document:9===d.nodeType?d:d.ownerDocument);if(!(f=!e)){a:{e=Jd(e);f=rd.onSelect;for(var g=0;g<f.length;g++)if(!e.has(f[g])){e=!1;break a}e=!0}f=!e}if(f)return null;e=b?Pa(b):window;switch(a){case "focus":if(yg(e)||"true"===e.contentEditable)nb=e,he=b,Pb=null;break;case "blur":Pb=he=nb=null;
break;case "mousedown":ge=!0;break;case "contextmenu":case "mouseup":case "dragend":return ge=!1,Dg(c,d);case "selectionchange":if(Zj)break;case "keydown":case "keyup":return Dg(c,d)}return null}},bk=R.extend({animationName:null,elapsedTime:null,pseudoElement:null}),ck=R.extend({clipboardData:function(a){return"clipboardData"in a?a.clipboardData:window.clipboardData}}),dk=dc.extend({relatedTarget:null}),ek={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",
Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},fk={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",
224:"Meta"},gk=dc.extend({key:function(a){if(a.key){var b=ek[a.key]||a.key;if("Unidentified"!==b)return b}return"keypress"===a.type?(a=Ac(a),13===a?"Enter":String.fromCharCode(a)):"keydown"===a.type||"keyup"===a.type?fk[a.keyCode]||"Unidentified":""},location:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,repeat:null,locale:null,getModifierState:fe,charCode:function(a){return"keypress"===a.type?Ac(a):0},keyCode:function(a){return"keydown"===a.type||"keyup"===a.type?a.keyCode:0},which:function(a){return"keypress"===
a.type?Ac(a):"keydown"===a.type||"keyup"===a.type?a.keyCode:0}}),hk=ec.extend({dataTransfer:null}),ik=dc.extend({touches:null,targetTouches:null,changedTouches:null,altKey:null,metaKey:null,ctrlKey:null,shiftKey:null,getModifierState:fe}),jk=R.extend({propertyName:null,elapsedTime:null,pseudoElement:null}),kk=ec.extend({deltaX:function(a){return"deltaX"in a?a.deltaX:"wheelDeltaX"in a?-a.wheelDeltaX:0},deltaY:function(a){return"deltaY"in a?a.deltaY:"wheelDeltaY"in a?-a.wheelDeltaY:"wheelDelta"in a?
-a.wheelDelta:0},deltaZ:null,deltaMode:null}),lk={eventTypes:dg,extractEvents:function(a,b,c,d,e){e=cg.get(a);if(!e)return null;switch(a){case "keypress":if(0===Ac(c))return null;case "keydown":case "keyup":a=gk;break;case "blur":case "focus":a=dk;break;case "click":if(2===c.button)return null;case "auxclick":case "dblclick":case "mousedown":case "mousemove":case "mouseup":case "mouseout":case "mouseover":case "contextmenu":a=ec;break;case "drag":case "dragend":case "dragenter":case "dragexit":case "dragleave":case "dragover":case "dragstart":case "drop":a=
hk;break;case "touchcancel":case "touchend":case "touchmove":case "touchstart":a=ik;break;case $h:case ai:case bi:a=bk;break;case ci:a=jk;break;case "scroll":a=dc;break;case "wheel":a=kk;break;case "copy":case "cut":case "paste":a=ck;break;case "gotpointercapture":case "lostpointercapture":case "pointercancel":case "pointerdown":case "pointermove":case "pointerout":case "pointerover":case "pointerup":a=hi;break;default:a=R}b=a.getPooled(e,b,c,d);lb(b);return b}};(function(a){if(ic)throw Error(k(101));
ic=Array.prototype.slice.call(a);nf()})("ResponderEventPlugin SimpleEventPlugin EnterLeaveEventPlugin ChangeEventPlugin SelectEventPlugin BeforeInputEventPlugin".split(" "));(function(a,b,c){td=a;rf=b;mf=c})(ae,Hb,Pa);pf({SimpleEventPlugin:lk,EnterLeaveEventPlugin:Yj,ChangeEventPlugin:Xj,SelectEventPlugin:ak,BeforeInputEventPlugin:Wj});var ie=[],ob=-1,Ca={},B={current:Ca},G={current:!1},Ra=Ca,bj=Pd,je=$f,Rg=Lj,aj=Nj,Dc=Oj,Ig=Zh,Jg=ag,Kg=Pj,Lg=Qj,Qg={},yj=Mj,Cj=void 0!==Yh?Yh:function(){},qa=null,
Ec=null,ke=!1,ii=ff(),Y=1E4>ii?ff:function(){return ff()-ii},Ic={current:null},Hc=null,qb=null,Gc=null,Tg=0,Jc=2,Ga=!1,Vb=da.ReactCurrentBatchConfig,$g=(new ea.Component).refs,Mc={isMounted:function(a){return(a=a._reactInternalFiber)?Na(a)===a:!1},enqueueSetState:function(a,b,c){a=a._reactInternalFiber;var d=ka(),e=Vb.suspense;d=Va(d,a,e);e=Ea(d,e);e.payload=b;void 0!==c&&null!==c&&(e.callback=c);Fa(a,e);Ja(a,d)},enqueueReplaceState:function(a,b,c){a=a._reactInternalFiber;var d=ka(),e=Vb.suspense;
d=Va(d,a,e);e=Ea(d,e);e.tag=1;e.payload=b;void 0!==c&&null!==c&&(e.callback=c);Fa(a,e);Ja(a,d)},enqueueForceUpdate:function(a,b){a=a._reactInternalFiber;var c=ka(),d=Vb.suspense;c=Va(c,a,d);d=Ea(c,d);d.tag=Jc;void 0!==b&&null!==b&&(d.callback=b);Fa(a,d);Ja(a,c)}},Qc=Array.isArray,wb=ah(!0),Fe=ah(!1),Sb={},ja={current:Sb},Ub={current:Sb},Tb={current:Sb},D={current:0},Sc=da.ReactCurrentDispatcher,X=da.ReactCurrentBatchConfig,Ia=0,z=null,K=null,J=null,Uc=!1,Tc={readContext:W,useCallback:S,useContext:S,
useEffect:S,useImperativeHandle:S,useLayoutEffect:S,useMemo:S,useReducer:S,useRef:S,useState:S,useDebugValue:S,useResponder:S,useDeferredValue:S,useTransition:S},dj={readContext:W,useCallback:ih,useContext:W,useEffect:eh,useImperativeHandle:function(a,b,c){c=null!==c&&void 0!==c?c.concat([a]):null;return ze(4,2,gh.bind(null,b,a),c)},useLayoutEffect:function(a,b){return ze(4,2,a,b)},useMemo:function(a,b){var c=ub();b=void 0===b?null:b;a=a();c.memoizedState=[a,b];return a},useReducer:function(a,b,c){var d=
ub();b=void 0!==c?c(b):b;d.memoizedState=d.baseState=b;a=d.queue={pending:null,dispatch:null,lastRenderedReducer:a,lastRenderedState:b};a=a.dispatch=ch.bind(null,z,a);return[d.memoizedState,a]},useRef:function(a){var b=ub();a={current:a};return b.memoizedState=a},useState:xe,useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=xe(a),d=c[0],e=c[1];eh(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=
xe(!1),c=b[0];b=b[1];return[ih(Ce.bind(null,b,a),[b,a]),c]}},ej={readContext:W,useCallback:Yc,useContext:W,useEffect:Xc,useImperativeHandle:hh,useLayoutEffect:fh,useMemo:jh,useReducer:Vc,useRef:dh,useState:function(a){return Vc(Ua)},useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=Vc(Ua),d=c[0],e=c[1];Xc(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=Vc(Ua),c=b[0];b=b[1];return[Yc(Ce.bind(null,
b,a),[b,a]),c]}},fj={readContext:W,useCallback:Yc,useContext:W,useEffect:Xc,useImperativeHandle:hh,useLayoutEffect:fh,useMemo:jh,useReducer:Wc,useRef:dh,useState:function(a){return Wc(Ua)},useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=Wc(Ua),d=c[0],e=c[1];Xc(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=Wc(Ua),c=b[0];b=b[1];return[Yc(Ce.bind(null,b,a),[b,a]),c]}},ra=null,Ka=null,Wa=
!1,gj=da.ReactCurrentOwner,ia=!1,Je={dehydrated:null,retryTime:0};var jj=function(a,b,c,d){for(c=b.child;null!==c;){if(5===c.tag||6===c.tag)a.appendChild(c.stateNode);else if(4!==c.tag&&null!==c.child){c.child.return=c;c=c.child;continue}if(c===b)break;for(;null===c.sibling;){if(null===c.return||c.return===b)return;c=c.return}c.sibling.return=c.return;c=c.sibling}};var wh=function(a){};var ij=function(a,b,c,d,e){var f=a.memoizedProps;if(f!==d){var g=b.stateNode;Ta(ja.current);a=null;switch(c){case "input":f=
Cd(g,f);d=Cd(g,d);a=[];break;case "option":f=Fd(g,f);d=Fd(g,d);a=[];break;case "select":f=M({},f,{value:void 0});d=M({},d,{value:void 0});a=[];break;case "textarea":f=Gd(g,f);d=Gd(g,d);a=[];break;default:"function"!==typeof f.onClick&&"function"===typeof d.onClick&&(g.onclick=uc)}Ud(c,d);var h,m;c=null;for(h in f)if(!d.hasOwnProperty(h)&&f.hasOwnProperty(h)&&null!=f[h])if("style"===h)for(m in g=f[h],g)g.hasOwnProperty(m)&&(c||(c={}),c[m]="");else"dangerouslySetInnerHTML"!==h&&"children"!==h&&"suppressContentEditableWarning"!==
h&&"suppressHydrationWarning"!==h&&"autoFocus"!==h&&(db.hasOwnProperty(h)?a||(a=[]):(a=a||[]).push(h,null));for(h in d){var k=d[h];g=null!=f?f[h]:void 0;if(d.hasOwnProperty(h)&&k!==g&&(null!=k||null!=g))if("style"===h)if(g){for(m in g)!g.hasOwnProperty(m)||k&&k.hasOwnProperty(m)||(c||(c={}),c[m]="");for(m in k)k.hasOwnProperty(m)&&g[m]!==k[m]&&(c||(c={}),c[m]=k[m])}else c||(a||(a=[]),a.push(h,c)),c=k;else"dangerouslySetInnerHTML"===h?(k=k?k.__html:void 0,g=g?g.__html:void 0,null!=k&&g!==k&&(a=a||
[]).push(h,k)):"children"===h?g===k||"string"!==typeof k&&"number"!==typeof k||(a=a||[]).push(h,""+k):"suppressContentEditableWarning"!==h&&"suppressHydrationWarning"!==h&&(db.hasOwnProperty(h)?(null!=k&&oa(e,h),a||g===k||(a=[])):(a=a||[]).push(h,k))}c&&(a=a||[]).push("style",c);e=a;if(b.updateQueue=e)b.effectTag|=4}};var kj=function(a,b,c,d){c!==d&&(b.effectTag|=4)};var pj="function"===typeof WeakSet?WeakSet:Set,wj="function"===typeof WeakMap?WeakMap:Map,sj=Math.ceil,gd=da.ReactCurrentDispatcher,
Uh=da.ReactCurrentOwner,H=0,Ye=8,ca=16,ma=32,Xa=0,hd=1,Oh=2,ad=3,bd=4,Xe=5,p=H,U=null,t=null,P=0,F=Xa,id=null,ta=1073741823,Yb=1073741823,kd=null,Xb=0,jd=!1,Re=0,Ph=500,l=null,cd=!1,Se=null,La=null,ld=!1,Zb=null,$b=90,bb=null,ac=0,af=null,dd=0,Ja=function(a,b){if(50<ac)throw ac=0,af=null,Error(k(185));a=ed(a,b);if(null!==a){var c=Cc();1073741823===b?(p&Ye)!==H&&(p&(ca|ma))===H?Te(a):(V(a),p===H&&ha()):V(a);(p&4)===H||98!==c&&99!==c||(null===bb?bb=new Map([[a,b]]):(c=bb.get(a),(void 0===c||c>b)&&bb.set(a,
b)))}};var zj=function(a,b,c){var d=b.expirationTime;if(null!==a){var e=b.pendingProps;if(a.memoizedProps!==e||G.current)ia=!0;else{if(d<c){ia=!1;switch(b.tag){case 3:sh(b);Ee();break;case 5:bh(b);if(b.mode&4&&1!==c&&e.hidden)return b.expirationTime=b.childExpirationTime=1,null;break;case 1:N(b.type)&&Bc(b);break;case 4:se(b,b.stateNode.containerInfo);break;case 10:d=b.memoizedProps.value;e=b.type._context;y(Ic,e._currentValue);e._currentValue=d;break;case 13:if(null!==b.memoizedState){d=b.child.childExpirationTime;
if(0!==d&&d>=c)return th(a,b,c);y(D,D.current&1);b=sa(a,b,c);return null!==b?b.sibling:null}y(D,D.current&1);break;case 19:d=b.childExpirationTime>=c;if(0!==(a.effectTag&64)){if(d)return vh(a,b,c);b.effectTag|=64}e=b.memoizedState;null!==e&&(e.rendering=null,e.tail=null);y(D,D.current);if(!d)return null}return sa(a,b,c)}ia=!1}}else ia=!1;b.expirationTime=0;switch(b.tag){case 2:d=b.type;null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2);a=b.pendingProps;e=pb(b,B.current);rb(b,c);e=we(null,
b,d,a,e,c);b.effectTag|=1;if("object"===typeof e&&null!==e&&"function"===typeof e.render&&void 0===e.$$typeof){b.tag=1;b.memoizedState=null;b.updateQueue=null;if(N(d)){var f=!0;Bc(b)}else f=!1;b.memoizedState=null!==e.state&&void 0!==e.state?e.state:null;ne(b);var g=d.getDerivedStateFromProps;"function"===typeof g&&Lc(b,d,g,a);e.updater=Mc;b.stateNode=e;e._reactInternalFiber=b;pe(b,d,a,c);b=Ie(null,b,d,!0,f,c)}else b.tag=0,T(null,b,e,c),b=b.child;return b;case 16:a:{e=b.elementType;null!==a&&(a.alternate=
null,b.alternate=null,b.effectTag|=2);a=b.pendingProps;ri(e);if(1!==e._status)throw e._result;e=e._result;b.type=e;f=b.tag=Gj(e);a=aa(e,a);switch(f){case 0:b=He(null,b,e,a,c);break a;case 1:b=rh(null,b,e,a,c);break a;case 11:b=nh(null,b,e,a,c);break a;case 14:b=oh(null,b,e,aa(e.type,a),d,c);break a}throw Error(k(306,e,""));}return b;case 0:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),He(a,b,d,e,c);case 1:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),rh(a,b,d,e,c);
case 3:sh(b);d=b.updateQueue;if(null===a||null===d)throw Error(k(282));d=b.pendingProps;e=b.memoizedState;e=null!==e?e.element:null;oe(a,b);Qb(b,d,null,c);d=b.memoizedState.element;if(d===e)Ee(),b=sa(a,b,c);else{if(e=b.stateNode.hydrate)Ka=kb(b.stateNode.containerInfo.firstChild),ra=b,e=Wa=!0;if(e)for(c=Fe(b,null,d,c),b.child=c;c;)c.effectTag=c.effectTag&-3|1024,c=c.sibling;else T(a,b,d,c),Ee();b=b.child}return b;case 5:return bh(b),null===a&&De(b),d=b.type,e=b.pendingProps,f=null!==a?a.memoizedProps:
null,g=e.children,Yd(d,e)?g=null:null!==f&&Yd(d,f)&&(b.effectTag|=16),qh(a,b),b.mode&4&&1!==c&&e.hidden?(b.expirationTime=b.childExpirationTime=1,b=null):(T(a,b,g,c),b=b.child),b;case 6:return null===a&&De(b),null;case 13:return th(a,b,c);case 4:return se(b,b.stateNode.containerInfo),d=b.pendingProps,null===a?b.child=wb(b,null,d,c):T(a,b,d,c),b.child;case 11:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),nh(a,b,d,e,c);case 7:return T(a,b,b.pendingProps,c),b.child;case 8:return T(a,
b,b.pendingProps.children,c),b.child;case 12:return T(a,b,b.pendingProps.children,c),b.child;case 10:a:{d=b.type._context;e=b.pendingProps;g=b.memoizedProps;f=e.value;var h=b.type._context;y(Ic,h._currentValue);h._currentValue=f;if(null!==g)if(h=g.value,f=Qa(h,f)?0:("function"===typeof d._calculateChangedBits?d._calculateChangedBits(h,f):1073741823)|0,0===f){if(g.children===e.children&&!G.current){b=sa(a,b,c);break a}}else for(h=b.child,null!==h&&(h.return=b);null!==h;){var m=h.dependencies;if(null!==
m){g=h.child;for(var l=m.firstContext;null!==l;){if(l.context===d&&0!==(l.observedBits&f)){1===h.tag&&(l=Ea(c,null),l.tag=Jc,Fa(h,l));h.expirationTime<c&&(h.expirationTime=c);l=h.alternate;null!==l&&l.expirationTime<c&&(l.expirationTime=c);Sg(h.return,c);m.expirationTime<c&&(m.expirationTime=c);break}l=l.next}}else g=10===h.tag?h.type===b.type?null:h.child:h.child;if(null!==g)g.return=h;else for(g=h;null!==g;){if(g===b){g=null;break}h=g.sibling;if(null!==h){h.return=g.return;g=h;break}g=g.return}h=
g}T(a,b,e.children,c);b=b.child}return b;case 9:return e=b.type,f=b.pendingProps,d=f.children,rb(b,c),e=W(e,f.unstable_observedBits),d=d(e),b.effectTag|=1,T(a,b,d,c),b.child;case 14:return e=b.type,f=aa(e,b.pendingProps),f=aa(e.type,f),oh(a,b,e,f,d,c);case 15:return ph(a,b,b.type,b.pendingProps,d,c);case 17:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2),b.tag=1,N(d)?(a=!0,Bc(b)):a=!1,rb(b,c),Yg(b,d,e),pe(b,d,e,c),Ie(null,
b,d,!0,a,c);case 19:return vh(a,b,c)}throw Error(k(156,b.tag));};var bf=null,Ne=null,la=function(a,b,c,d){return new Fj(a,b,c,d)};ef.prototype.render=function(a){md(a,this._internalRoot,null,null)};ef.prototype.unmount=function(){var a=this._internalRoot,b=a.containerInfo;md(null,a,null,function(){b[Lb]=null})};var Di=function(a){if(13===a.tag){var b=Fc(ka(),150,100);Ja(a,b);df(a,b)}};var Yf=function(a){13===a.tag&&(Ja(a,3),df(a,3))};var Bi=function(a){if(13===a.tag){var b=ka();b=Va(b,a,null);Ja(a,
b);df(a,b)}};sd=function(a,b,c){switch(b){case "input":Dd(a,c);b=c.name;if("radio"===c.type&&null!=b){for(c=a;c.parentNode;)c=c.parentNode;c=c.querySelectorAll("input[name="+JSON.stringify(""+b)+'][type="radio"]');for(b=0;b<c.length;b++){var d=c[b];if(d!==a&&d.form===a.form){var e=ae(d);if(!e)throw Error(k(90));Gf(d);Dd(d,e)}}}break;case "textarea":Lf(a,c);break;case "select":b=c.value,null!=b&&hb(a,!!c.multiple,b,!1)}};(function(a,b,c,d){ee=a;eg=b;vd=c;vf=d})(Qh,function(a,b,c,d,e){var f=p;p|=4;
try{return Da(98,a.bind(null,b,c,d,e))}finally{p=f,p===H&&ha()}},function(){(p&(1|ca|ma))===H&&(uj(),xb())},function(a,b){var c=p;p|=2;try{return a(b)}finally{p=c,p===H&&ha()}});var mk={Events:[Hb,Pa,ae,pf,qd,lb,function(a){Kd(a,Ki)},sf,tf,sc,pc,xb,{current:!1}]};(function(a){var b=a.findFiberByHostInstance;return Ej(M({},a,{overrideHookState:null,overrideProps:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:da.ReactCurrentDispatcher,findHostInstanceByFiber:function(a){a=Sf(a);
return null===a?null:a.stateNode},findFiberByHostInstance:function(a){return b?b(a):null},findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null}))})({findFiberByHostInstance:Bb,bundleType:0,version:"16.13.1",rendererPackageName:"react-dom"});I.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=mk;I.createPortal=Xh;I.findDOMNode=function(a){if(null==a)return null;if(1===a.nodeType)return a;var b=a._reactInternalFiber;if(void 0===
b){if("function"===typeof a.render)throw Error(k(188));throw Error(k(268,Object.keys(a)));}a=Sf(b);a=null===a?null:a.stateNode;return a};I.flushSync=function(a,b){if((p&(ca|ma))!==H)throw Error(k(187));var c=p;p|=1;try{return Da(99,a.bind(null,b))}finally{p=c,ha()}};I.hydrate=function(a,b,c){if(!bc(b))throw Error(k(200));return nd(null,a,b,!0,c)};I.render=function(a,b,c){if(!bc(b))throw Error(k(200));return nd(null,a,b,!1,c)};I.unmountComponentAtNode=function(a){if(!bc(a))throw Error(k(40));return a._reactRootContainer?
(Rh(function(){nd(null,null,a,!1,function(){a._reactRootContainer=null;a[Lb]=null})}),!0):!1};I.unstable_batchedUpdates=Qh;I.unstable_createPortal=function(a,b){return Xh(a,b,2<arguments.length&&void 0!==arguments[2]?arguments[2]:null)};I.unstable_renderSubtreeIntoContainer=function(a,b,c,d){if(!bc(c))throw Error(k(200));if(null==a||void 0===a._reactInternalFiber)throw Error(k(38));return nd(a,b,c,!1,d)};I.version="16.13.1"});
</script>
    <script>const e = React.createElement;

function pathToString(path) {
  if (path[0] === '/') {
    return '/' + path.slice(1).join('/');
  } else {
    return path.join('/');
  }
}

function findCommonPath(files) {
  if (!files || !files.length) {
    return [];
  }

  function isPrefix(arr, prefix) {
    if (arr.length < prefix.length) {
      return false;
    }
    for (let i = prefix.length - 1; i >= 0; --i) {
      if (arr[i] !== prefix[i]) {
        return false;
      }
    }
    return true;
  }

  let commonPath = files[0].path.slice(0, -1);
  while (commonPath.length) {
    if (files.every(file => isPrefix(file.path, commonPath))) {
      break;
    }
    commonPath.pop();
  }
  return commonPath;
}

function findFolders(files) {
  if (!files || !files.length) {
    return [];
  }

  let folders = files.filter(file => file.path.length > 1).map(file => file.path[0]);
  folders = [...new Set(folders)]; // unique
  folders.sort();

  folders = folders.map(folder => {
    let filesInFolder = files
      .filter(file => file.path[0] === folder)
      .map(file => ({
        ...file,
        path: file.path.slice(1),
        parent: [...file.parent, file.path[0]],
      }));

    const children = findFolders(filesInFolder); // recursion

    return {
      is_folder: true,
      path: [folder],
      parent: files[0].parent,
      children,
      covered: children.reduce((sum, file) => sum + file.covered, 0),
      coverable: children.reduce((sum, file) => sum + file.coverable, 0),
      prevRun: {
        covered: children.reduce((sum, file) => sum + file.prevRun.covered, 0),
        coverable: children.reduce((sum, file) => sum + file.prevRun.coverable, 0),
      }
    };
  });

  return [
    ...folders,
    ...files.filter(file => file.path.length === 1),
  ];
}

class App extends React.Component {
  constructor(...args) {
    super(...args);

    this.state = {
      current: [],
    };
  }

  componentDidMount() {
    this.updateStateFromLocation();
    window.addEventListener("hashchange", () => this.updateStateFromLocation(), false);
  }

  updateStateFromLocation() {
    if (window.location.hash.length > 1) {
      const current = window.location.hash.substr(1).split('/');
      this.setState({current});
    } else {
      this.setState({current: []});
    }
  }

  getCurrentPath() {
    let file = this.props.root;
    let path = [file];
    for (let p of this.state.current) {
      file = file.children.find(file => file.path[0] === p);
      if (!file) {
        return path;
      }
      path.push(file);
    }
    return path;
  }

  render() {
    const path = this.getCurrentPath();
    const file = path[path.length - 1];

    let w = null;
    if (file.is_folder) {
      w = e(FilesList, {
        folder: file,
        onSelectFile: this.selectFile.bind(this),
        onBack: path.length > 1 ? this.back.bind(this) : null,
      });
    } else {
      w = e(DisplayFile, {
        file,
        onBack: this.back.bind(this),
      });
    }

    return e('div', {className: 'app'}, w);
  }

  selectFile(file) {
    this.setState(({current}) => {
      return {current: [...current, file.path[0]]};
    }, () => this.updateHash());
  }

  back(file) {
    this.setState(({current}) => {
      return {current: current.slice(0, current.length - 1)};
    }, () => this.updateHash());
  }

  updateHash() {
    if (!this.state.current || !this.state.current.length) {
      window.location = '#';
    } else {
      window.location = '#' + this.state.current.join('/');
    }
  }
}

function FilesList({folder, onSelectFile, onBack}) {
  let files = folder.children;
  return e('div', {className: 'display-folder'},
    e(FileHeader, {file: folder, onBack}),
    e('table', {className: 'files-list'},
      e('thead', {className: 'files-list__head'},
        e('tr', null,
          e('th', null, "Path"),
          e('th', null, "Coverage")
        )
      ),
      e('tbody', {className: 'files-list__body'},
        files.map(file => e(File, {file, onClick: onSelectFile}))
      )
    )
  );
}

function File({file, onClick}) {
  const coverage = file.coverable ? file.covered / file.coverable * 100 : -1;
  const coverageDelta = file.prevRun &&
    (file.covered / file.coverable * 100 - file.prevRun.covered / file.prevRun.coverable * 100);

  return e('tr', {
      className: 'files-list__file'
        + (coverage >= 0 && coverage < 50 ? ' files-list__file_low': '')
        + (coverage >= 50 && coverage < 80 ? ' files-list__file_medium': '')
        + (coverage >= 80 ? ' files-list__file_high': '')
        + (file.is_folder ? ' files-list__file_folder': ''),
      onClick: () => onClick(file),
    },
    e('td', null, e('a', null, pathToString(file.path))),
    e('td', null,
      file.covered + ' / ' + file.coverable +
      (coverage >= 0 ? ' (' + coverage.toFixed(2) + '%)' : ''),
      e('span', {title: 'Change from the previous run'},
        (coverageDelta ? ` (${coverageDelta > 0 ? '+' : ''}${coverageDelta.toFixed(2)}%)` : ''))
    )
  );
}

function DisplayFile({file, onBack}) {
  return e('div', {className: 'display-file'},
    e(FileHeader, {file, onBack}),
    e(FileContent, {file})
  );
}

function FileHeader({file, onBack}) {
  const coverage = file.covered / file.coverable * 100;
  const coverageDelta = file.prevRun && (coverage - file.prevRun.covered / file.prevRun.coverable * 100);

  return e('div', {className: 'file-header'},
    onBack ? e('a', {className: 'file-header__back', onClick: onBack}, 'Back') : null,
    e('div', {className: 'file-header__name'}, pathToString([...file.parent, ...file.path])),
    e('div', {className: 'file-header__stat'},
      'Covered: ' + file.covered + ' of ' + file.coverable +
      (file.coverable ? ' (' + coverage.toFixed(2) + '%)' : ''),
      e('span', {title: 'Change from the previous run'},
        (coverageDelta ? ` (${coverageDelta > 0 ? '+' : ''}${coverageDelta.toFixed(2)}%)` : ''))
    )
  );
}

function FileContent({file}) {
  return e('pre', {className: 'file-content'},
    file.content.split(/\r?\n/).map((line, index) => {
      const trace = file.traces.find(trace => trace.line === index + 1);
      const covered = trace && trace.stats.Line;
      const uncovered = trace && !trace.stats.Line;
      return e('code', {
          className: 'code-line'
            + (covered ? ' code-line_covered' : '')
            + (uncovered ? ' code-line_uncovered' : ''),
          title: trace ? JSON.stringify(trace.stats, null, 2) : null,
        }, line);
    })
  );
}

(function(){
  const commonPath = findCommonPath(data.files);
  const prevFilesMap = new Map();

  previousData && previousData.files.forEach((file) => {
    const path = file.path.slice(commonPath.length).join('/');
    prevFilesMap.set(path, file);
  });

  const files = data.files.map((file) => {
    const path = file.path.slice(commonPath.length);
    const { covered = 0, coverable = 0 } = prevFilesMap.get(path.join('/')) || {};
    return {
      ...file,
      path,
      parent: commonPath,
      prevRun: { covered, coverable },
    };
  });

  const children = findFolders(files);

  const root = {
    is_folder: true,
    children,
    path: commonPath,
    parent: [],
    covered: children.reduce((sum, file) => sum + file.covered, 0),
    coverable: children.reduce((sum, file) => sum + file.coverable, 0),
    prevRun: {
      covered: children.reduce((sum, file) => sum + file.prevRun.covered, 0),
      coverable: children.reduce((sum, file) => sum + file.prevRun.coverable, 0),
    }
  };

  ReactDOM.render(e(App, {root, prevFilesMap}), document.getElementById('root'));
}());
</script>
</body>
</html>